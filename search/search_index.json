{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"chap1/1Exam_Guide/","text":"L1 AWS Certified Solutions Architect \u2013 Associate (SAA-C02) Exam Guide Domain 1: Design Resilient Architectures 30% 1-1 Design a multi-tier architecture solution Determine a solution design based on access patterns. Determine a scaling strategy for components used in a design. Select an appropriate database based on requirements. Select an appropriate compute and storage service based on requirements. 1-2 Design highly available and/or fault-tolerant architectures Determine the amount of resources needed to provide a fault-tolerant architecture across Availability Zones. Select a highly available configuration to mitigate single points of failure. Apply AWS services to improve the reliability of legacy applications when application changes are not possible. Select an appropriate disaster recovery strategy to meet business requirements. Identify key performance indicators to ensure the high availability of the solution. 1-3 Design decoupling mechanisms using AWS services Determine which AWS services can be leveraged to achieve loose coupling of components. Determine when to leverage serverless technologies to enable decoupling. 1-4 Choose appropriate resilient storage Define a strategy to ensure the durability of data. Identify how data service consistency will affect the operation of the application. Select data services that will meet the access requirements of the application Identify storage services that can be used with hybrid or non-cloud-native applications. Domain 2: Design High-Performing Architectures 28% 2-1 Identify elastic and scalable compute solutions for a workload Select the appropriate instance(s) based on compute, storage, and networking requirements. Choose the appropriate architecture and services that scale to meet performance requirements. Identify metrics to monitor the performance of the solution. 2-2 Select high-performing and scalable storage solutions for a workload Select a storage service and configuration that meets performance demands. Determine storage services that can scale to accommodate future needs . 2-3 Select high-performing networking solutions for a workload Select appropriate AWS connectivity options to meet performance demands. Select appropriate features to optimize connectivity to AWS public services. Determine an edge caching strategy to provide performance benefits. Select appropriate data transfer service for migration and/or ingestion. 2-4 Choose high-performing database solutions for a workload Select an appropriate database scaling strategy. Determine when database caching is required for performance improvement. Choose a suitable database service to meet performance needs. Domain 3: Design Secure Applications and Architectures 24% 3-1 Design secure access to AWS resources Determine when to choose between users, groups, and roles. Interpret the net effect of a given access policy. Select appropriate techniques to secure a root account. Determine ways to secure credentials using features of AWS IAM. Determine the secure method for an application to access AWS APIs. Select appropriate services to create traceability for access to AWS resources. 3-2 Design secure application tiers Given traffic control requirements, determine when and how to use security groups and network ACLs. Determine a network segmentation strategy using public and private subnets. Select the appropriate routing mechanism to securely access AWS service endpoints or internet-based resources from Amazon VPC. Select appropriate AWS services to protect applications from external threats. 3-3 Select appropriate data security options Determine the policies that need to be applied to objects based on access patterns. Select appropriate encryption options for data at rest and in transit for AWS services. Select appropriate key management options based on requirements. Domain 4: Design Cost-Optimized Architectures 18% 4-1 Identify cost-effective storage solutions Determine the most cost-effective data storage options based on requirements. Apply automated processes to ensure that data over time is stored on storage tiers that minimize costs. 4-2 Identify cost-effective compute and database services Determine the most cost-effective Amazon EC2 billing options for each aspect of the workload Determine the most cost-effective database options based on requirements. Select appropriate scaling strategies from a cost perspective. Select and size compute resources that are optimally suited for the workload. Determine options to minimize total cost of ownership (TCO) through managed services and serverless architectures. 4-3 Design cost-optimized network architectures Identify when content delivery can be used to reduce costs. Determine strategies to reduce data transfer costs within AWS. Determine the most cost-effective connectivity options between AWS and on-premises environments. 5\u3001AWS services and features Analytics: Amazon Athena Amazon Elasticsearch Service (Amazon ES) Amazon EMR AWS Glue Amazon Kinesis Amazon QuickSight AWS Billing and Cost Management: AWS Budgets Cost Explorer Application Integration: Amazon Simple Notification Service (Amazon SNS) Amazon Simple Queue Service (Amazon SQS) Compute: Amazon EC2 AWS Elastic Beanstalk Amazon Elastic Container Service (Amazon ECS) Amazon Elastic Kubernetes Service (Amazon EKS) Elastic Load Balancing AWS Fargate AWS Lambda Database Amazon Aurora Amazon DynamoDB Amazon ElastiCache Amazon RDS Amazon Redshift Management and Governance: AWS Auto Scaling AWS Backup AWS CloudFormation AWS CloudTrail Amazon CloudWatch AWS Config Amazon EventBridge (Amazon CloudWatch Events) AWS Organizations AWS Resource Access Manager AWS Systems Manager AWS Trusted Advisor Migration and Transfer: AWS Database Migration Service (AWS DMS) AWS DataSync AWS Migration Hub AWS Server Migration Service (AWS SMS) AWS Snowball AWS Transfer Family Networking and Content Delivery: Amazon API Gateway Amazon CloudFront AWS Direct Connect AWS Global Accelerator Amazon Route 53 AWS Transit Gateway Amazon VPC (and associated features) Security, Identity, and Compliance: AWS Certificate Manager (ACM) AWS Directory Service Amazon GuardDuty AWS Identity and Access Management (IAM) Amazon Inspector AWS Key Management Service (AWS KMS) Amazon Macie AWS Secrets Manager AWS Shield AWS Single Sign-On AWS WAF Storage: Amazon Elastic Block Store (Amazon EBS) Amazon Elastic File System (Amazon EFS) Amazon FSx Amazon S3 Amazon S3 Glacier AWS Storage Gateway","title":"L1 SAA-C02 Exam Guide"},{"location":"chap1/1Exam_Guide/#l1-aws-certified-solutions-architect-associate-saa-c02-exam-guide","text":"","title":"L1 AWS Certified Solutions Architect \u2013 Associate (SAA-C02) Exam Guide"},{"location":"chap1/1Exam_Guide/#domain-1-design-resilient-architectures-30","text":"","title":"Domain 1: Design Resilient Architectures  30%"},{"location":"chap1/1Exam_Guide/#1-1-design-a-multi-tier-architecture-solution","text":"Determine a solution design based on access patterns. Determine a scaling strategy for components used in a design. Select an appropriate database based on requirements. Select an appropriate compute and storage service based on requirements.","title":"1-1  Design a multi-tier architecture solution"},{"location":"chap1/1Exam_Guide/#1-2-design-highly-available-andor-fault-tolerant-architectures","text":"Determine the amount of resources needed to provide a fault-tolerant architecture across Availability Zones. Select a highly available configuration to mitigate single points of failure. Apply AWS services to improve the reliability of legacy applications when application changes are not possible. Select an appropriate disaster recovery strategy to meet business requirements. Identify key performance indicators to ensure the high availability of the solution.","title":"1-2 Design highly available and/or fault-tolerant architectures"},{"location":"chap1/1Exam_Guide/#1-3-design-decoupling-mechanisms-using-aws-services","text":"Determine which AWS services can be leveraged to achieve loose coupling of components. Determine when to leverage serverless technologies to enable decoupling.","title":"1-3 Design decoupling mechanisms using AWS services"},{"location":"chap1/1Exam_Guide/#1-4-choose-appropriate-resilient-storage","text":"Define a strategy to ensure the durability of data. Identify how data service consistency will affect the operation of the application. Select data services that will meet the access requirements of the application Identify storage services that can be used with hybrid or non-cloud-native applications.","title":"1-4 Choose appropriate resilient storage"},{"location":"chap1/1Exam_Guide/#domain-2-design-high-performing-architectures-28","text":"","title":"Domain 2: Design High-Performing Architectures 28%"},{"location":"chap1/1Exam_Guide/#2-1-identify-elastic-and-scalable-compute-solutions-for-a-workload","text":"Select the appropriate instance(s) based on compute, storage, and networking requirements. Choose the appropriate architecture and services that scale to meet performance requirements. Identify metrics to monitor the performance of the solution.","title":"2-1 Identify elastic and scalable compute solutions for a workload"},{"location":"chap1/1Exam_Guide/#2-2-select-high-performing-and-scalable-storage-solutions-for-a-workload","text":"Select a storage service and configuration that meets performance demands. Determine storage services that can scale to accommodate future needs .","title":"2-2 Select high-performing and scalable storage solutions for a workload"},{"location":"chap1/1Exam_Guide/#2-3-select-high-performing-networking-solutions-for-a-workload","text":"Select appropriate AWS connectivity options to meet performance demands. Select appropriate features to optimize connectivity to AWS public services. Determine an edge caching strategy to provide performance benefits. Select appropriate data transfer service for migration and/or ingestion.","title":"2-3 Select high-performing networking solutions for a workload"},{"location":"chap1/1Exam_Guide/#2-4-choose-high-performing-database-solutions-for-a-workload","text":"Select an appropriate database scaling strategy. Determine when database caching is required for performance improvement. Choose a suitable database service to meet performance needs.","title":"2-4 Choose high-performing database solutions for a workload"},{"location":"chap1/1Exam_Guide/#domain-3-design-secure-applications-and-architectures-24","text":"","title":"Domain 3: Design Secure Applications and Architectures 24%"},{"location":"chap1/1Exam_Guide/#3-1-design-secure-access-to-aws-resources","text":"Determine when to choose between users, groups, and roles. Interpret the net effect of a given access policy. Select appropriate techniques to secure a root account. Determine ways to secure credentials using features of AWS IAM. Determine the secure method for an application to access AWS APIs. Select appropriate services to create traceability for access to AWS resources.","title":"3-1 Design secure access to AWS resources"},{"location":"chap1/1Exam_Guide/#3-2-design-secure-application-tiers","text":"Given traffic control requirements, determine when and how to use security groups and network ACLs. Determine a network segmentation strategy using public and private subnets. Select the appropriate routing mechanism to securely access AWS service endpoints or internet-based resources from Amazon VPC. Select appropriate AWS services to protect applications from external threats.","title":"3-2 Design secure application tiers"},{"location":"chap1/1Exam_Guide/#3-3-select-appropriate-data-security-options","text":"Determine the policies that need to be applied to objects based on access patterns. Select appropriate encryption options for data at rest and in transit for AWS services. Select appropriate key management options based on requirements.","title":"3-3 Select appropriate data security options"},{"location":"chap1/1Exam_Guide/#domain-4-design-cost-optimized-architectures-18","text":"","title":"Domain 4: Design Cost-Optimized Architectures 18%"},{"location":"chap1/1Exam_Guide/#4-1-identify-cost-effective-storage-solutions","text":"Determine the most cost-effective data storage options based on requirements. Apply automated processes to ensure that data over time is stored on storage tiers that minimize costs.","title":"4-1 Identify cost-effective storage solutions"},{"location":"chap1/1Exam_Guide/#4-2-identify-cost-effective-compute-and-database-services","text":"Determine the most cost-effective Amazon EC2 billing options for each aspect of the workload Determine the most cost-effective database options based on requirements. Select appropriate scaling strategies from a cost perspective. Select and size compute resources that are optimally suited for the workload. Determine options to minimize total cost of ownership (TCO) through managed services and serverless architectures.","title":"4-2 Identify cost-effective compute and database services"},{"location":"chap1/1Exam_Guide/#4-3-design-cost-optimized-network-architectures","text":"Identify when content delivery can be used to reduce costs. Determine strategies to reduce data transfer costs within AWS. Determine the most cost-effective connectivity options between AWS and on-premises environments.","title":"4-3 Design cost-optimized network architectures"},{"location":"chap1/1Exam_Guide/#5aws-services-and-features","text":"Analytics: Amazon Athena Amazon Elasticsearch Service (Amazon ES) Amazon EMR AWS Glue Amazon Kinesis Amazon QuickSight AWS Billing and Cost Management: AWS Budgets Cost Explorer Application Integration: Amazon Simple Notification Service (Amazon SNS) Amazon Simple Queue Service (Amazon SQS) Compute: Amazon EC2 AWS Elastic Beanstalk Amazon Elastic Container Service (Amazon ECS) Amazon Elastic Kubernetes Service (Amazon EKS) Elastic Load Balancing AWS Fargate AWS Lambda Database Amazon Aurora Amazon DynamoDB Amazon ElastiCache Amazon RDS Amazon Redshift Management and Governance: AWS Auto Scaling AWS Backup AWS CloudFormation AWS CloudTrail Amazon CloudWatch AWS Config Amazon EventBridge (Amazon CloudWatch Events) AWS Organizations AWS Resource Access Manager AWS Systems Manager AWS Trusted Advisor Migration and Transfer: AWS Database Migration Service (AWS DMS) AWS DataSync AWS Migration Hub AWS Server Migration Service (AWS SMS) AWS Snowball AWS Transfer Family Networking and Content Delivery: Amazon API Gateway Amazon CloudFront AWS Direct Connect AWS Global Accelerator Amazon Route 53 AWS Transit Gateway Amazon VPC (and associated features) Security, Identity, and Compliance: AWS Certificate Manager (ACM) AWS Directory Service Amazon GuardDuty AWS Identity and Access Management (IAM) Amazon Inspector AWS Key Management Service (AWS KMS) Amazon Macie AWS Secrets Manager AWS Shield AWS Single Sign-On AWS WAF Storage: Amazon Elastic Block Store (Amazon EBS) Amazon Elastic File System (Amazon EFS) Amazon FSx Amazon S3 Amazon S3 Glacier AWS Storage Gateway","title":"5\u3001AWS services and features"},{"location":"chap1/2Learning_path/","text":"L2 AWS Solutions Architect \u2013 Associate SAA-C02 Exam Topics https://jayendrapatil.com/aws-certified-solutions-architect-associate-saa-c02-exam-learning-path/#google_vignette Networking Be sure to create VPC from scratch. This is mandatory . Create VPC and understand whats an CIDR and addressing patterns Create public and private subnets, configure proper routes, security groups, NACLs. (hint: Subnets are public or private depending on whether they can route traffic directly through Internet gateway) Create Bastion for communication with instances Create NAT Gateway or Instances for instances in private subnets to interact with internet Create two tier architecture with application in public and database in private subnets Create three tier architecture with web servers in public, application and database servers in private. (hint: focus on security group configuration with least privilege) Make sure to understand how the communication happens between Internet, Public subnets, Private subnets, NAT, Bastion etc. Understand difference between Security Groups and NACLs Security Groups are Stateful vs NACLs are stateless . Also only NACLs provide an ability to deny or block IPs Understand VPC endpoints and what services it can help interact (hint: VPC Endpoints routes traffic internally without Internet ) VPC Gateway Endpoints supports S3 and DynamoDB . VPC Interface Endpoints OR Private Links supports others Understand difference between NAT Gateway and NAT Instance (hint: NAT Gateway is AWS managed and is scalable and highly available) Understand how NAT high availability can be achieved (hint: provision NAT in each AZ and route traffic from subnets within that AZ through that NAT Gateway) Understand VPN and Direct Connect for on-premises to AWS connectivity VPN provides quick connectivity, cost-effective, secure channel, however routes through internet and does not provide consistent throughput Direct Connect provides consistent dedicated throughput without Internet, however requires time to setup and is not cost-effective Understand Data Migration techniques Understand Data Migration techniques Choose Snowball vs Snowmobile vs Direct Connect vs VPN depending on the bandwidth available, data transfer needed, time available, encryption requirement, one-time or continuous requirement Snowball , SnowMobile are for one-time data, cost-effective, quick and ideal for huge data transfer Direct Connect , VPN are ideal for continuous or frequent data transfers Understand CloudFront as CDN and the static and dynamic caching it provides, what can be its origin (hint: CloudFront can point to on-premises sources and its usecases with S3 to reduce load and cost) Understand Route 53 for routing Understand Route 53 health checks and failover routing Understand Route 53 Routing Policies it provides and their use cases mainly for high availability (hint: focus on weighted, latency, geolocation, failover routing) Be sure to cover ELB concepts in deep. SAA-C02 focuses on ALB and NLB and does not cover CLB Understand differences between CLB vs ALB vs NLB ALB is layer 7 while NLB is layer 4 ALB provides content based, host based, path based routing ALB provides dynamic port mapping which allows same tasks to be hosted on ECS node NLB provides low latency and ability to scale NLB provides static IP address Security Understand IAM as a whole Focus on IAM role (hint: can be used for EC2 application access and Cross-account access) Understand IAM identity providers and federation and use cases Understand MFA and how would implement two factor authentication for an application Understand IAM Policies (hint: expect couple of questions with policies defined and you need to select correct statements) Understand encryption services KMS for key management and envelope encryption Focus on S3 with SSE, SSE-C, SSE-KMS Know SQS now provides SSE support AWS WAF integrates with CloudFront to provide protection against Cross-site scripting (XSS) attacks. It also provide IP blocking and geo-protection. AWS Shield integrates with CloudFront to provide protection against DDoS. Refer Disaster Recovery whitepaper, be sure you know the different recovery types with impact on RTO/RPO. Storage Understand various storage options S3, EBS, Instance store, EFS, Glacier, FSx and what are the use cases and anti patterns for each Instance Store Understand Instance Store (hint: it is physically attached to the EC2 instance and provides the lowest latency and highest IOPS) Elastic Block Storage \u2013 EBS Understand various EBS volume types and their use cases in terms of IOPS and throughput. SSD for IOPS and HDD for throughput Understand Burst performance and I/O credits to handle occasional peaks Understand EBS Snapshots (hint: backups are automated, snapshots are manual) Simple Storage Service \u2013 S3 Cover S3 in depth Understand S3 storage classes with lifecycle policies Understand the difference between SA Standard vs SA IA vs SA IA One Zone in terms of cost and durability Understand S3 Data Protection (hint: S3 Client side encryption encrypts data before storing it in S3) Understand S3 features including S3 provides a cost effective static website hosting S3 versioning provides protection against accidental overwrites and deletions S3 Pre-Signed URLs for both upload and download provides access without needing AWS credentials S3 CORS allows cross domain calls S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Understand Glacier as an archival storage with various retrieval patterns Glacier Expedited retrieval now allows object retrieval within mins Understand Storage gateway and its different types. Cached Volume Gateway provides access to frequently accessed data, while using AWS as the actual storage Stored Volume gateway uses AWS as a backup, while the data is being stored on-premises as well File Gateway supports SMB protocol Understand FSx easy and cost effective to launch and run popular file systems. FSx provides two file systems to choose from: Amazon FSx for Windows File Server for business applications and Amazon FSx for Lustre for high-performance workloads. Understand the difference between EBS vs S3 vs EFS EFS provides shared volume across multiple EC2 instances, while EBS can be attached to a single volume within the same AZ. Understand the difference between EBS vs Instance Store Would recommend referring Storage Options whitepaper, although a bit dated 90% still holds right Compute Understand Elastic Cloud Compute \u2013 EC2 Understand Auto Scaling and ELB , how they work together to provide High Available and Scalable solution. (hint: Span both ELB and Auto Scaling across Multi-AZs to provide High Availability ) Understand EC2 Instance Purchase Types \u2013 Reserved, Scheduled Reserved, On-demand and Spot and their use cases Choose Reserved Instances for continuous persistent load Choose Scheduled Reserved Instances for load with fixed scheduled and time interval Choose Spot instances for fault tolerant and Spiky loads Reserved instances provides cost benefits for long terms requirements over On-demand instances Spot instances provides cost benefits for temporary fault tolerant spiky load Understand EC2 Placement Groups (hint: Cluster placement groups provide low latency and high throughput communication, while Spread placement group provides high availability) Understand Lambda and serverless architecture, its features and use cases. (hint: Lambda integrated with API Gateway to provide a serverless, highly scalable, cost-effective architecture ) Understand ECS with its ability to deploy containers and micro services architecture. ECS role for tasks can be provided through taskRoleArn ALB provides dynamic port mapping to allow multiple same tasks on the same node Know Elastic Beanstalk at a high level, what it provides and its ability to get an application running quickly. Databases Understand relational and NoSQLs data storage options which include RDS, DynamoDB, Aurora and their use cases RDS Understand RDS features \u2013 Read Replicas vs Multi-AZ Read Replicas for scalability, Multi-AZ for High Availability Multi-AZ are regional only Read Replicas can span across regions and can be used for disaster recovery Understand Automated Backups, underlying volume types Aurora Understand Aurora provides multiple read replicas and replicates 6 copies of data across AZs Understand Aurora Serverless provides a highly scalable cost-effective database solution DynamoDB Understand DynamoDB with its low latency performance, key-value store (hint: DynamoDB is not a relational database) DynamoDB DAX provides caching for DynamoDB Understand DynamoDB provisioned throughput for Read/Writes (It is more cover in Developer exam though.) Know ElastiCache use cases, mainly for caching performance Integration Tools Understand SQS as message queuing service and SNS as pub/sub notification service Understand SQS features like visibility, long poll vs short poll Focus on SQS as a decoupling service Understand SQS Standard vs SQS FIFO difference (hint: FIFO provides exactly once delivery both low throughput ) Analytics Know Redshift as a business intelligence tool Know Kinesis for real time data capture and analytics At least know what AWS Glue does, so you can eliminate the answer Management Tools Understand CloudWatch monitoring to provide operational transparency Know which EC2 metrics it can track. Remember, it cannot track memory and disk space/swap utilization Understand CloudWatch is extendable with custom metrics Understand CloudTrail for Audit Have a basic understanding of CloudFormation, OpsWorks AWS Whitepapers & Cheat sheets Architecting for the AWS Cloud: Best Practices AWS Well-Architected Framework whitepaper AWS Storage & Content Delivery Services Cheat sheet AWS Compute Services Cheat Sheet AWS Networking Services Cheat Sheet AWS Database Services Cheat Sheet","title":"L2 Associate SAA-C02 Exam Topics"},{"location":"chap1/2Learning_path/#l2-aws-solutions-architect-associate-saa-c02-exam-topics","text":"https://jayendrapatil.com/aws-certified-solutions-architect-associate-saa-c02-exam-learning-path/#google_vignette","title":"L2 AWS Solutions Architect \u2013 Associate SAA-C02 Exam Topics"},{"location":"chap1/2Learning_path/#networking","text":"Be sure to create VPC from scratch. This is mandatory . Create VPC and understand whats an CIDR and addressing patterns Create public and private subnets, configure proper routes, security groups, NACLs. (hint: Subnets are public or private depending on whether they can route traffic directly through Internet gateway) Create Bastion for communication with instances Create NAT Gateway or Instances for instances in private subnets to interact with internet Create two tier architecture with application in public and database in private subnets Create three tier architecture with web servers in public, application and database servers in private. (hint: focus on security group configuration with least privilege) Make sure to understand how the communication happens between Internet, Public subnets, Private subnets, NAT, Bastion etc. Understand difference between Security Groups and NACLs Security Groups are Stateful vs NACLs are stateless . Also only NACLs provide an ability to deny or block IPs Understand VPC endpoints and what services it can help interact (hint: VPC Endpoints routes traffic internally without Internet ) VPC Gateway Endpoints supports S3 and DynamoDB . VPC Interface Endpoints OR Private Links supports others Understand difference between NAT Gateway and NAT Instance (hint: NAT Gateway is AWS managed and is scalable and highly available) Understand how NAT high availability can be achieved (hint: provision NAT in each AZ and route traffic from subnets within that AZ through that NAT Gateway) Understand VPN and Direct Connect for on-premises to AWS connectivity VPN provides quick connectivity, cost-effective, secure channel, however routes through internet and does not provide consistent throughput Direct Connect provides consistent dedicated throughput without Internet, however requires time to setup and is not cost-effective Understand Data Migration techniques Understand Data Migration techniques Choose Snowball vs Snowmobile vs Direct Connect vs VPN depending on the bandwidth available, data transfer needed, time available, encryption requirement, one-time or continuous requirement Snowball , SnowMobile are for one-time data, cost-effective, quick and ideal for huge data transfer Direct Connect , VPN are ideal for continuous or frequent data transfers Understand CloudFront as CDN and the static and dynamic caching it provides, what can be its origin (hint: CloudFront can point to on-premises sources and its usecases with S3 to reduce load and cost) Understand Route 53 for routing Understand Route 53 health checks and failover routing Understand Route 53 Routing Policies it provides and their use cases mainly for high availability (hint: focus on weighted, latency, geolocation, failover routing) Be sure to cover ELB concepts in deep. SAA-C02 focuses on ALB and NLB and does not cover CLB Understand differences between CLB vs ALB vs NLB ALB is layer 7 while NLB is layer 4 ALB provides content based, host based, path based routing ALB provides dynamic port mapping which allows same tasks to be hosted on ECS node NLB provides low latency and ability to scale NLB provides static IP address","title":"Networking"},{"location":"chap1/2Learning_path/#security","text":"Understand IAM as a whole Focus on IAM role (hint: can be used for EC2 application access and Cross-account access) Understand IAM identity providers and federation and use cases Understand MFA and how would implement two factor authentication for an application Understand IAM Policies (hint: expect couple of questions with policies defined and you need to select correct statements) Understand encryption services KMS for key management and envelope encryption Focus on S3 with SSE, SSE-C, SSE-KMS Know SQS now provides SSE support AWS WAF integrates with CloudFront to provide protection against Cross-site scripting (XSS) attacks. It also provide IP blocking and geo-protection. AWS Shield integrates with CloudFront to provide protection against DDoS. Refer Disaster Recovery whitepaper, be sure you know the different recovery types with impact on RTO/RPO.","title":"Security"},{"location":"chap1/2Learning_path/#storage","text":"Understand various storage options S3, EBS, Instance store, EFS, Glacier, FSx and what are the use cases and anti patterns for each","title":"Storage"},{"location":"chap1/2Learning_path/#instance-store","text":"Understand Instance Store (hint: it is physically attached to the EC2 instance and provides the lowest latency and highest IOPS)","title":"Instance Store"},{"location":"chap1/2Learning_path/#elastic-block-storage-ebs","text":"Understand various EBS volume types and their use cases in terms of IOPS and throughput. SSD for IOPS and HDD for throughput Understand Burst performance and I/O credits to handle occasional peaks Understand EBS Snapshots (hint: backups are automated, snapshots are manual)","title":"Elastic Block Storage \u2013 EBS"},{"location":"chap1/2Learning_path/#simple-storage-service-s3","text":"Cover S3 in depth Understand S3 storage classes with lifecycle policies Understand the difference between SA Standard vs SA IA vs SA IA One Zone in terms of cost and durability Understand S3 Data Protection (hint: S3 Client side encryption encrypts data before storing it in S3) Understand S3 features including S3 provides a cost effective static website hosting S3 versioning provides protection against accidental overwrites and deletions S3 Pre-Signed URLs for both upload and download provides access without needing AWS credentials S3 CORS allows cross domain calls S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Understand Glacier as an archival storage with various retrieval patterns Glacier Expedited retrieval now allows object retrieval within mins Understand Storage gateway and its different types. Cached Volume Gateway provides access to frequently accessed data, while using AWS as the actual storage Stored Volume gateway uses AWS as a backup, while the data is being stored on-premises as well File Gateway supports SMB protocol Understand FSx easy and cost effective to launch and run popular file systems. FSx provides two file systems to choose from: Amazon FSx for Windows File Server for business applications and Amazon FSx for Lustre for high-performance workloads. Understand the difference between EBS vs S3 vs EFS EFS provides shared volume across multiple EC2 instances, while EBS can be attached to a single volume within the same AZ. Understand the difference between EBS vs Instance Store Would recommend referring Storage Options whitepaper, although a bit dated 90% still holds right","title":"Simple Storage Service \u2013 S3"},{"location":"chap1/2Learning_path/#compute","text":"Understand Elastic Cloud Compute \u2013 EC2 Understand Auto Scaling and ELB , how they work together to provide High Available and Scalable solution. (hint: Span both ELB and Auto Scaling across Multi-AZs to provide High Availability ) Understand EC2 Instance Purchase Types \u2013 Reserved, Scheduled Reserved, On-demand and Spot and their use cases Choose Reserved Instances for continuous persistent load Choose Scheduled Reserved Instances for load with fixed scheduled and time interval Choose Spot instances for fault tolerant and Spiky loads Reserved instances provides cost benefits for long terms requirements over On-demand instances Spot instances provides cost benefits for temporary fault tolerant spiky load Understand EC2 Placement Groups (hint: Cluster placement groups provide low latency and high throughput communication, while Spread placement group provides high availability) Understand Lambda and serverless architecture, its features and use cases. (hint: Lambda integrated with API Gateway to provide a serverless, highly scalable, cost-effective architecture ) Understand ECS with its ability to deploy containers and micro services architecture. ECS role for tasks can be provided through taskRoleArn ALB provides dynamic port mapping to allow multiple same tasks on the same node Know Elastic Beanstalk at a high level, what it provides and its ability to get an application running quickly.","title":"Compute"},{"location":"chap1/2Learning_path/#databases","text":"Understand relational and NoSQLs data storage options which include RDS, DynamoDB, Aurora and their use cases","title":"Databases"},{"location":"chap1/2Learning_path/#rds","text":"Understand RDS features \u2013 Read Replicas vs Multi-AZ Read Replicas for scalability, Multi-AZ for High Availability Multi-AZ are regional only Read Replicas can span across regions and can be used for disaster recovery Understand Automated Backups, underlying volume types","title":"RDS"},{"location":"chap1/2Learning_path/#aurora","text":"Understand Aurora provides multiple read replicas and replicates 6 copies of data across AZs Understand Aurora Serverless provides a highly scalable cost-effective database solution","title":"Aurora"},{"location":"chap1/2Learning_path/#dynamodb","text":"Understand DynamoDB with its low latency performance, key-value store (hint: DynamoDB is not a relational database) DynamoDB DAX provides caching for DynamoDB Understand DynamoDB provisioned throughput for Read/Writes (It is more cover in Developer exam though.) Know ElastiCache use cases, mainly for caching performance","title":"DynamoDB"},{"location":"chap1/2Learning_path/#integration-tools","text":"Understand SQS as message queuing service and SNS as pub/sub notification service Understand SQS features like visibility, long poll vs short poll Focus on SQS as a decoupling service Understand SQS Standard vs SQS FIFO difference (hint: FIFO provides exactly once delivery both low throughput )","title":"Integration Tools"},{"location":"chap1/2Learning_path/#analytics","text":"Know Redshift as a business intelligence tool Know Kinesis for real time data capture and analytics At least know what AWS Glue does, so you can eliminate the answer","title":"Analytics"},{"location":"chap1/2Learning_path/#management-tools","text":"Understand CloudWatch monitoring to provide operational transparency Know which EC2 metrics it can track. Remember, it cannot track memory and disk space/swap utilization Understand CloudWatch is extendable with custom metrics Understand CloudTrail for Audit Have a basic understanding of CloudFormation, OpsWorks","title":"Management Tools"},{"location":"chap1/2Learning_path/#aws-whitepapers-cheat-sheets","text":"Architecting for the AWS Cloud: Best Practices AWS Well-Architected Framework whitepaper AWS Storage & Content Delivery Services Cheat sheet AWS Compute Services Cheat Sheet AWS Networking Services Cheat Sheet AWS Database Services Cheat Sheet","title":"AWS Whitepapers &amp; Cheat sheets"},{"location":"chap10/1aws_cms/","text":"L1 AWS Cloud Migration Services 1 AWS Cloud Migration Services AWS Cloud Migration services help to address a lot of common use cases such as cloud migration, disaster recovery, data center decommission, and content distribution. For migrating data from On Premises to AWS, the major aspect for considerations are amount of data and network speed data security in transit existing application knowledge for recreation NOTE: Topic mainly for Professional Exam Only 1 Application & Database Migration Services 1-1 AWS EC2 VM Import/Export allows easy import of virtual machine images from existing environment to EC2 instances and export them back to on-premises environment allows leveraging of existing investments in the virtual machines, built to meet compliance requirements, configuration management and IT security by bringing those virtual machines into EC2 as ready-to-use instances Common usages include Migrate Existing Applications and Workloads to EC2, allows to preserve software and settings that configured in the existing VMs Copy Your VM Image Catalog to Amazon EC2 Create a Disaster Recovery Repository for your VM images 1-2 AWS Server Migration Service (SMS) is an agentless service which makes it easier and faster to migrate thousands of on-premises workloads to AWS. helps automate, schedule, and track incremental replications of live server volumes, making it easier to coordinate large-scale server migrations. currently supports migration of virtual machines from VMware vSphere, Windows Hyper-V and Azure VM to AWS supports migrating Windows Server 2003, 2008, 2012, and 2016, and Windows 7, 8, and 10; Red Hat Enterprise Linux (RHEL), SUSE/SLES, CentOS, Ubuntu, Oracle Linux, Fedora, and Debian Linux OS replicates each server volume, which is saved as a new AMI, which can be launched as an EC2 instance is a significant enhancement of EC2 VM Import/Export service is used to Re-host 1-3 AWS Database Migration Service (DMS) helps migrate databases to AWS quickly and securely. source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle or Microsoft SQL Server to Amazon Aurora. monitors for replication tasks, network or host failures, and automatically provisions a host replacement in case of failures that can\u2019t be repaired supports both one-time data migration into RDS and EC2-based databases as well as for continuous data replication supports continuous replication of the data with high availability and consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift and Amazon S3 provides free AWS Schema Conversion Tool (SCT) that automates the conversion of Oracle PL/SQL and SQL Server T-SQL code to equivalent code in the Amazon Aurora / MySQL dialect of SQL or the equivalent PL/pgSQL code in PostgreSQL 1-4 AWS Application Discovery Service helps enterprise customers plan migration projects by gathering information about their on-premises data centers. helps enterprise customers plan migration projects by gathering information about their on-premises data centers provides protection for the collected data by encrypting it both in transit to AWS and at rest within the Application Discovery Service data store. 2 Data Transfer Services 2-1 VPN connection utilizes IPSec to establish encrypted network connectivity between on-premises network and VPC over the Internet. connections can be configured in minutes and a good solution for an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. still requires internet and be configured using VGW and CGW 2-2 AWS Direct Connect provides a dedicated physical connection between the corporate network and AWS Direct Connect location with no data transfer over the Internet . helps bypass Internet service providers (ISPs) in the network path helps reduce network costs, increase bandwidth throughput, and provide a more consistent network experience than with Internet-based connection takes time to setup and involves third parties are not redundant and would need another direct connect connection or a VPN connection Security provides a dedicated physical connection without internet For additional security can be used with VPN 2-3 AWS Import/Export (upgraded to Snowball) accelerates moving large amounts of data into and out of AWS using secure Snowball appliances AWS transfers the data directly onto and off of the storage devices using Amazon\u2019s high-speed internal network, bypassing the Internet Data Migration for significant data size, AWS Import/Export is faster than Internet transfer is and more cost-effective than upgrading the connectivity if loading the data over the Internet would take a week or more, AWS Import/Export should be considered data from appliances can be imported to S3, Glacier and EBS volumes and exported from S3 not suitable for applications that cannot tolerate offline transfer time Security Snowball uses an industry-standard Trusted Platform Module (TPM) that has a dedicated processor designed to detect any unauthorized modifications to the hardware, firmware, or software to physically secure the AWS Snowball device. 2-4 Snow Family AWS Snowball is a petabyte-scale data transfer service built around a secure suitcase-sized device that moves data into and out of the AWS Cloud quickly and efficiently. transfers the data to S3 bucket transfer times are about a week from start to finish. are commonly used to ship terabytes or petabytes of analytics data, healthcare and life sciences data, video libraries, image repositories, backups, and archives as part of data center shutdown, tape replacement, or application migration projects. AWS Snowball Edge devices contain slightly larger capacity and an embedded computing platform that helps perform simple processing tasks. can be rack shelved and may also be clustered together, making it simpler to collect and store data in extremely remote locations. commonly used in environments with intermittent connectivity (such as manufacturing, industrial, and transportation); or in extremely remote locations (such as military or maritime operations) before shipping them back to AWS data centers. delivers serverless computing applications at the network edge using AWS Greengrass and Lambda functions. common use cases include capturing IoT sensor streams, on-the-fly media transcoding, image compression, metrics aggregation and industrial control signaling and alarming. AWS Snowmobile moves up to 100PB of data (equivalent to 1,250 AWS Snowball devices) in a 45-foot long ruggedized shipping container and is ideal for multi-petabyte or Exabyte-scale digital media migrations and datacenter shutdowns. arrives at the customer site and appears as a network-attached data store for more secure, high-speed data transfer. After data is transferred to Snowmobile, it is driven back to an AWS Region where the data is loaded into S3. is tamper-resistant, waterproof, and temperature controlled with multiple layers of logical and physical security \u2014 including encryption, fire suppression, dedicated security personnel, GPS tracking, alarm monitoring, 24/7 video surveillance, and an escort security vehicle during transit. 2-5 AWS Storage Gateway connects an on-premises software appliance with cloud-based storage to provide seamless and secure integration between an organization\u2019s on-premises IT environment and the AWS storage infrastructure provides low-latency performance by maintaining frequently accessed data on-premises while securely storing all of the data encrypted in S3 or Glacier. for disaster recovery scenarios, Storage Gateway, together with EC2, can serve as a cloud-hosted solution that mirrors the entire production environment Data Migration with gateway-cached volumes, S3 can be used to hold primary data while frequently accessed data is cached locally for faster access reducing the need to scale on premises storage infrastructure with gateway-stored volumes, entire data is stored locally while asynchronously backing up data to S3 with gateway-VTL, offline data archiving can be performed by presenting existing backup application with an iSCSI-based VTL consisting of a virtual media changer and virtual tape drives Security Encrypts all data in transit to and from AWS by using SSL/TLS. All data in AWS Storage Gateway is encrypted at rest using AES-256. Authentication between the gateway and iSCSI initiators can be secured by using Challenge-Handshake Authentication Protocol (CHAP). 2-6 S3 Data Transfer Files up to 5GB can be transferred using single operation Multipart uploads can be used to upload files up to 5 TB and speed up data uploads by dividing the file into multiple parts transfer rate still limited by the network speed Security Data in transit can be secured by using SSL/TLS or client-side encryption. Encrypt data at-rest by performing server-side encryption using Amazon S3-Managed Keys (SSE-S3), AWS Key Management Service (KMS)-Managed Keys (SSE-KMS), or Customer Provided Keys (SSE-C). Or by performing client-side encryption using AWS KMS\u2013Managed Customer Master Key (CMK) or Client-Side Master Key. 3 AWS Certification Exam Your must architect the migration of a web application to AWS. The application consists of Linux web servers running a custom web server. You are required to save the logs generated from the application to a durable location. What options could you select to migrate the application to AWS? (Choose 2) Create an AWS Elastic Beanstalk application using the custom web server platform. Specify the web server executable and the application project and source files. Enable log file rotation to Amazon Simple Storage Service (S3). (EB does not work with Custom server executable) Create Dockerfile for the application. Create an AWS OpsWorks stack consisting of a custom layer. Create custom recipes to install Docker and to deploy your Docker container using the Dockerfile. Create custom recipes to install and configure the application to publish the logs to Amazon CloudWatch Logs (although this is one of the option, the last sentence mentions configure the application to push the logs to S3, which would need changes to application as it needs to use SDK or CLI) Create Dockerfile for the application. Create an AWS OpsWorks stack consisting of a Docker layer that uses the Dockerfile. Create custom recipes to install and configure Amazon Kinesis to publish the logs into Amazon CloudWatch. (Kinesis not needed) Create a Dockerfile for the application. Create an AWS Elastic Beanstalk application using the Docker platform and the Dockerfile. Enable logging the Docker configuration to automatically publish the application logs. Enable log file rotation to Amazon S3. (Use Docker configuration with awslogs and EB with Docker) Use VM import/Export to import a virtual machine image of the server into AWS as an AMI. Create an Amazon Elastic Compute Cloud (EC2) instance from AMI, and install and configure the Amazon CloudWatch Logs agent. Create a new AMI from the instance. Create an AWS Elastic Beanstalk application using the AMI platform and the new AMI. (Use VM Import/Export to create AMI and CloudWatch logs agent to log) Your company hosts an on-premises legacy engineering application with 900GB of data shared via a central file server. The engineering data consists of thousands of individual files ranging in size from megabytes to multiple gigabytes. Engineers typically modify 5-10 percent of the files a day. Your CTO would like to migrate this application to AWS, but only if the application can be migrated over the weekend to minimize user downtime. You calculate that it will take a minimum of 48 hours to transfer 900GB of data using your company\u2019s existing 45-Mbps Internet connection. After replicating the application\u2019s environment in AWS, which option will allow you to move the application\u2019s data to AWS without losing any data and within the given timeframe? Copy the data to Amazon S3 using multiple threads and multi-part upload for large files over the weekend, and work in parallel with your developers to reconfigure the replicated application environment to leverage Amazon S3 to serve the engineering files. (Still limited by 45 Mbps speed with minimum 48 hours when utilized to max) Sync the application data to Amazon S3 starting a week before the migration, on Friday morning perform a final sync, and copy the entire data set to your AWS file server after the sync completes. (Works best as the data changes can be propagated over the week and are fractional and downtime would be know ) Copy the application data to a 1-TB USB drive on Friday and immediately send overnight, with Saturday delivery, the USB drive to AWS Import/Export to be imported as an EBS volume, mount the resulting EBS volume to your AWS file server on Sunday. (Downtime is not known when the data upload would be done, although Amazon says the same day the package is received) Leverage the AWS Storage Gateway to create a Gateway-Stored volume. On Friday copy the application data to the Storage Gateway volume. After the data has been copied, perform a snapshot of the volume and restore the volume as an EBS volume to be attached to your AWS file server on Sunday. (Still uses the internet) You are tasked with moving a legacy application from a virtual machine running inside your datacenter to an Amazon VPC. Unfortunately this app requires access to a number of on-premises services and no one who configured the app still works for your company. Even worse there\u2019s no documentation for it. What will allow the application running inside the VPC to reach back and access its internal dependencies without being reconfigured? (Choose 3 answers) An AWS Direct Connect link between the VPC and the network housing the internal services An Internet Gateway to allow a VPN connection. (Virtual and Customer gateway is needed) An Elastic IP address on the VPC instance An IP address space that does not conflict with the one on-premises Entries in Amazon Route 53 that allow the Instance to resolve its dependencies\u2019 IP addresses A VM Import of the current virtual machine An enterprise runs 103 line-of-business applications on virtual machines in an on-premises data center. Many of the applications are simple PHP, Java, or Ruby web applications, are no longer actively developed, and serve little traffic. Which approach should be used to migrate these applications to AWS with the LOWEST infrastructure costs? Deploy the applications to single-instance AWS Elastic Beanstalk environments without a load balancer. Use AWS SMS to create AMIs for each virtual machine and run them in Amazon EC2. Convert each application to a Docker image and deploy to a small Amazon ECS cluster behind an Application Load Balancer. Use VM Import/Export to create AMIs for each virtual machine and run them in single-instance AWS Elastic Beanstalk environments by configuring a custom image.","title":"L1 AWS Cloud Migration Services"},{"location":"chap10/1aws_cms/#l1-aws-cloud-migration-services","text":"","title":"L1 AWS Cloud Migration Services"},{"location":"chap10/1aws_cms/#1-aws-cloud-migration-services","text":"AWS Cloud Migration services help to address a lot of common use cases such as cloud migration, disaster recovery, data center decommission, and content distribution. For migrating data from On Premises to AWS, the major aspect for considerations are amount of data and network speed data security in transit existing application knowledge for recreation NOTE: Topic mainly for Professional Exam Only","title":"1 AWS Cloud Migration Services"},{"location":"chap10/1aws_cms/#1-application-database-migration-services","text":"","title":"1 Application &amp; Database Migration Services"},{"location":"chap10/1aws_cms/#1-1-aws-ec2-vm-importexport","text":"allows easy import of virtual machine images from existing environment to EC2 instances and export them back to on-premises environment allows leveraging of existing investments in the virtual machines, built to meet compliance requirements, configuration management and IT security by bringing those virtual machines into EC2 as ready-to-use instances Common usages include Migrate Existing Applications and Workloads to EC2, allows to preserve software and settings that configured in the existing VMs Copy Your VM Image Catalog to Amazon EC2 Create a Disaster Recovery Repository for your VM images","title":"1-1 AWS EC2 VM Import/Export"},{"location":"chap10/1aws_cms/#1-2-aws-server-migration-service-sms","text":"is an agentless service which makes it easier and faster to migrate thousands of on-premises workloads to AWS. helps automate, schedule, and track incremental replications of live server volumes, making it easier to coordinate large-scale server migrations. currently supports migration of virtual machines from VMware vSphere, Windows Hyper-V and Azure VM to AWS supports migrating Windows Server 2003, 2008, 2012, and 2016, and Windows 7, 8, and 10; Red Hat Enterprise Linux (RHEL), SUSE/SLES, CentOS, Ubuntu, Oracle Linux, Fedora, and Debian Linux OS replicates each server volume, which is saved as a new AMI, which can be launched as an EC2 instance is a significant enhancement of EC2 VM Import/Export service is used to Re-host","title":"1-2 AWS Server Migration Service (SMS)"},{"location":"chap10/1aws_cms/#1-3-aws-database-migration-service-dms","text":"helps migrate databases to AWS quickly and securely. source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle or Microsoft SQL Server to Amazon Aurora. monitors for replication tasks, network or host failures, and automatically provisions a host replacement in case of failures that can\u2019t be repaired supports both one-time data migration into RDS and EC2-based databases as well as for continuous data replication supports continuous replication of the data with high availability and consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift and Amazon S3 provides free AWS Schema Conversion Tool (SCT) that automates the conversion of Oracle PL/SQL and SQL Server T-SQL code to equivalent code in the Amazon Aurora / MySQL dialect of SQL or the equivalent PL/pgSQL code in PostgreSQL","title":"1-3 AWS Database Migration Service (DMS)"},{"location":"chap10/1aws_cms/#1-4-aws-application-discovery-service","text":"helps enterprise customers plan migration projects by gathering information about their on-premises data centers. helps enterprise customers plan migration projects by gathering information about their on-premises data centers provides protection for the collected data by encrypting it both in transit to AWS and at rest within the Application Discovery Service data store.","title":"1-4 AWS Application Discovery Service"},{"location":"chap10/1aws_cms/#2-data-transfer-services","text":"","title":"2 Data Transfer Services"},{"location":"chap10/1aws_cms/#2-1-vpn","text":"connection utilizes IPSec to establish encrypted network connectivity between on-premises network and VPC over the Internet. connections can be configured in minutes and a good solution for an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. still requires internet and be configured using VGW and CGW","title":"2-1 VPN"},{"location":"chap10/1aws_cms/#2-2-aws-direct-connect","text":"provides a dedicated physical connection between the corporate network and AWS Direct Connect location with no data transfer over the Internet . helps bypass Internet service providers (ISPs) in the network path helps reduce network costs, increase bandwidth throughput, and provide a more consistent network experience than with Internet-based connection takes time to setup and involves third parties are not redundant and would need another direct connect connection or a VPN connection Security provides a dedicated physical connection without internet For additional security can be used with VPN","title":"2-2 AWS Direct Connect"},{"location":"chap10/1aws_cms/#2-3-aws-importexport-upgraded-to-snowball","text":"accelerates moving large amounts of data into and out of AWS using secure Snowball appliances AWS transfers the data directly onto and off of the storage devices using Amazon\u2019s high-speed internal network, bypassing the Internet Data Migration for significant data size, AWS Import/Export is faster than Internet transfer is and more cost-effective than upgrading the connectivity if loading the data over the Internet would take a week or more, AWS Import/Export should be considered data from appliances can be imported to S3, Glacier and EBS volumes and exported from S3 not suitable for applications that cannot tolerate offline transfer time Security Snowball uses an industry-standard Trusted Platform Module (TPM) that has a dedicated processor designed to detect any unauthorized modifications to the hardware, firmware, or software to physically secure the AWS Snowball device.","title":"2-3 AWS Import/Export (upgraded to Snowball)"},{"location":"chap10/1aws_cms/#2-4-snow-family","text":"AWS Snowball is a petabyte-scale data transfer service built around a secure suitcase-sized device that moves data into and out of the AWS Cloud quickly and efficiently. transfers the data to S3 bucket transfer times are about a week from start to finish. are commonly used to ship terabytes or petabytes of analytics data, healthcare and life sciences data, video libraries, image repositories, backups, and archives as part of data center shutdown, tape replacement, or application migration projects. AWS Snowball Edge devices contain slightly larger capacity and an embedded computing platform that helps perform simple processing tasks. can be rack shelved and may also be clustered together, making it simpler to collect and store data in extremely remote locations. commonly used in environments with intermittent connectivity (such as manufacturing, industrial, and transportation); or in extremely remote locations (such as military or maritime operations) before shipping them back to AWS data centers. delivers serverless computing applications at the network edge using AWS Greengrass and Lambda functions. common use cases include capturing IoT sensor streams, on-the-fly media transcoding, image compression, metrics aggregation and industrial control signaling and alarming. AWS Snowmobile moves up to 100PB of data (equivalent to 1,250 AWS Snowball devices) in a 45-foot long ruggedized shipping container and is ideal for multi-petabyte or Exabyte-scale digital media migrations and datacenter shutdowns. arrives at the customer site and appears as a network-attached data store for more secure, high-speed data transfer. After data is transferred to Snowmobile, it is driven back to an AWS Region where the data is loaded into S3. is tamper-resistant, waterproof, and temperature controlled with multiple layers of logical and physical security \u2014 including encryption, fire suppression, dedicated security personnel, GPS tracking, alarm monitoring, 24/7 video surveillance, and an escort security vehicle during transit.","title":"2-4 Snow Family"},{"location":"chap10/1aws_cms/#2-5-aws-storage-gateway","text":"connects an on-premises software appliance with cloud-based storage to provide seamless and secure integration between an organization\u2019s on-premises IT environment and the AWS storage infrastructure provides low-latency performance by maintaining frequently accessed data on-premises while securely storing all of the data encrypted in S3 or Glacier. for disaster recovery scenarios, Storage Gateway, together with EC2, can serve as a cloud-hosted solution that mirrors the entire production environment Data Migration with gateway-cached volumes, S3 can be used to hold primary data while frequently accessed data is cached locally for faster access reducing the need to scale on premises storage infrastructure with gateway-stored volumes, entire data is stored locally while asynchronously backing up data to S3 with gateway-VTL, offline data archiving can be performed by presenting existing backup application with an iSCSI-based VTL consisting of a virtual media changer and virtual tape drives Security Encrypts all data in transit to and from AWS by using SSL/TLS. All data in AWS Storage Gateway is encrypted at rest using AES-256. Authentication between the gateway and iSCSI initiators can be secured by using Challenge-Handshake Authentication Protocol (CHAP).","title":"2-5 AWS Storage Gateway"},{"location":"chap10/1aws_cms/#2-6-s3","text":"Data Transfer Files up to 5GB can be transferred using single operation Multipart uploads can be used to upload files up to 5 TB and speed up data uploads by dividing the file into multiple parts transfer rate still limited by the network speed Security Data in transit can be secured by using SSL/TLS or client-side encryption. Encrypt data at-rest by performing server-side encryption using Amazon S3-Managed Keys (SSE-S3), AWS Key Management Service (KMS)-Managed Keys (SSE-KMS), or Customer Provided Keys (SSE-C). Or by performing client-side encryption using AWS KMS\u2013Managed Customer Master Key (CMK) or Client-Side Master Key.","title":"2-6 S3"},{"location":"chap10/1aws_cms/#3-aws-certification-exam","text":"Your must architect the migration of a web application to AWS. The application consists of Linux web servers running a custom web server. You are required to save the logs generated from the application to a durable location. What options could you select to migrate the application to AWS? (Choose 2) Create an AWS Elastic Beanstalk application using the custom web server platform. Specify the web server executable and the application project and source files. Enable log file rotation to Amazon Simple Storage Service (S3). (EB does not work with Custom server executable) Create Dockerfile for the application. Create an AWS OpsWorks stack consisting of a custom layer. Create custom recipes to install Docker and to deploy your Docker container using the Dockerfile. Create custom recipes to install and configure the application to publish the logs to Amazon CloudWatch Logs (although this is one of the option, the last sentence mentions configure the application to push the logs to S3, which would need changes to application as it needs to use SDK or CLI) Create Dockerfile for the application. Create an AWS OpsWorks stack consisting of a Docker layer that uses the Dockerfile. Create custom recipes to install and configure Amazon Kinesis to publish the logs into Amazon CloudWatch. (Kinesis not needed) Create a Dockerfile for the application. Create an AWS Elastic Beanstalk application using the Docker platform and the Dockerfile. Enable logging the Docker configuration to automatically publish the application logs. Enable log file rotation to Amazon S3. (Use Docker configuration with awslogs and EB with Docker) Use VM import/Export to import a virtual machine image of the server into AWS as an AMI. Create an Amazon Elastic Compute Cloud (EC2) instance from AMI, and install and configure the Amazon CloudWatch Logs agent. Create a new AMI from the instance. Create an AWS Elastic Beanstalk application using the AMI platform and the new AMI. (Use VM Import/Export to create AMI and CloudWatch logs agent to log) Your company hosts an on-premises legacy engineering application with 900GB of data shared via a central file server. The engineering data consists of thousands of individual files ranging in size from megabytes to multiple gigabytes. Engineers typically modify 5-10 percent of the files a day. Your CTO would like to migrate this application to AWS, but only if the application can be migrated over the weekend to minimize user downtime. You calculate that it will take a minimum of 48 hours to transfer 900GB of data using your company\u2019s existing 45-Mbps Internet connection. After replicating the application\u2019s environment in AWS, which option will allow you to move the application\u2019s data to AWS without losing any data and within the given timeframe? Copy the data to Amazon S3 using multiple threads and multi-part upload for large files over the weekend, and work in parallel with your developers to reconfigure the replicated application environment to leverage Amazon S3 to serve the engineering files. (Still limited by 45 Mbps speed with minimum 48 hours when utilized to max) Sync the application data to Amazon S3 starting a week before the migration, on Friday morning perform a final sync, and copy the entire data set to your AWS file server after the sync completes. (Works best as the data changes can be propagated over the week and are fractional and downtime would be know ) Copy the application data to a 1-TB USB drive on Friday and immediately send overnight, with Saturday delivery, the USB drive to AWS Import/Export to be imported as an EBS volume, mount the resulting EBS volume to your AWS file server on Sunday. (Downtime is not known when the data upload would be done, although Amazon says the same day the package is received) Leverage the AWS Storage Gateway to create a Gateway-Stored volume. On Friday copy the application data to the Storage Gateway volume. After the data has been copied, perform a snapshot of the volume and restore the volume as an EBS volume to be attached to your AWS file server on Sunday. (Still uses the internet) You are tasked with moving a legacy application from a virtual machine running inside your datacenter to an Amazon VPC. Unfortunately this app requires access to a number of on-premises services and no one who configured the app still works for your company. Even worse there\u2019s no documentation for it. What will allow the application running inside the VPC to reach back and access its internal dependencies without being reconfigured? (Choose 3 answers) An AWS Direct Connect link between the VPC and the network housing the internal services An Internet Gateway to allow a VPN connection. (Virtual and Customer gateway is needed) An Elastic IP address on the VPC instance An IP address space that does not conflict with the one on-premises Entries in Amazon Route 53 that allow the Instance to resolve its dependencies\u2019 IP addresses A VM Import of the current virtual machine An enterprise runs 103 line-of-business applications on virtual machines in an on-premises data center. Many of the applications are simple PHP, Java, or Ruby web applications, are no longer actively developed, and serve little traffic. Which approach should be used to migrate these applications to AWS with the LOWEST infrastructure costs? Deploy the applications to single-instance AWS Elastic Beanstalk environments without a load balancer. Use AWS SMS to create AMIs for each virtual machine and run them in Amazon EC2. Convert each application to a Docker image and deploy to a small Amazon ECS cluster behind an Application Load Balancer. Use VM Import/Export to create AMIs for each virtual machine and run them in single-instance AWS Elastic Beanstalk environments by configuring a custom image.","title":"3 AWS Certification Exam"},{"location":"chap10/2AWS_IE/","text":"L2 AWS Import/Export 1 AWS Import/Export Disk AWS Import/Export accelerates moving large amounts of data into and out of AWS using portable storage devices for transport AWS transfers the data directly onto and off of storage devices using Amazon\u2019s high-speed internal network, bypassing the Internet , and can be much faster and more cost effective than upgrading connectivity. AWS Import/Export can be implemented in two different ways 1-1 AWS Import/Export Disk (Disk) originally the only service offered by AWS for data transfer by mail Disk supports transfers data directly onto and off of storage devices you own using the Amazon high-speed internal network 1-2 AWS Snowball is generally faster and cheaper to use than Disk for importing data into Amazon S3 AWS Import/Export supports importing data to several types of AWS storage, including EBS snapshots, S3 buckets, and Glacier vaults. exporting data out from S3 only Data load typically begins the next business day after the storage device arrives at AWS and after the data export or import completes, the storage device is returned 1-3 Ideal Usage Patterns AWS Import/Export is ideal for transferring large amounts of data in and out of the AWS cloud, especially in cases where transferring the data over the Internet would be too slow (a week or more) or too costly. Common use cases include first time migration \u2013 initial data upload to AWS content distribution or regular data interchange to/from your customers or business associates, off-site backup \u2013 transfer to Amazon S3 or Amazon Glacier for off-site backup and archival storage, and disaster recovery \u2013 quick retrieval (export) of large backups from Amazon S3 or Amazon Glacier 2 AWS Import/Export Disk Jobs 2-1 AWS Import/Export jobs can be created in 2 steps Submit a Job request to AWS where each job corresponds to exactly one storage device Send your storage device to AWS , which after the data is uploaded or downloaded is returned back 2-2 AWS Import/Export jobs can be created using a command line tool, which requires no programming or programmatically using the AWS SDK for Java or the REST API to send requests to AWS or even through third party tools 2-3 AWS Import/Export Data Encrption supports data encryption methods PIN-code encryption, Hardware-based device encryption that uses a physical PIN pad for access to the data. TrueCrypt software encryption, Disk encryption using TrueCrypt, which is an open-source encryption application. Creating an import or export job with encryption requires providing the PIN code or password for the selected encryption method Although is is not mandatory for the data to be encrypted for import jobs, it is highly recommended All export jobs require data encryption can use either hardware encryption or software encryption or both methods. AWS Import/Export supported Job Types Import to S3 Import to Glacier Import to EBS Export to S3 AWS erases the device after every import job prior to return shipping. 3 Guidelines and Limitations AWS Import/Export does not support Server-Side Encryption (SSE) when importing data . Maximum file size of a single file or object to be imported is 5 TB. Files and objects larger than 5 TB won\u2019t be imported. Maximum device capacity is 16 TB for Amazon Simple Storage Service (Amazon S3) and Amazon EBS jobs. Maximum device capacity is 4 TB for Amazon Glacier jobs. AWS Import/Export exports only the latest version from an Amazon S3 bucket that has versioning turned on . 4 AWS Certification You are working with a customer who has 10 TB of archival data that they want to migrate to Amazon Glacier. The customer has a 1-Mbps connection to the Internet. Which service or feature provides the fastest method of getting the data into Amazon Glacier? Amazon Glacier multipart upload AWS Storage Gateway VM Import/Export AWS Import/Export (Normal upload will take ~900 days as the internet max speed is capped)","title":"L2 AWS Import/Export"},{"location":"chap10/2AWS_IE/#l2-aws-importexport","text":"","title":"L2 AWS Import/Export"},{"location":"chap10/2AWS_IE/#1-aws-importexport-disk","text":"AWS Import/Export accelerates moving large amounts of data into and out of AWS using portable storage devices for transport AWS transfers the data directly onto and off of storage devices using Amazon\u2019s high-speed internal network, bypassing the Internet , and can be much faster and more cost effective than upgrading connectivity. AWS Import/Export can be implemented in two different ways","title":"1 AWS Import/Export Disk"},{"location":"chap10/2AWS_IE/#1-1-aws-importexport-disk-disk","text":"originally the only service offered by AWS for data transfer by mail Disk supports transfers data directly onto and off of storage devices you own using the Amazon high-speed internal network","title":"1-1 AWS Import/Export Disk (Disk)"},{"location":"chap10/2AWS_IE/#1-2-aws-snowball","text":"is generally faster and cheaper to use than Disk for importing data into Amazon S3 AWS Import/Export supports importing data to several types of AWS storage, including EBS snapshots, S3 buckets, and Glacier vaults. exporting data out from S3 only Data load typically begins the next business day after the storage device arrives at AWS and after the data export or import completes, the storage device is returned","title":"1-2 AWS Snowball"},{"location":"chap10/2AWS_IE/#1-3-ideal-usage-patterns","text":"AWS Import/Export is ideal for transferring large amounts of data in and out of the AWS cloud, especially in cases where transferring the data over the Internet would be too slow (a week or more) or too costly. Common use cases include first time migration \u2013 initial data upload to AWS content distribution or regular data interchange to/from your customers or business associates, off-site backup \u2013 transfer to Amazon S3 or Amazon Glacier for off-site backup and archival storage, and disaster recovery \u2013 quick retrieval (export) of large backups from Amazon S3 or Amazon Glacier","title":"1-3 Ideal Usage Patterns"},{"location":"chap10/2AWS_IE/#2-aws-importexport-disk-jobs","text":"","title":"2 AWS Import/Export Disk Jobs"},{"location":"chap10/2AWS_IE/#2-1-aws-importexport-jobs-can-be-created-in-2-steps","text":"Submit a Job request to AWS where each job corresponds to exactly one storage device Send your storage device to AWS , which after the data is uploaded or downloaded is returned back","title":"2-1 AWS Import/Export jobs can be created in 2 steps"},{"location":"chap10/2AWS_IE/#2-2-aws-importexport-jobs-can-be-created","text":"using a command line tool, which requires no programming or programmatically using the AWS SDK for Java or the REST API to send requests to AWS or even through third party tools","title":"2-2 AWS Import/Export jobs can be created"},{"location":"chap10/2AWS_IE/#2-3-aws-importexport-data-encrption","text":"supports data encryption methods PIN-code encryption, Hardware-based device encryption that uses a physical PIN pad for access to the data. TrueCrypt software encryption, Disk encryption using TrueCrypt, which is an open-source encryption application. Creating an import or export job with encryption requires providing the PIN code or password for the selected encryption method Although is is not mandatory for the data to be encrypted for import jobs, it is highly recommended All export jobs require data encryption can use either hardware encryption or software encryption or both methods. AWS Import/Export supported Job Types Import to S3 Import to Glacier Import to EBS Export to S3 AWS erases the device after every import job prior to return shipping.","title":"2-3 AWS Import/Export Data Encrption"},{"location":"chap10/2AWS_IE/#3-guidelines-and-limitations","text":"AWS Import/Export does not support Server-Side Encryption (SSE) when importing data . Maximum file size of a single file or object to be imported is 5 TB. Files and objects larger than 5 TB won\u2019t be imported. Maximum device capacity is 16 TB for Amazon Simple Storage Service (Amazon S3) and Amazon EBS jobs. Maximum device capacity is 4 TB for Amazon Glacier jobs. AWS Import/Export exports only the latest version from an Amazon S3 bucket that has versioning turned on .","title":"3 Guidelines and Limitations"},{"location":"chap10/2AWS_IE/#4-aws-certification","text":"You are working with a customer who has 10 TB of archival data that they want to migrate to Amazon Glacier. The customer has a 1-Mbps connection to the Internet. Which service or feature provides the fastest method of getting the data into Amazon Glacier? Amazon Glacier multipart upload AWS Storage Gateway VM Import/Export AWS Import/Export (Normal upload will take ~900 days as the internet max speed is capped)","title":"4 AWS Certification"},{"location":"chap10/3AWS_DTS/","text":"L3 AWS Data Transfer Services 1 AWS Data Transfer Services AWS provides a suite of data transfer services that includes many methods that to migrate your data more effectively. Data Transfer services work both Online and Offline and the usage depends on several factors like the amount of data , the time required, frequency, available bandwidth, and cost. Online data transfer and hybrid cloud storage A network link to the VPC, transfer data to AWS or use S3 for hybrid cloud storage with existing on-premises applications. helps both to lift and shift large datasets once, as well as help you integrate existing process flows like backup and recovery or continuous data streams directly with cloud storage. Offline data migration to S3. use shippable, ruggedized devices are ideal for moving large archives, data lakes, or in situations where bandwidth and data volumes cannot pass over your networks within your desired time frame . 2 Online data transfer 2-1 VPN connect securely between data centers and AWS quick to set up and cost-efficient ideal for small data transfers and connectivity not reliable as still uses shared Internet connection 2-2 Direct Connect provides a dedicated physical connection to accelerate network transfers between data centers and AWS provides reliable data transfer ideal for regular large data transfer needs time to setup is not a cost-efficient solution can be secured using VPN over Direct Connect 2-3 AWS S3 Transfer Acceleration makes public Internet transfers to S3 faster. helps maximize the available bandwidth regardless of distance or varying Internet weather, and there are no special clients or proprietary network protocols. Simply change the endpoint you use with your S3 bucket and acceleration is automatically applied. ideal for recurring jobs that travel across the globe, such as media uploads, backups, and local data processing tasks that are regularly sent to a central location 2-4 AWS DataSync automates moving data between on-premises storage and S3 or Elastic File System (Amazon EFS). automatically handles many of the tasks related to data transfers that can slow down migrations or burden the IT operations, including running your own instances, handling encryption, managing scripts, network optimization, and data integrity validation . helps transfer data at speeds up to 10 times faster than open-source tools. uses AWS Direct Connect or internet links to AWS and is ideal for one-time data migrations, recurring data processing workflows, and automated replication for data protection and recovery. 3 Offline data transfer 3-1 AWS Snowcone AWS Snowcone is portable, rugged, and secure that provides edge computing and data transfer devices. Snowcone can be used to collect, process, and move data to AWS, either offline by shipping the device or online with AWS DataSync . AWS Snowcone stores data securely in edge locations , and can run edge computing workloads that use AWS IoT Greengrass or EC2 instances. Snowcone devices are small and weigh 4.5 lbs. (2.1 kg), so you can carry one in a backpack or fit it in tight spaces for IoT, vehicular, or even drone use cases. 3-2 AWS Snowball AWS Snowball is a data migration and edge computing device that comes in two device options: Compute Optimized Snowball Edge Compute Optimized devices provide 52 vCPUs, 42 terabytes of usable block or object storage, and an optional GPU for use cases such as advanced machine learning and full-motion video analysis in disconnected environments. Storage Optimized. Snowball Edge Storage Optimized devices provide 40 vCPUs of compute capacity coupled with 80 terabytes of usable block or S3-compatible object storage. It is well-suited for local storage and large-scale data transfer. Customers can use these two options for data collection, machine learning and processing, and storage in environments with intermittent connectivity (such as manufacturing, industrial, and transportation) or in extremely remote locations (such as military or maritime operations) before shipping it back to AWS. Snowball devices may also be rack mounted and clustered together to build larger, temporary installations. 3-3 AWS Snowmobile AWS Snowmobile moves up to 100 PB of data in a 45-foot long ruggedized shipping container and is ideal for multi-petabyte or Exabyte-scale digital media migrations and data center shutdowns. A Snowmobile arrives at the customer site and appears as a network-attached data store for more secure, high-speed data transfer. After data is transferred to Snowmobile, it is driven back to an AWS Region where the data is loaded into S3. Snowmobile is tamper-resistant, waterproof, and temperature controlled with multiple layers of logical and physical security \u2013 including encryption, fire suppression, dedicated security personnel, GPS tracking, alarm monitoring, 24/7 video surveillance, and an escort security vehicle during transit. 3-4 Data Transfer Chart \u2013 Bandwidth vs Time 4 AWS Certification Exam An organization is moving non-business-critical applications to AWS while maintaining a mission critical application in an on-premises data center. An on-premises application must share limited confidential information with the applications in AWS. The Internet performance is unpredictable. Which configuration will ensure continued connectivity between sites MOST securely? VPN and a cached storage gateway AWS Snowball Edge VPN Gateway over AWS Direct Connect AWS Direct Connect A company wants to transfer petabyte-scale of data to AWS for their analytics, however are constrained on their internet connectivity? Which AWS service can help them transfer the data quickly? S3 enhanced uploader Snowmobile Snowball Direct Connect A company wants to transfer its video library data, which runs in exabytes, to AWS. Which AWS service can help the company transfer the data? Snowmobile Snowball S3 upload S3 enhanced uploader You are working with a customer who has 100 TB of archival data that they want to migrate to Amazon Glacier. The customer has a 1-Gbps connection to the Internet. Which service or feature provides the fastest method of getting the data into Amazon Glacier? Amazon Glacier multipart upload AWS Storage Gateway VM Import/Export AWS Snowball","title":"L3 AWS Data Transfer Services"},{"location":"chap10/3AWS_DTS/#l3-aws-data-transfer-services","text":"","title":"L3 AWS Data Transfer Services"},{"location":"chap10/3AWS_DTS/#1-aws-data-transfer-services","text":"AWS provides a suite of data transfer services that includes many methods that to migrate your data more effectively. Data Transfer services work both Online and Offline and the usage depends on several factors like the amount of data , the time required, frequency, available bandwidth, and cost.","title":"1 AWS Data Transfer Services"},{"location":"chap10/3AWS_DTS/#online-data-transfer-and-hybrid-cloud-storage","text":"A network link to the VPC, transfer data to AWS or use S3 for hybrid cloud storage with existing on-premises applications. helps both to lift and shift large datasets once, as well as help you integrate existing process flows like backup and recovery or continuous data streams directly with cloud storage.","title":"Online data transfer and hybrid cloud storage"},{"location":"chap10/3AWS_DTS/#offline-data-migration-to-s3","text":"use shippable, ruggedized devices are ideal for moving large archives, data lakes, or in situations where bandwidth and data volumes cannot pass over your networks within your desired time frame .","title":"Offline data migration to S3."},{"location":"chap10/3AWS_DTS/#2-online-data-transfer","text":"","title":"2 Online data transfer"},{"location":"chap10/3AWS_DTS/#2-1-vpn","text":"connect securely between data centers and AWS quick to set up and cost-efficient ideal for small data transfers and connectivity not reliable as still uses shared Internet connection","title":"2-1 VPN"},{"location":"chap10/3AWS_DTS/#2-2-direct-connect","text":"provides a dedicated physical connection to accelerate network transfers between data centers and AWS provides reliable data transfer ideal for regular large data transfer needs time to setup is not a cost-efficient solution can be secured using VPN over Direct Connect","title":"2-2 Direct Connect"},{"location":"chap10/3AWS_DTS/#2-3-aws-s3-transfer-acceleration","text":"makes public Internet transfers to S3 faster. helps maximize the available bandwidth regardless of distance or varying Internet weather, and there are no special clients or proprietary network protocols. Simply change the endpoint you use with your S3 bucket and acceleration is automatically applied. ideal for recurring jobs that travel across the globe, such as media uploads, backups, and local data processing tasks that are regularly sent to a central location","title":"2-3 AWS S3 Transfer Acceleration"},{"location":"chap10/3AWS_DTS/#2-4-aws-datasync","text":"automates moving data between on-premises storage and S3 or Elastic File System (Amazon EFS). automatically handles many of the tasks related to data transfers that can slow down migrations or burden the IT operations, including running your own instances, handling encryption, managing scripts, network optimization, and data integrity validation . helps transfer data at speeds up to 10 times faster than open-source tools. uses AWS Direct Connect or internet links to AWS and is ideal for one-time data migrations, recurring data processing workflows, and automated replication for data protection and recovery.","title":"2-4 AWS DataSync"},{"location":"chap10/3AWS_DTS/#3-offline-data-transfer","text":"","title":"3 Offline data transfer"},{"location":"chap10/3AWS_DTS/#3-1-aws-snowcone","text":"AWS Snowcone is portable, rugged, and secure that provides edge computing and data transfer devices. Snowcone can be used to collect, process, and move data to AWS, either offline by shipping the device or online with AWS DataSync . AWS Snowcone stores data securely in edge locations , and can run edge computing workloads that use AWS IoT Greengrass or EC2 instances. Snowcone devices are small and weigh 4.5 lbs. (2.1 kg), so you can carry one in a backpack or fit it in tight spaces for IoT, vehicular, or even drone use cases.","title":"3-1 AWS Snowcone"},{"location":"chap10/3AWS_DTS/#3-2-aws-snowball","text":"AWS Snowball is a data migration and edge computing device that comes in two device options: Compute Optimized Snowball Edge Compute Optimized devices provide 52 vCPUs, 42 terabytes of usable block or object storage, and an optional GPU for use cases such as advanced machine learning and full-motion video analysis in disconnected environments. Storage Optimized. Snowball Edge Storage Optimized devices provide 40 vCPUs of compute capacity coupled with 80 terabytes of usable block or S3-compatible object storage. It is well-suited for local storage and large-scale data transfer. Customers can use these two options for data collection, machine learning and processing, and storage in environments with intermittent connectivity (such as manufacturing, industrial, and transportation) or in extremely remote locations (such as military or maritime operations) before shipping it back to AWS. Snowball devices may also be rack mounted and clustered together to build larger, temporary installations.","title":"3-2 AWS Snowball"},{"location":"chap10/3AWS_DTS/#3-3-aws-snowmobile","text":"AWS Snowmobile moves up to 100 PB of data in a 45-foot long ruggedized shipping container and is ideal for multi-petabyte or Exabyte-scale digital media migrations and data center shutdowns. A Snowmobile arrives at the customer site and appears as a network-attached data store for more secure, high-speed data transfer. After data is transferred to Snowmobile, it is driven back to an AWS Region where the data is loaded into S3. Snowmobile is tamper-resistant, waterproof, and temperature controlled with multiple layers of logical and physical security \u2013 including encryption, fire suppression, dedicated security personnel, GPS tracking, alarm monitoring, 24/7 video surveillance, and an escort security vehicle during transit.","title":"3-3 AWS Snowmobile"},{"location":"chap10/3AWS_DTS/#3-4-data-transfer-chart-bandwidth-vs-time","text":"","title":"3-4 Data Transfer Chart \u2013 Bandwidth vs Time"},{"location":"chap10/3AWS_DTS/#4-aws-certification-exam","text":"An organization is moving non-business-critical applications to AWS while maintaining a mission critical application in an on-premises data center. An on-premises application must share limited confidential information with the applications in AWS. The Internet performance is unpredictable. Which configuration will ensure continued connectivity between sites MOST securely? VPN and a cached storage gateway AWS Snowball Edge VPN Gateway over AWS Direct Connect AWS Direct Connect A company wants to transfer petabyte-scale of data to AWS for their analytics, however are constrained on their internet connectivity? Which AWS service can help them transfer the data quickly? S3 enhanced uploader Snowmobile Snowball Direct Connect A company wants to transfer its video library data, which runs in exabytes, to AWS. Which AWS service can help the company transfer the data? Snowmobile Snowball S3 upload S3 enhanced uploader You are working with a customer who has 100 TB of archival data that they want to migrate to Amazon Glacier. The customer has a 1-Gbps connection to the Internet. Which service or feature provides the fastest method of getting the data into Amazon Glacier? Amazon Glacier multipart upload AWS Storage Gateway VM Import/Export AWS Snowball","title":"4 AWS Certification Exam"},{"location":"chap11/4Elasticsearch/","text":"L4 AWS Elasticsearch 1 AWS Elasticsearch Elasticsearch Service is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud. Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and clickstream analytics Elasticsearch provides real-time, distributed search and analytics engine ability to provision all the resources for Elasticsearch cluster and launches the cluster easy to use cluster scaling options. Scaling Elasticsearch Service domain by adding or modifying instances, and storage volumes is an online operation that does not require any downtime. provides self-healing clusters, which automatically detects and replaces failed Elasticsearch nodes, reducing the overhead associated with self-managed infrastructures domain snapshots to back up and restore ES domains and replicate domains across AZs data durability enhanced security with IAM access control and security groups node monitoring storage volumes for the data using EBS volumes Multiple geographical locations for your resources, known as regions and Availability Zones ability to span cluster nodes across multiple AZs in the same region, known as zone awareness , for high availability and redundancy. Elasticsearch Service automatically distributes the primary and replica shards across instances in different AZs. dedicated master nodes to improve cluster stability data visualization using the Kibana tool integration with CloudWatch for monitoring ES domain metrics integration with CloudTrail for auditing configuration API calls to ES domains integration with S3, Kinesis, and DynamoDB for loading streaming data ability to handle structured and Unstructured data HTTP Rest APIs 2 Elasticsearch Domains Elasticsearch Service domains are Elasticsearch clusters created using the Elasticsearch Service console, CLI, or API. Each domain is an Elasticsearch cluster in the cloud with the specified compute and storage resources. Enables you to create and delete domains, define infrastructure attributes, and control access and security. Elasticsearch Service automates common administrative tasks, such as performing backups, monitoring instances and patching software once the domain is running 3 Elasticsearch Security Access to Elasticsearch Service management APIs for operations such as creating and scaling domains are controlled with AWS IAM policies . Elasticsearch Service domains can be configured to be accessible with an endpoint within the VPC or a public endpoint accessible to the internet. Network access for VPC endpoints is controlled with security groups and for public endpoints access can be granted or restricted by IP address. Elasticsearch Service provides user authentication via IAM and basic authentication using username and password. Authorization can be granted at the domain level (via Domain Access Policies) as well as at the index, document, and field level (via the fine-grained access control feature powered by Open Distro for Elasticsearch). Authorization can be granted at the domain level (via Domain Access Policies) as well as at the index, document, and field level (via the fine-grained access control feature powered by Open Distro for Elasticsearch). Fine-grained access control feature extends Kibana with read-only views and secure multi-tenant support. Elasticsearch Service supports an integration with Cognito, to allow your end-users to log-in to Kibana through enterprise identity providers such as Microsoft Active Directory using SAML 2.0, Cognito User Pools, and more Elasticsearch Service supports encryption at rest through AWS Key Management Service (KMS), node-to-node encryption over TLS, and the ability to require clients to communicate of HTTPS Encryption at rest encrypts shards, log files, swap files, and automated S3 snapshots. 4 AWS Certification Exam You need to perform ad-hoc analysis on log data, including searching quickly for specific error codes and reference numbers. Which should you evaluate first? AWS Elasticsearch Service (Elasticsearch Service (ES) is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS cloud. Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and click stream analytics) AWS RedShift AWS EMR AWS DynamoDB You are hired as the new head of operations for a SaaS company. Your CTO has asked you to make debugging any part of your entire operation simpler and as fast as possible. She complains that she has no idea what is going on in the complex, service-oriented architecture, because the developers just log to disk, and it\u2019s very hard to find errors in logs on so many services. How can you best meet this requirement and satisfy your CTO? Copy all log files into AWS S3 using a cron job on each instance. Use an S3 Notification Configuration on the PutBucket event and publish events to AWS Lambda. Use the Lambda to analyze logs as soon as they come in and flag issues. Begin using CloudWatch Logs on every service. Stream all Log Groups into S3 objects. Use AWS EMR cluster jobs to perform adhoc MapReduce analysis and write new queries when needed. Copy all log files into AWS S3 using a cron job on each instance. Use an S3 Notification Configuration on the PutBucket event and publish events to AWS Kinesis. Use Apache Spark on AWS EMR to perform at-scale stream processing queries on the log chunks and flag issues. Begin using CloudWatch Logs on every service. Stream all Log Groups into an AWS Elasticsearch Service Domain running Kibana 4 and perform log analysis on a search cluster. (AWS Elasticsearch with Kibana stack is designed specifically for real-time, ad-hoc log analysis and aggregation)","title":"L4 AWS Elasticsearch"},{"location":"chap11/4Elasticsearch/#l4-aws-elasticsearch","text":"","title":"L4 AWS Elasticsearch"},{"location":"chap11/4Elasticsearch/#1-aws-elasticsearch","text":"Elasticsearch Service is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud. Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and clickstream analytics","title":"1 AWS Elasticsearch"},{"location":"chap11/4Elasticsearch/#elasticsearch-provides","text":"real-time, distributed search and analytics engine ability to provision all the resources for Elasticsearch cluster and launches the cluster easy to use cluster scaling options. Scaling Elasticsearch Service domain by adding or modifying instances, and storage volumes is an online operation that does not require any downtime. provides self-healing clusters, which automatically detects and replaces failed Elasticsearch nodes, reducing the overhead associated with self-managed infrastructures domain snapshots to back up and restore ES domains and replicate domains across AZs data durability enhanced security with IAM access control and security groups node monitoring storage volumes for the data using EBS volumes Multiple geographical locations for your resources, known as regions and Availability Zones ability to span cluster nodes across multiple AZs in the same region, known as zone awareness , for high availability and redundancy. Elasticsearch Service automatically distributes the primary and replica shards across instances in different AZs. dedicated master nodes to improve cluster stability data visualization using the Kibana tool integration with CloudWatch for monitoring ES domain metrics integration with CloudTrail for auditing configuration API calls to ES domains integration with S3, Kinesis, and DynamoDB for loading streaming data ability to handle structured and Unstructured data HTTP Rest APIs","title":"Elasticsearch provides"},{"location":"chap11/4Elasticsearch/#2-elasticsearch-domains","text":"Elasticsearch Service domains are Elasticsearch clusters created using the Elasticsearch Service console, CLI, or API. Each domain is an Elasticsearch cluster in the cloud with the specified compute and storage resources. Enables you to create and delete domains, define infrastructure attributes, and control access and security. Elasticsearch Service automates common administrative tasks, such as performing backups, monitoring instances and patching software once the domain is running","title":"2 Elasticsearch Domains"},{"location":"chap11/4Elasticsearch/#3-elasticsearch-security","text":"Access to Elasticsearch Service management APIs for operations such as creating and scaling domains are controlled with AWS IAM policies . Elasticsearch Service domains can be configured to be accessible with an endpoint within the VPC or a public endpoint accessible to the internet. Network access for VPC endpoints is controlled with security groups and for public endpoints access can be granted or restricted by IP address. Elasticsearch Service provides user authentication via IAM and basic authentication using username and password. Authorization can be granted at the domain level (via Domain Access Policies) as well as at the index, document, and field level (via the fine-grained access control feature powered by Open Distro for Elasticsearch). Authorization can be granted at the domain level (via Domain Access Policies) as well as at the index, document, and field level (via the fine-grained access control feature powered by Open Distro for Elasticsearch). Fine-grained access control feature extends Kibana with read-only views and secure multi-tenant support. Elasticsearch Service supports an integration with Cognito, to allow your end-users to log-in to Kibana through enterprise identity providers such as Microsoft Active Directory using SAML 2.0, Cognito User Pools, and more Elasticsearch Service supports encryption at rest through AWS Key Management Service (KMS), node-to-node encryption over TLS, and the ability to require clients to communicate of HTTPS Encryption at rest encrypts shards, log files, swap files, and automated S3 snapshots.","title":"3 Elasticsearch Security"},{"location":"chap11/4Elasticsearch/#4-aws-certification-exam","text":"You need to perform ad-hoc analysis on log data, including searching quickly for specific error codes and reference numbers. Which should you evaluate first? AWS Elasticsearch Service (Elasticsearch Service (ES) is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS cloud. Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and click stream analytics) AWS RedShift AWS EMR AWS DynamoDB You are hired as the new head of operations for a SaaS company. Your CTO has asked you to make debugging any part of your entire operation simpler and as fast as possible. She complains that she has no idea what is going on in the complex, service-oriented architecture, because the developers just log to disk, and it\u2019s very hard to find errors in logs on so many services. How can you best meet this requirement and satisfy your CTO? Copy all log files into AWS S3 using a cron job on each instance. Use an S3 Notification Configuration on the PutBucket event and publish events to AWS Lambda. Use the Lambda to analyze logs as soon as they come in and flag issues. Begin using CloudWatch Logs on every service. Stream all Log Groups into S3 objects. Use AWS EMR cluster jobs to perform adhoc MapReduce analysis and write new queries when needed. Copy all log files into AWS S3 using a cron job on each instance. Use an S3 Notification Configuration on the PutBucket event and publish events to AWS Kinesis. Use Apache Spark on AWS EMR to perform at-scale stream processing queries on the log chunks and flag issues. Begin using CloudWatch Logs on every service. Stream all Log Groups into an AWS Elasticsearch Service Domain running Kibana 4 and perform log analysis on a search cluster. (AWS Elasticsearch with Kibana stack is designed specifically for real-time, ad-hoc log analysis and aggregation)","title":"4 AWS Certification Exam"},{"location":"chap2/10EXAM_cs/","text":"L10 AWS Certification Exam Cheat Sheet 1 AWS Global Infrastructure 1-1 AWS Region, AZs, Edge locations Each region is a separate geographic area, completely independent, isolated from the other regions & helps achieve the greatest possible fault tolerance and stability Communication between regions is across the public Internet Each region has multiple Availability Zones Each AZ is physically isolated, geographically separated from each other and designed as an independent failure zone AZs are connected with low-latency private links (not public internet) Edge locations are locations maintained by AWS through a worldwide network of data centers for the distribution of content to reduce latency. 1-2 AWS Local Zones AWS Local Zones place select AWS services closer to end-users , which allows running highly-demanding applications that require single-digit millisecond latencies to the end-users such as media & entertainment content creation, real-time gaming, machine learning etc. AWS Local Zones provide a high-bandwidth, secure connection between local workloads and those running in the AWS Region, allowing you to seamlessly connect to the full range of in-region services through the same APIs and tool sets. 2 AWS Wavelength AWS infrastructure deployments embed AWS compute and storage services within the telecommunications providers\u2019 datacenters and help seamlessly access the breadth of AWS services in the region . AWS Wavelength brings services to the edge of the 5G network , without leaving the mobile provider\u2019s network reducing the extra network hops, minimizing the latency to connect to an application from a mobile device. 3 AWS Outposts AWS Outposts bring native AWS services, infrastructure, and operating models to virtually any data center, co-location space, or on-premises facility. AWS Outposts is designed for connected environments and can be used to support workloads that need to remain on-premises due to low latency, compliance or local data processing needs . 4 AWS Organizations AWS Organizations offers policy-based management for multiple AWS accounts Organizations allows creation of groups of accounts and then apply policies to those groups Organizations enables you to centrally manage policies across multiple accounts, without requiring custom scripts and manual processes . Organizations helps simplify the billing for multiple accounts by enabling the setup of a single payment method for all the accounts in the organization through consolidated billing 5 Consolidate Billing Paying account with multiple linked accounts Paying account is independent and should be only used for billing purpose Paying account cannot access resources of other accounts unless given exclusively access through Cross Account roles All linked accounts are independent and soft limit of 20 One bill per AWS account provides Volume pricing discount for usage across the accounts allows unused Reserved Instances to be applied across the group Free tier is not applicable across the accounts 6 Tags & Resource Groups are metadata , specified as key/value pairs with the AWS resources are for labelling purposes and helps managing, organizing resources can be inherited when created resources created from Auto Scaling, Cloud Formation, Elastic Beanstalk etc can be used for Cost allocation to categorize and track the AWS costs Conditional Access Control policy to define permission to allow or deny access on resources based on tags Resource Group is a collection of resources that share one or more tags 7 IDS/IPS Promiscuous mode is not allowed , as AWS and Hypervisor will not deliver any traffic to instances this is not specifically addressed to the instance IDS/IPS strategies Host Based Firewall \u2013 Forward Deployed IDS where the IDS itself is installed on the instances Host Based Firewall \u2013 Traffic Replication where IDS agents installed on instances which send/duplicate the data to a centralized IDS system In-Line Firewall \u2013 Inbound IDS/IPS Tier (like a WAF configuration) which identifies and drops suspect packets 8 DDOS Mitigation 8-1 Minimize the Attack surface use ELB/CloudFront/Route 53 to distribute load maintain resources in private subnets and use Bastion servers 8-2 Scale to absorb the attack scaling helps buy time to analyze and respond to an attack auto scaling with ELB to handle increase in load to help absorb attacks CloudFront, Route 53 inherently scales as per the demand 8-3 Safeguard exposed resources user Route 53 for aliases to hide source IPs and Private DNS use CloudFront geo restriction and Origin Access Identity use WAF as part of the infrastructure 8-4 Learn normal behavior (IDS/WAF) analyze and benchmark to define rules on normal behavior use CloudWatch Create a plan for attacks 9 AWS Services Region, AZ, Subnet VPC limitations Services like IAM (user, role, group, SSL certificate), Route 53, STS are Global and available across regions All other AWS services are limited to Region or within Region and do not exclusively copy data across regions unless configured AMI are limited to region and need to be copied over to other region EBS volumes are limited to the Availability Zone, and can be migrated by creating snapshots and copying them to another region Reserved instances (can be migrated to other Availability Zone now) cannot be migrated to another region RDS instances are limited to the region and can be recreated in a different region by either using snapshots or promoting a Read Replica Cluster Placement groups are limited to single Availability Zones Spread Placement groups can span across multiple Availability Zones S3 data is replicated within the region and can be move to another region using cross region replication DynamoDB maintains data within the region can be r eplicated to another region using DynamoDB cross region replication (using DynamoDB streams) or Data Pipeline using EMR (old method) Redshift Cluster span within an Availability Zone only, and can be created in other AZ using snapshots 10 Disaster Recovery Whitepaper RTO is the time it takes after a disruption to restore a business process to its service level and RPO acceptable amount of data loss measured in time before the disaster occurs Techniques ( RTO & RPO reduces and the Cost goes up as we go down) Backup & Restore \u2013 Data is backed up and restored , within nothing running Pilot light \u2013 Only minimal critical service like RDS is running and rest of the services can be recreated and scaled during recovery Warm Standby \u2013 Fully functional site with minimal configuration is available and can be scaled during recovery Multi-Site \u2013 Fully functional site with identical configuration is available and processes the load Services Region and AZ to launch services across multiple facilities EC2 instances with the ability to scale and launch across AZs EBS with Snapshot to recreate volumes in different AZ or region AMI to quickly launch preconfigured EC2 instances ELB and Auto Scaling to scale and launch instances across AZs VPC to create private, isolated section Elastic IP address as static IP address ENI with pre allocated Mac Address Route 53 is highly available and scalable DNS service to distribute traffic across EC2 instances and ELB in different AZs and regions Direct Connect for speed data transfer (takes time to setup and expensive then VPN) S3 and Glacier (with RTO of 3-5 hours) provides durable storage RDS snapshots and Multi AZ support and Read Replicas across regions DynamoDB with cross region replication Redshift snapshots to recreate the cluster Storage Gateway to backup the data in AWS Import/Export to move large amount of data to AWS (if internet speed is the bottleneck) CloudFormation, Elastic Beanstalk and Opsworks as orchestration tools for automation and recreate the infrastructure","title":"L10 AWS Certification Exam Cheat Sheet"},{"location":"chap2/10EXAM_cs/#l10-aws-certification-exam-cheat-sheet","text":"","title":"L10 AWS Certification Exam Cheat Sheet"},{"location":"chap2/10EXAM_cs/#1-aws-global-infrastructure","text":"","title":"1 AWS Global Infrastructure"},{"location":"chap2/10EXAM_cs/#1-1-aws-region-azs-edge-locations","text":"Each region is a separate geographic area, completely independent, isolated from the other regions & helps achieve the greatest possible fault tolerance and stability Communication between regions is across the public Internet Each region has multiple Availability Zones Each AZ is physically isolated, geographically separated from each other and designed as an independent failure zone AZs are connected with low-latency private links (not public internet) Edge locations are locations maintained by AWS through a worldwide network of data centers for the distribution of content to reduce latency.","title":"1-1 AWS Region, AZs, Edge locations"},{"location":"chap2/10EXAM_cs/#1-2-aws-local-zones","text":"AWS Local Zones place select AWS services closer to end-users , which allows running highly-demanding applications that require single-digit millisecond latencies to the end-users such as media & entertainment content creation, real-time gaming, machine learning etc. AWS Local Zones provide a high-bandwidth, secure connection between local workloads and those running in the AWS Region, allowing you to seamlessly connect to the full range of in-region services through the same APIs and tool sets.","title":"1-2 AWS Local Zones"},{"location":"chap2/10EXAM_cs/#2-aws-wavelength","text":"AWS infrastructure deployments embed AWS compute and storage services within the telecommunications providers\u2019 datacenters and help seamlessly access the breadth of AWS services in the region . AWS Wavelength brings services to the edge of the 5G network , without leaving the mobile provider\u2019s network reducing the extra network hops, minimizing the latency to connect to an application from a mobile device.","title":"2 AWS Wavelength"},{"location":"chap2/10EXAM_cs/#3-aws-outposts","text":"AWS Outposts bring native AWS services, infrastructure, and operating models to virtually any data center, co-location space, or on-premises facility. AWS Outposts is designed for connected environments and can be used to support workloads that need to remain on-premises due to low latency, compliance or local data processing needs .","title":"3 AWS Outposts"},{"location":"chap2/10EXAM_cs/#4-aws-organizations","text":"AWS Organizations offers policy-based management for multiple AWS accounts Organizations allows creation of groups of accounts and then apply policies to those groups Organizations enables you to centrally manage policies across multiple accounts, without requiring custom scripts and manual processes . Organizations helps simplify the billing for multiple accounts by enabling the setup of a single payment method for all the accounts in the organization through consolidated billing","title":"4 AWS Organizations"},{"location":"chap2/10EXAM_cs/#5-consolidate-billing","text":"Paying account with multiple linked accounts Paying account is independent and should be only used for billing purpose Paying account cannot access resources of other accounts unless given exclusively access through Cross Account roles All linked accounts are independent and soft limit of 20 One bill per AWS account provides Volume pricing discount for usage across the accounts allows unused Reserved Instances to be applied across the group Free tier is not applicable across the accounts","title":"5 Consolidate Billing"},{"location":"chap2/10EXAM_cs/#6-tags-resource-groups","text":"are metadata , specified as key/value pairs with the AWS resources are for labelling purposes and helps managing, organizing resources can be inherited when created resources created from Auto Scaling, Cloud Formation, Elastic Beanstalk etc can be used for Cost allocation to categorize and track the AWS costs Conditional Access Control policy to define permission to allow or deny access on resources based on tags Resource Group is a collection of resources that share one or more tags","title":"6 Tags &amp; Resource Groups"},{"location":"chap2/10EXAM_cs/#7-idsips","text":"Promiscuous mode is not allowed , as AWS and Hypervisor will not deliver any traffic to instances this is not specifically addressed to the instance IDS/IPS strategies Host Based Firewall \u2013 Forward Deployed IDS where the IDS itself is installed on the instances Host Based Firewall \u2013 Traffic Replication where IDS agents installed on instances which send/duplicate the data to a centralized IDS system In-Line Firewall \u2013 Inbound IDS/IPS Tier (like a WAF configuration) which identifies and drops suspect packets","title":"7 IDS/IPS"},{"location":"chap2/10EXAM_cs/#8-ddos-mitigation","text":"","title":"8 DDOS Mitigation"},{"location":"chap2/10EXAM_cs/#8-1-minimize-the-attack-surface","text":"use ELB/CloudFront/Route 53 to distribute load maintain resources in private subnets and use Bastion servers","title":"8-1 Minimize the Attack surface"},{"location":"chap2/10EXAM_cs/#8-2-scale-to-absorb-the-attack","text":"scaling helps buy time to analyze and respond to an attack auto scaling with ELB to handle increase in load to help absorb attacks CloudFront, Route 53 inherently scales as per the demand","title":"8-2 Scale to absorb the attack"},{"location":"chap2/10EXAM_cs/#8-3-safeguard-exposed-resources","text":"user Route 53 for aliases to hide source IPs and Private DNS use CloudFront geo restriction and Origin Access Identity use WAF as part of the infrastructure","title":"8-3 Safeguard exposed resources"},{"location":"chap2/10EXAM_cs/#8-4-learn-normal-behavior-idswaf","text":"analyze and benchmark to define rules on normal behavior use CloudWatch Create a plan for attacks","title":"8-4 Learn normal behavior (IDS/WAF)"},{"location":"chap2/10EXAM_cs/#9-aws-services-region-az-subnet-vpc-limitations","text":"Services like IAM (user, role, group, SSL certificate), Route 53, STS are Global and available across regions All other AWS services are limited to Region or within Region and do not exclusively copy data across regions unless configured AMI are limited to region and need to be copied over to other region EBS volumes are limited to the Availability Zone, and can be migrated by creating snapshots and copying them to another region Reserved instances (can be migrated to other Availability Zone now) cannot be migrated to another region RDS instances are limited to the region and can be recreated in a different region by either using snapshots or promoting a Read Replica Cluster Placement groups are limited to single Availability Zones Spread Placement groups can span across multiple Availability Zones S3 data is replicated within the region and can be move to another region using cross region replication DynamoDB maintains data within the region can be r eplicated to another region using DynamoDB cross region replication (using DynamoDB streams) or Data Pipeline using EMR (old method) Redshift Cluster span within an Availability Zone only, and can be created in other AZ using snapshots","title":"9 AWS Services Region, AZ, Subnet VPC limitations"},{"location":"chap2/10EXAM_cs/#10-disaster-recovery-whitepaper","text":"RTO is the time it takes after a disruption to restore a business process to its service level and RPO acceptable amount of data loss measured in time before the disaster occurs Techniques ( RTO & RPO reduces and the Cost goes up as we go down) Backup & Restore \u2013 Data is backed up and restored , within nothing running Pilot light \u2013 Only minimal critical service like RDS is running and rest of the services can be recreated and scaled during recovery Warm Standby \u2013 Fully functional site with minimal configuration is available and can be scaled during recovery Multi-Site \u2013 Fully functional site with identical configuration is available and processes the load Services Region and AZ to launch services across multiple facilities EC2 instances with the ability to scale and launch across AZs EBS with Snapshot to recreate volumes in different AZ or region AMI to quickly launch preconfigured EC2 instances ELB and Auto Scaling to scale and launch instances across AZs VPC to create private, isolated section Elastic IP address as static IP address ENI with pre allocated Mac Address Route 53 is highly available and scalable DNS service to distribute traffic across EC2 instances and ELB in different AZs and regions Direct Connect for speed data transfer (takes time to setup and expensive then VPN) S3 and Glacier (with RTO of 3-5 hours) provides durable storage RDS snapshots and Multi AZ support and Read Replicas across regions DynamoDB with cross region replication Redshift snapshots to recreate the cluster Storage Gateway to backup the data in AWS Import/Export to move large amount of data to AWS (if internet speed is the bottleneck) CloudFormation, Elastic Beanstalk and Opsworks as orchestration tools for automation and recreate the infrastructure","title":"10 Disaster Recovery Whitepaper"},{"location":"chap2/1best_prac/","text":"L1 AWS Best Practices 1\u3001AWS Design Principles 1-1 Scalability While AWS provides virtually unlimited on-demand capacity, the architecture should be designed to take advantage of those resources There are two ways to scale an IT architecture Vertical Scaling takes place through increasing specifications of an individual resource for e.g. updating EC2 instance type with increasing RAM, CPU, IOPS, or networking capabilities will eventually hit a limit, and is not always a cost effective or highly available approach Horizontal Scaling takes place through increasing number of resources for e.g. adding more EC2 instances or EBS volumes can help leverage the elasticity of cloud computing not all the architectures can be designed to distribute their workload to multiple resources applications designed should be stateless , that needs no knowledge of previous interactions and stores no session information capacity can be increased and decreased, after running tasks have been drained State, if needed, can be implemented using Low latency external store, for e.g. DynamoDB, Redis, to maintain state information Session affinity , for e.g. ELB sticky sessions, to bind all the transactions of a session to a specific compute resource. However, it cannot be guaranteed or take advantage of newly added resources for existing sessions Load can be distributed across multiple resources using Push model, for e.g. through ELB where it distributes the load across multiple EC2 instances Pull model, for e.g. through SQS or Kinesis where multiple consumers subscribe and consume Distributed processing, for e.g. using EMR or Kinesis, helps process large amounts of data by dividing task and its data into many small fragments of works 2\u3001Disposable Resources Instead of Fixed Servers Resources need to be treated as temporary disposable resources rather than fixed permanent on-premises resources before AWS focuses on the concept of Immutable infrastructure servers once launched, is never updated throughout its lifetime. updates can be performed on a new server with latest configurations, this ensures resources are always in a consistent (and tested) state and easier rollbacks AWS provides multiple ways to instantiate compute resources in an automated and repeatable way 2-1 Bootstraping scripts to configure and setup for e.g. using data scripts and cloud-init to install software or copy resources and code 2-2 Golden Images a snapshot of a particular state of that resource, faster start times and removes dependencies to configuration services or third-party repositories 2-3 Containers AWS support for docker images through Elastic Beanstalk and ECS Docker allows packaging a piece of software in a Docker Image, which is a standardized unit for software development, containing everything the software needs to run: code, runtime, system tools, system libraries, etc 2-4 Infrastructure as Code AWS assets are programmable, techniques, practices, and tools from software development can be applied to make the whole infrastructure reusable, maintainable, extensible, and testable. AWS provides services like CloudFormation, OpsWorks for deployment 3\u3001Automation AWS provides various automation tools and services which help improve system\u2019s stability, efficiency and time to market. Elastic Beanstalk a PaaS that allows quick application deployment while handling resource provisioning, load balancing, auto scaling, monitoring etc EC2 Auto Recovery creates CloudWatch alarm that monitors an EC2 instance and automatically recovers it if it becomes impaired. A recovered instance is identical to the original instance, including the instance ID, private & Elastic IP addresses, and all instance metadata. Instance is migrated through reboot, in memory contents are lost. Auto Scaling allows maintain application availability and scale the capacity up or down automatically as per defined conditions CloudWatch Alarms allows SNS triggers to be configured when a particular metric goes beyond a specified threshold for a specified number of periods CloudWatch Events allows real-time stream of system events that describe changes in AWS resources OpsWorks allows continuous configuration through lifecycle events that automatically update the instances\u2019 configuration to adapt to environment changes. Events can be used to trigger Chef recipes on each instance to perform specific configuration tasks Lambda Scheduled Events allows Lambda function creation and direct AWS Lambda to execute it on a regular schedule. 4\u3001Loose Coupling AWS helps loose coupled architecture that reduces interdependencies, a change or failure in a component does not cascade to other components 4-1 Asynchronous Integration does not involve direct point-to-point interaction but usually through an intermediate durable storage layer for e.g. SQS, Kinesis decouples the components and introduces additional resiliency suitable for any interaction that doesn\u2019t need an immediate response and an ack that a request has been registered will suffice 4-2 Service Discovery allows new resources to be launched or terminated at any point in time and discovered as well for e.g. using ELB as a single point of contact with hiding the underlying instance details or Route 53 zones to abstract load balancer\u2019s endpoint 4-3 Well-Defined Interfaces allows various components to interact with each other through specific, technology agnostic interfaces for e.g. RESTful apis with API Gateway 5\u3001Databases AWS provides different categories of database technologies 5-1 Relational Databases (RDS) normalizes data into well-defined tabular structures known as tables, which consist of rows and columns provide a powerful query language, flexible indexing capabilities, strong integrity controls, and the ability to combine data from multiple tables in a fast and efficient manner allows vertical scalability by increasing resources and horizontal scalability using Read Replicas for read capacity and sharding or data partitioning for write capacity provides High Availability using Multi-AZ deployment, where data is synchronously replicated 5-2 NoSQL Databases (DynamoDB) provides databases that trade some of the query and transaction capabilities of relational databases for a more flexible data model that seamlessly scales horizontally perform data partitioning and replication to scale both the reads and writes in a horizontal fashion DynamoDB service synchronously replicates data across three facilities in an AWS region to provide fault tolerance in the event of a server failure or Availability Zone disruption 5-3 Data Warehouse (Redshift) Specialized type of relational database, optimized for analysis and reporting of large amounts of data Redshift achieves efficient storage and optimum query performance through a combination of massively parallel processing (MPP), columnar data storage, and targeted data compression encoding schemes Redshift achieves efficient storage and optimum query performance through a combination of massively parallel processing (MPP) , columnar data storage, and targeted data compression encoding schemes Redshift MPP architecture enables increasing performance by increasing the number of nodes in the data warehouse cluster 6\u3001Removing Single Points of Failure AWS provides ways to implement redundancy, automate recovery and reduce disruption at every layer of the architecture AWS supports redundancy in the following ways 6-1 Standby Redundancy When a resource fails, functionality is recovered on a secondary resource using a process called failover . Failover will typically require some time before it completes, and during that period the resource remains unavailable. Secondary resource can either be launched automatically only when needed (to reduce cost), or it can be already running idle (to accelerate failover and minimize disruption) . Standby redundancy is often used for stateful components such as relational databases. 6-2 Active Redundancy requests are distributed to multiple redundant compute resources, if one fails, the rest can simply absorb a larger share of the workload . Compared to standby redundancy, it can achieve better utilization and affect a smaller population when there is a failure. AWS supports replication 6-3 Synchronous replication acknowledges a transaction after it has been durably stored in both the primary location and its replicas. protects data integrity from the event of a primary node failure used to scale read capacity for queries that require the most up-to-date data (strong consistency) . compromises performance and availability 6-4 Asynchronous replication decouples the primary node from its replicas at the expense of introducing replication lag used to horizontally scale the system\u2019s read capacity for queries that can tolerate that replication lag. 6-5 Quorum-based replication combines synchronous and asynchronous replication to overcome the challenges of large-scale distributed database systems Replication to multiple nodes can be managed by defining a minimum number of nodes that must participate in a successful write operation 6-6 AWS provide services to reduce or remove single point of failure Regions, Availability Zones with multiple data centers ELB or Route 53 to configure health checks and mask failure by routing traffic to healthy endpoints Auto Scaling to automatically replace unhealthy nodes EC2 auto-recovery to recover unhealthy impaired nodes S3, DynamoDB with data redundantly stored across multiple facilities Multi-AZ RDS and Read Replicas ElastiCache Redis engine supports replication with automatic failover","title":"L1 AWS Best Practices"},{"location":"chap2/1best_prac/#l1-aws-best-practices","text":"","title":"L1 AWS Best Practices"},{"location":"chap2/1best_prac/#1aws-design-principles","text":"","title":"1\u3001AWS Design Principles"},{"location":"chap2/1best_prac/#1-1-scalability","text":"While AWS provides virtually unlimited on-demand capacity, the architecture should be designed to take advantage of those resources There are two ways to scale an IT architecture Vertical Scaling takes place through increasing specifications of an individual resource for e.g. updating EC2 instance type with increasing RAM, CPU, IOPS, or networking capabilities will eventually hit a limit, and is not always a cost effective or highly available approach Horizontal Scaling takes place through increasing number of resources for e.g. adding more EC2 instances or EBS volumes can help leverage the elasticity of cloud computing not all the architectures can be designed to distribute their workload to multiple resources applications designed should be stateless , that needs no knowledge of previous interactions and stores no session information capacity can be increased and decreased, after running tasks have been drained State, if needed, can be implemented using Low latency external store, for e.g. DynamoDB, Redis, to maintain state information Session affinity , for e.g. ELB sticky sessions, to bind all the transactions of a session to a specific compute resource. However, it cannot be guaranteed or take advantage of newly added resources for existing sessions Load can be distributed across multiple resources using Push model, for e.g. through ELB where it distributes the load across multiple EC2 instances Pull model, for e.g. through SQS or Kinesis where multiple consumers subscribe and consume Distributed processing, for e.g. using EMR or Kinesis, helps process large amounts of data by dividing task and its data into many small fragments of works","title":"1-1 Scalability"},{"location":"chap2/1best_prac/#2disposable-resources-instead-of-fixed-servers","text":"Resources need to be treated as temporary disposable resources rather than fixed permanent on-premises resources before AWS focuses on the concept of Immutable infrastructure servers once launched, is never updated throughout its lifetime. updates can be performed on a new server with latest configurations, this ensures resources are always in a consistent (and tested) state and easier rollbacks AWS provides multiple ways to instantiate compute resources in an automated and repeatable way","title":"2\u3001Disposable Resources Instead of Fixed Servers"},{"location":"chap2/1best_prac/#2-1-bootstraping","text":"scripts to configure and setup for e.g. using data scripts and cloud-init to install software or copy resources and code","title":"2-1 Bootstraping"},{"location":"chap2/1best_prac/#2-2-golden-images","text":"a snapshot of a particular state of that resource, faster start times and removes dependencies to configuration services or third-party repositories","title":"2-2 Golden Images"},{"location":"chap2/1best_prac/#2-3-containers","text":"AWS support for docker images through Elastic Beanstalk and ECS Docker allows packaging a piece of software in a Docker Image, which is a standardized unit for software development, containing everything the software needs to run: code, runtime, system tools, system libraries, etc","title":"2-3 Containers"},{"location":"chap2/1best_prac/#2-4-infrastructure-as-code","text":"AWS assets are programmable, techniques, practices, and tools from software development can be applied to make the whole infrastructure reusable, maintainable, extensible, and testable. AWS provides services like CloudFormation, OpsWorks for deployment","title":"2-4 Infrastructure as Code"},{"location":"chap2/1best_prac/#3automation","text":"AWS provides various automation tools and services which help improve system\u2019s stability, efficiency and time to market. Elastic Beanstalk a PaaS that allows quick application deployment while handling resource provisioning, load balancing, auto scaling, monitoring etc EC2 Auto Recovery creates CloudWatch alarm that monitors an EC2 instance and automatically recovers it if it becomes impaired. A recovered instance is identical to the original instance, including the instance ID, private & Elastic IP addresses, and all instance metadata. Instance is migrated through reboot, in memory contents are lost. Auto Scaling allows maintain application availability and scale the capacity up or down automatically as per defined conditions CloudWatch Alarms allows SNS triggers to be configured when a particular metric goes beyond a specified threshold for a specified number of periods CloudWatch Events allows real-time stream of system events that describe changes in AWS resources OpsWorks allows continuous configuration through lifecycle events that automatically update the instances\u2019 configuration to adapt to environment changes. Events can be used to trigger Chef recipes on each instance to perform specific configuration tasks Lambda Scheduled Events allows Lambda function creation and direct AWS Lambda to execute it on a regular schedule.","title":"3\u3001Automation"},{"location":"chap2/1best_prac/#4loose-coupling","text":"AWS helps loose coupled architecture that reduces interdependencies, a change or failure in a component does not cascade to other components","title":"4\u3001Loose Coupling"},{"location":"chap2/1best_prac/#4-1-asynchronous-integration","text":"does not involve direct point-to-point interaction but usually through an intermediate durable storage layer for e.g. SQS, Kinesis decouples the components and introduces additional resiliency suitable for any interaction that doesn\u2019t need an immediate response and an ack that a request has been registered will suffice","title":"4-1 Asynchronous Integration"},{"location":"chap2/1best_prac/#4-2-service-discovery","text":"allows new resources to be launched or terminated at any point in time and discovered as well for e.g. using ELB as a single point of contact with hiding the underlying instance details or Route 53 zones to abstract load balancer\u2019s endpoint","title":"4-2 Service Discovery"},{"location":"chap2/1best_prac/#4-3-well-defined-interfaces","text":"allows various components to interact with each other through specific, technology agnostic interfaces for e.g. RESTful apis with API Gateway","title":"4-3 Well-Defined Interfaces"},{"location":"chap2/1best_prac/#5databases","text":"AWS provides different categories of database technologies","title":"5\u3001Databases"},{"location":"chap2/1best_prac/#5-1-relational-databases-rds","text":"normalizes data into well-defined tabular structures known as tables, which consist of rows and columns provide a powerful query language, flexible indexing capabilities, strong integrity controls, and the ability to combine data from multiple tables in a fast and efficient manner allows vertical scalability by increasing resources and horizontal scalability using Read Replicas for read capacity and sharding or data partitioning for write capacity provides High Availability using Multi-AZ deployment, where data is synchronously replicated","title":"5-1 Relational Databases (RDS)"},{"location":"chap2/1best_prac/#5-2-nosql-databases-dynamodb","text":"provides databases that trade some of the query and transaction capabilities of relational databases for a more flexible data model that seamlessly scales horizontally perform data partitioning and replication to scale both the reads and writes in a horizontal fashion DynamoDB service synchronously replicates data across three facilities in an AWS region to provide fault tolerance in the event of a server failure or Availability Zone disruption","title":"5-2 NoSQL Databases (DynamoDB)"},{"location":"chap2/1best_prac/#5-3-data-warehouse-redshift","text":"Specialized type of relational database, optimized for analysis and reporting of large amounts of data Redshift achieves efficient storage and optimum query performance through a combination of massively parallel processing (MPP), columnar data storage, and targeted data compression encoding schemes Redshift achieves efficient storage and optimum query performance through a combination of massively parallel processing (MPP) , columnar data storage, and targeted data compression encoding schemes Redshift MPP architecture enables increasing performance by increasing the number of nodes in the data warehouse cluster","title":"5-3 Data Warehouse (Redshift)"},{"location":"chap2/1best_prac/#6removing-single-points-of-failure","text":"AWS provides ways to implement redundancy, automate recovery and reduce disruption at every layer of the architecture AWS supports redundancy in the following ways","title":"6\u3001Removing Single Points of Failure"},{"location":"chap2/1best_prac/#6-1-standby-redundancy","text":"When a resource fails, functionality is recovered on a secondary resource using a process called failover . Failover will typically require some time before it completes, and during that period the resource remains unavailable. Secondary resource can either be launched automatically only when needed (to reduce cost), or it can be already running idle (to accelerate failover and minimize disruption) . Standby redundancy is often used for stateful components such as relational databases.","title":"6-1 Standby Redundancy"},{"location":"chap2/1best_prac/#6-2-active-redundancy","text":"requests are distributed to multiple redundant compute resources, if one fails, the rest can simply absorb a larger share of the workload . Compared to standby redundancy, it can achieve better utilization and affect a smaller population when there is a failure. AWS supports replication","title":"6-2 Active Redundancy"},{"location":"chap2/1best_prac/#6-3-synchronous-replication","text":"acknowledges a transaction after it has been durably stored in both the primary location and its replicas. protects data integrity from the event of a primary node failure used to scale read capacity for queries that require the most up-to-date data (strong consistency) . compromises performance and availability","title":"6-3 Synchronous replication"},{"location":"chap2/1best_prac/#6-4-asynchronous-replication","text":"decouples the primary node from its replicas at the expense of introducing replication lag used to horizontally scale the system\u2019s read capacity for queries that can tolerate that replication lag.","title":"6-4 Asynchronous replication"},{"location":"chap2/1best_prac/#6-5-quorum-based-replication","text":"combines synchronous and asynchronous replication to overcome the challenges of large-scale distributed database systems Replication to multiple nodes can be managed by defining a minimum number of nodes that must participate in a successful write operation","title":"6-5 Quorum-based replication"},{"location":"chap2/1best_prac/#6-6-aws-provide-services-to-reduce-or-remove-single-point-of-failure","text":"Regions, Availability Zones with multiple data centers ELB or Route 53 to configure health checks and mask failure by routing traffic to healthy endpoints Auto Scaling to automatically replace unhealthy nodes EC2 auto-recovery to recover unhealthy impaired nodes S3, DynamoDB with data redundantly stored across multiple facilities Multi-AZ RDS and Read Replicas ElastiCache Redis engine supports replication with automatic failover","title":"6-6 AWS provide services to reduce or remove single point of failure"},{"location":"chap2/2s3_cs/","text":"L2 AWS Storage & Content Delivery Cheat Sheet 1\u3001Simple Storage Service \u2013 S3 provides key-value based object storage with unlimited storage, unlimited objects up to 5 TB for the internet offers an extremely durable, highly available, and infinitely scalable data storage infrastructure at very low costs . is Object-level storage (not a Block level storage) and cannot be used to host OS or dynamic websites (but can work with Javascript SDK) provides durability by redundantly storing objects on multiple facilities within a region S3 resources consist of globally unique buckets with objects and related metadata. The data model is a flat structure with no hierarchies or folders . regularly verifies the integrity of data using checksums and provides the auto-healing capability S3 Replication enables automatic, asynchronous copying of objects across S3 buckets in the same or different AWS regions using SRR or CRR . Replication needs versioning enabled on either side. S3 Transfer Acceleration helps speed data transport over long distances between a client and an S3 bucket using CloudFront edge locations. S3 supports cost-effective Static Website hosting with Client-side scripts . S3 CORS \u2013 Cross-Origin Resource Sharing allows cross-origin access to S3 resources . S3 Access Logs enables tracking access requests to an S3 bucket. S3 notification feature enables notifications to be triggered when certain events happen in the bucket. S3 Inventory helps manage the storage and can be used to audit and report on the replication and encryption status of the objects for business, compliance, and regulatory needs. Requestor Pays help bucket owner to specify that the requester requesting the download will be charged for the download. S3 Batch Operations help perform large-scale batch operations on S3 objects and can perform a single operation on lists of specified S3 objects. Pre-Signed URLs can be used shared for uploading/downloading objects for a limited time w ithout requiring AWS security credentials . Multipart Uploads allows parallel uploads with improved throughput and bandwidth utilization fault tolerance and quick recovery from network issues ability to pause and resume uploads begin an upload before the final object size is known Versioning helps preserve, retrieve, and restore every version of every object protect from unintended overwrites and accidental deletions protects individual files but does NOT protect from Bucket deletion MFA (Multi-Factor Authentication) can be enabled for additional security for the deletion of objects. Integrates with CloudTrail, CloudWatch, and SNS for event notifications S3 Storage Classes S3 Standard default storage class, ideal for frequently accessed data 99.999999999% durability & 99.99% availability Low latency and high throughput performance designed to sustain the loss of data in a two facilities S3 Standard-Infrequent Access (S3 Standard-IA) optimized for long-lived and less frequently accessed data designed to sustain the loss of data in a two facilities 99.999999999% durability & 99.9% availability suitable for objects greater than 128 KB kept for at least 30 days S3 One Zone-Infrequent Access (S3 One Zone-IA) optimized for rapid access, less frequently access data ideal for secondary backups and reproducible data stores data in a single AZ, data stored in this storage class will be lost in the event of AZ destruction . 99.999999999% durability & 99.5% availability S3 Glacier suitable for low cost data archiving, where data access is infrequent provides retrieval time of minutes to several hours Expedited \u2013 1 to 5 minutes Standard \u2013 3 to 5 hours Bulk \u2013 5 to 12 hour s 99.999999999% durability & 99.9% availability Minimum storage duration of 90 days S3 Glacier Deep Archive (S3 Glacier Deep Archive) provides lowest cost data archiving, where data access is infrequent 99.999999999% durability & 99.9% availability provides retrieval time of several (12-48) hours Standard \u2013 12 hours Bulk \u2013 48 hours Minimum storage duration of 180 days supports long-term retention and digital preservation for data that may be accessed once or twice a year Lifecycle Management policies transition to move objects to different storage classes and Glacier expiration to remove objects and object versions can be applied to both current and non-current objects, in case, versioning is enabled . Data Consistency Model provides strong read-after-write consistency for PUT and DELETE requests of objects in the S3 bucket in all AWS Regions updates to a single key are atomic does not currently support object locking for concurrent writes S3 Security IAM policies \u2013 grant users within your own AWS account permission to access S3 resources Bucket and Object ACL \u2013 grant other AWS accounts (not specific users) access to S3 resources Bucket policies \u2013 allows to add or deny permissions across some or all of the objects within a single bucket S3 Access Points simplify data access for any AWS service or customer application that stores data in S3. S3 Glacier Vault Lock helps deploy and enforce compliance controls for individual S3 Glacier vaults with a vault lock policy . S3 VPC Gateway Endpoint enables private connections between a VPC and S3, without requiring that you use an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection . Support SSL encryption of data in transit and data encryption at rest S3 Data Encryption supports data at rest and data in transit encryption Server-Side Encryption SSE-S3 \u2013 encrypts S3 objects using keys handled & managed by AWS SSE-KMS \u2013 leverage AWS Key Management Service to manage encryption keys. KMS provides control and audit trail over the keys. SSE-C \u2013 when you want to manage your own encryption keys. AWS does not store the encryption key. Requires HTTPS. Client-Side Encryption Client library such as the S3 Encryption Client Clients must encrypt data themselves before sending it to S3 Clients must decrypt data themselves when retrieving from S3 Customer fully manages the keys and encryption cycle S3 Best Practices use parallel threads and Multipart upload for faster writes use parallel threads and Range Header GET for faster reads for list operations with a large number of objects, it\u2019s better to build a secondary index in DynamoDB use Versioning to protect from unintended overwrites and deletions , but this does not protect against bucket deletion use VPC S3 Endpoints with VPC to transfer data using Amazon internal network 2\u3001Instance Store provides temporary or ephemeral block-level storage for an EC2 instance is physically attached to the Instance deliver very high random I/O performance , which is a good option when storage with very low latency is needed cannot be dynamically resized data persists when an instance is rebooted data does not persists if the underlying disk drive fails instance stops i.e. if the EBS backed instance with instance store volumes attached is stopped instance terminates can be attached to an EC2 instance only when the instance is launched is ideal for the temporary storage of information that changes frequently , such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. 3\u3001Elastic Block Store \u2013 EBS is virtual network-attached block storage provides highly available , reliable, durable, block-level storage volumes that can be attached to a running instance provides high durability and are redundant in an AZ , as the data is automatically replicated within that AZ to prevent data loss due to any single hardware component failure persists and is independent of EC2 lifecycle multiple volumes can be attached to a single EC2 instance can be detached & attached to another EC2 instance in that same AZ only volumes are Zonal i.e. created in a specific AZ and CAN\u2019T span across AZs snapshots for making volume available to different AZ, create a snapshot of the volume and restore it to a new volume in any AZ within the region for making the volume available to different Region, the snapshot of the volume can be copied to a different region and restored as a volume PIOPS is designed to run transactions applications that require high and consistent IO for e.g. Relation database, NoSQL , etc PIOPS is designed to run transactions applications that require high and consistent IO for e.g. Relation database, NoSQL, etc Multi-Attach enables attaching a single Provisioned IOPS SSD (io1 or io2) volume to multiple instances that are in the same AZ. 4\u3001EBS Encryption allow encryption using the EBS encryption feature. All data stored at rest, disk I/O, and snapshots created from the volume are encrypted. uses 256-bit AES algorithms (AES-256) and an Amazon-managed KMS Snapshots of encrypted EBS volumes are automatically encrypted. 5\u3001EBS Snapshots helps create backups of EBS volumes are incremental occur asynchronously , consume the instance IOPS are regional and CANNOT span across regions can be copied across regions to make it easier to leverage multiple regions for geographical expansion, data center migration, and disaster recovery can be shared by making them public or with specific AWS accounts by modifying the access permissions of the snapshots support EBS encryption Snapshots of encrypted volumes are automatically encrypted Volumes created from encrypted snapshots are automatically encrypted All data in flight between the instance and the volume is encrypted Volumes created from an unencrypted snapshot owned or have access to can be encrypted on the fly. Encrypted snapshot owned or having access to, can be encrypted with a different key during the copy process. can be automated using AWS Data Lifecycle Manager 6\u3001EBS vs Instance Store 6-1 AWS EBS vs Instance Store Overview EC2 instances support two types for block level storage Elastic Block Store (EBS) Instance Store (Ephemeral store) EC2 Instances can be launched using either Elastic Block Store (EBS) or Instance Store volume as root volumes and additional volumes . EC2 instances can be launched by choosing between AMIs backed by EC2 instance store and AMIs backed by EBS . However, AWS recommends using EBS backed AMIs because they launch faster and use persistent storage. 6-2 Instance Store (Ephemeral storage) An Instance store backed instance is an EC2 instance using an Instance store as root device volume created from a template stored in S3 . Instance store volumes access storage from disks that are physically attached to the host computer . When an Instance stored instance is launched, the image that is used to boot the instance is copied to the root volume ( typically sda1 ). Instance store provides temporary block-level storage for instances. Data on an instance store volume persists only during the life of the associated instance; if an instance is stopped or terminated, any data on instance store volumes is lost. 6-3 Key points for Instance store backed Instance Boot time is slower than EBS backed volumes and usually less than 5 min Can be selected as Root Volume and attached as additional volumes Instance store backed Instances can be of a maximum 10GiB volume size Instance store volume can be attached as additional volumes only when the instance is being launched and cannot be attached once the Instance is up and running Instance store backed Instances cannot be stopped , as when stopped and started AWS does not guarantee the instance would be launched in the same host, and hence the data is lost Data on Instance store volume is LOST in the following scenarios: Failure of an underlying drive Stopping an EBS-backed instance where instance stores are attached as additional volumes Termination of the Instance Data on Instance store volume is NOT LOST when the instance is rebooted For EC2 instance store-backed instances AWS recommends to distribute the data on the instance stores across multiple AZs back up critical data from the instance store volumes to persistent storage on a regular basis. AMI creation requires usage on AMI tools and needs to be executed from within the running instance Instance store backed Instances cannot be upgraded 6-3 Elastic Block Store (EBS) An \u201cEBS-backed\u201d instance means that the root device for an instance launched from the AMI is an EBS volume created from an EBS snapshot An EBS volume behaves like a raw, unformatted, external block device that can be attached to a single instance and is not physically attached to the Instance host computer (more like network-attached storage). Volume persists independently from the running life of an instance . After an EBS volume is attached to an instance, you can use it like any other physical hard drive. EBS volume can be detached from one instance and attached to another instance EBS volumes can be created as encrypted volumes using the EBS encryption feature 6-4 Key points for EBS backed Instance Boot time is very fast usually less than a min Can be selected as Root Volume and attached as additional volumes EBS backed Instances can be of maximum 64TiB volume size depending upon the OS EBS volume can be attached as additional volumes when the Instance is launched and even when the Instance is up and running Data on the EBS volume is LOST for EBS Root volume, if Delete On Termination flag is enabled , which is the default. Attached EBS volumes, if the Delete On Termination flag is enabled. It\u2019s disabled, by default. Data on EBS volume is NOT LOST in the following scenarios: Reboot on the Instance Stopping an EBS-backed instance Termination of the Instance for the additional EBS volumes. Additional EBS volumes are detached with their data intact When an EBS-backed instance is in a stopped state, various instance\u2013 and volume-related tasks can be done for e.g. you can modify the properties of the instance, you can change the size of your instance or update the kernel it is using, you can attach your root volume to a different running instance for debugging or any other purpose EBS volumes are AZ scoped and tied to a single AZ in which created EBS volumes are automatically replicated within that zone to prevent data loss due to the failure of any single hardware component AMI creation is easy using a Single command EBS backed Instances can be upgraded for instance type, Kernel, RAM disk, and user data 6-4 EBS vs Instance Store Comparision 6-5 Boot Times EBS-backed AMIs launch faster than EC2 instance store-backed AMIs. When an EC2 instance store-backed AMI is launched, all the parts have to be retrieved from S3 before the instance is available. With an EBS-backed AMI is launched, parts are lazily loaded and only the parts required to boot the instance need to be retrieved from the snapshot before the instance is available. However, the performance of an instance that uses an EBS volume for its root device is slower for a short time while the remaining parts are retrieved from the snapshot and loaded into the volume. When you stop and restart the instance, it launches quickly, because the state is stored in an EBS volume. EC2 EBS-backed (EBS root) instance is stopped, what happens to the data on any ephemeral store volumes ? Data is automatically saved in an EBS volume. Data is unavailable until the instance is restarted. Data will be deleted and will no longer be accessible . Data is automatically saved as an EBS snapshot. When an EC2 instance that is backed by an S3-based AMI is terminated, what happens to the data on the root volume? Data is automatically saved as an EBS snapshot. Data is automatically saved as an EBS volume. Data is unavailable until the instance is restarted. Data is automatically deleted. Which of the following will occur when an EC2 instance in a VPC (Virtual Private Cloud) with an associated Elastic IP is stopped and started? (Choose 2 answers) The Elastic IP will be dissociated from the instance All data on instance-store devices will be lost All data on EBS (Elastic Block Store) devices will be lost The ENI (Elastic Network Interface) is detached The underlying host for the instance is changed Which of the following provides the fastest storage medium? Amazon S3 Amazon EBS using Provisioned IOPS (PIOPS) SSD Instance (ephemeral) store (SSD Instance Storage provides 100,000 IOPS on some instance types, much faster than any network-attached storage) AWS Storage Gateway 7\u3001Glacier suitable for archiving data, where data access is infrequent and a retrieval time of several hours (3 to 5 hours) is acceptable ( Not true anymore with enhancements from AWS ) provides a high durability by storing archive in multiple facilities and multiple devices at a very low cost storage performs regular, systematic data integrity checks and is built to be automatically self healing aggregate files into bigger files before sending them to Glacier and use range retrievals to retrieve partial file and reduce costs improve speed and reliability with multipart upload automatically encrypts the data using AES-256 upload or download data to Glacier via SSL encrypted endpoints 8\u3001EFS fully-managed, easy to set up, scale, and cost-optimize file storage can automatically scale from gigabytes to petabytes of data without needing to provision storage provides managed NFS (network file system) that can be mounted on and accessed by multiple EC2 in multiple AZs simultaneously highly durable, highly scalable and highly available. stores data redundantly across multiple Availability Zones grows and shrinks automatically as files are added and removed, so you there is no need to manage storage procurement or provisioning. expensive (3x gp2), but you pay per use uses the Network File System version 4 (NFS v4) protocol is compatible with all Linux-based AMIs for EC 2, POSIX file system (~Linux) that has a standard file API does not support Windows AMI does not support Windows AMI offers the ability to encrypt data at rest using KMS and in transit. can be accessed from on-premises using an AWS Direct Connect or AWS VPN connection between the on-premises datacenter and VPC . can be accessed concurrently from servers in the on-premises datacenter as well as EC2 instances in the Amazon VPC Performance mode General purpose (default) latency-sensitive use cases (web server, CMS, etc\u2026) Max I/O higher latency, throughput, highly parallel (big data, media processing) Storage Tiers Standard for frequently accessed files ideal for active file system workloads and you pay only for the file system storage you use per month Infrequent access (EFS-IA) a lower cost storage class that\u2019s cost-optimized for files infrequently accessed i.e. not accessed every day cost to retrieve files, lower price to store EFS Lifecycle Management with choosing an age-off policy allows moving files to EFS IA Lifecycle Management automatically moves the data to the EFS IA storage class according to the lifecycle policy. for e.g., you can move files automatically into EFS IA fourteen days of not being accessed . EFS is a shared POSIX system for Linux systems and does not work for Windows 8\u3001Amazon FSx for Windows is a fully managed, highly reliable, and scalable Windows file system share drive supports SMB protocol & Windows NTFS supports Microsoft Active Directory integration, ACLs, user quotas built on SSD, scale up to 10s of GB/s, millions of IOPS, 100s PB of data is accessible from Windows, Linux, and MacOS compute instances can be accessed from the on-premise infrastructure can be configured to be Multi-AZ (high availability) supports encryption of data at rest and in transit provides data deduplication, which enables further cost optimization by removing redundant data. data is backed-up daily to S3 9\u3001Amazon FSx for Lustre provides easy and cost effective way to launch and run the world\u2019s most popular high-performance file system . is a type of parallel distributed file system, for large-scale computing Lustre is derived from \u201cLinux\u201d and \u201ccluster\u201d Machine Learning, High Performance Computing (HPC) esp. Video Processing, Financial Modeling, Electronic Design Automation scales up to 100s GB/s, millions of IOPS, sub-ms latencies seamless integration with S3, it transparently presents S3 objects as files and allows you to write changed data back to S3 . can \u201cread S3\u201d as a file system (through FSx) can write the output of the computations back to S3 (through FSx) supports encryption of data at rest and in transit can be used from on-premise servers 10\u3001CloudFront provides low latency and high data transfer speeds for distribution of static, dynamic web or streaming content to web users delivers the content through a worldwide network of data centers called Edge Locations keeps persistent connections with the origin servers so that the files can be fetched from the origin servers as quickly as possible . dramatically reduces the number of network hops that users\u2019 requests must pass through supports multiple origin server options , like AWS hosted service for e.g. S3, EC2, ELB or an on premise server , which stores the original, definitive version of the objects single distribution can have multiple origin s and Path pattern in a cache behavior determines which requests are routed to the origin supports Web Download distribution and RTMP Streaming distribution Web distribution supports static, dynamic web content, on demand using progressive download & HLS and live streaming video content RTMP supports streaming of media files using Adobe Media Server and the Adobe Real-Time Messaging Protocol (RTMP) ONLY supports HTTPS using either dedicated IP address , which is expensive as dedicated IP address is assigned to each CloudFront edge location Server Name Indication (SNI), which is free but supported by modern browsers only with the domain name available in the request header For E2E HTTPS connection , Viewers -> CloudFront needs either self signed certificate, or certificate issued by CA or ACM CloudFront -> Origin needs certificate issued by ACM for ELB and by CA for other origins Security Origin Access Identity (OAI) can be used to restrict the content from S3 origin to be accessible from CloudFront only supports Geo restriction (Geo-Blocking) to whitelist or blacklist countries that can access the content Signed URLs for RTMP distribution as signed cookies aren\u2019t supported to restrict access to individual files, for e.g., an installation download for your application . users using a client, for e.g. a custom HTTP client , that doesn\u2019t support cookies Signed Cookies provide access to multiple restricted files, for e.g., video part files in HLS format or all of the files in the subscribers\u2019 area of a website. don\u2019t want to change the current URLs Integrates with AWS WAF , a web application firewall that helps protect web applications from attacks by allowing rules configured based on IP addresses, HTTP headers, and custom URI supports GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE to get object & object headers, add, update, and delete objects only caches responses to GET and HEAD requests and, optionally, OPTIONS requests does not cache responses to PUT, POST, PATCH, DELETE request methods and these requests are proxied back to the origin object removal from cache would be removed upon expiry (TTL) from the cache, by default 24 hrs can be invalidated explicitly , but has a cost associated, however might continue to see the old version until it expires from those caches objects can be invalidated only for Web distribution change object name, versioning , to serve different version supports adding or modifying custom headers before the request is sent to origin which can be used to validate if user is accessing the content from CDN identifying CDN from which the request was forwarded from, in case of multiple CloudFront distribution for viewers not supporting CORS to return the Access-Control-Allow-Origin header for every request supports Partial GET requests using range header to download object in smaller units improving the efficiency of partial downloads and recovery from partially failed transfers supports compression to compress and serve compressed files when viewer requests include Accept-Encoding: gzip in the request header supports different price class to include all regions, to include only least expensive regions and other regions to exclude most expensive regions supports access logs which contain detailed information about every user request for both web and RTMP distribution 11\u3001AWS Import/Export accelerates moving large amounts of data into and out of AWS using portable storage devices for transport and transfers data directly using Amazon\u2019s high speed internal network, bypassing the internet. suitable for use cases with large datasets low bandwidth connections first time migration of data Importing data to several types of AWS storage, including EBS snapshots, S3 buckets, and Glacier vaults . Exporting data out from S3 only, with versioning enabled only the latest version is exported Import data can be encrypted (optional but recommended) while export is always encrypted using Truecrypt Amazon will wipe the device if specified, however it will not destroy the device","title":"L2 AWS Storage & Content Delivery Cheat Sheet"},{"location":"chap2/2s3_cs/#l2-aws-storage-content-delivery-cheat-sheet","text":"","title":"L2 AWS Storage &amp; Content Delivery Cheat Sheet"},{"location":"chap2/2s3_cs/#1simple-storage-service-s3","text":"provides key-value based object storage with unlimited storage, unlimited objects up to 5 TB for the internet offers an extremely durable, highly available, and infinitely scalable data storage infrastructure at very low costs . is Object-level storage (not a Block level storage) and cannot be used to host OS or dynamic websites (but can work with Javascript SDK) provides durability by redundantly storing objects on multiple facilities within a region S3 resources consist of globally unique buckets with objects and related metadata. The data model is a flat structure with no hierarchies or folders . regularly verifies the integrity of data using checksums and provides the auto-healing capability S3 Replication enables automatic, asynchronous copying of objects across S3 buckets in the same or different AWS regions using SRR or CRR . Replication needs versioning enabled on either side. S3 Transfer Acceleration helps speed data transport over long distances between a client and an S3 bucket using CloudFront edge locations. S3 supports cost-effective Static Website hosting with Client-side scripts . S3 CORS \u2013 Cross-Origin Resource Sharing allows cross-origin access to S3 resources . S3 Access Logs enables tracking access requests to an S3 bucket. S3 notification feature enables notifications to be triggered when certain events happen in the bucket. S3 Inventory helps manage the storage and can be used to audit and report on the replication and encryption status of the objects for business, compliance, and regulatory needs. Requestor Pays help bucket owner to specify that the requester requesting the download will be charged for the download. S3 Batch Operations help perform large-scale batch operations on S3 objects and can perform a single operation on lists of specified S3 objects. Pre-Signed URLs can be used shared for uploading/downloading objects for a limited time w ithout requiring AWS security credentials .","title":"1\u3001Simple Storage Service \u2013 S3"},{"location":"chap2/2s3_cs/#multipart-uploads-allows","text":"parallel uploads with improved throughput and bandwidth utilization fault tolerance and quick recovery from network issues ability to pause and resume uploads begin an upload before the final object size is known","title":"Multipart Uploads allows"},{"location":"chap2/2s3_cs/#versioning","text":"helps preserve, retrieve, and restore every version of every object protect from unintended overwrites and accidental deletions protects individual files but does NOT protect from Bucket deletion","title":"Versioning"},{"location":"chap2/2s3_cs/#mfa-multi-factor-authentication","text":"can be enabled for additional security for the deletion of objects. Integrates with CloudTrail, CloudWatch, and SNS for event notifications","title":"MFA (Multi-Factor Authentication)"},{"location":"chap2/2s3_cs/#s3-storage-classes","text":"S3 Standard default storage class, ideal for frequently accessed data 99.999999999% durability & 99.99% availability Low latency and high throughput performance designed to sustain the loss of data in a two facilities S3 Standard-Infrequent Access (S3 Standard-IA) optimized for long-lived and less frequently accessed data designed to sustain the loss of data in a two facilities 99.999999999% durability & 99.9% availability suitable for objects greater than 128 KB kept for at least 30 days S3 One Zone-Infrequent Access (S3 One Zone-IA) optimized for rapid access, less frequently access data ideal for secondary backups and reproducible data stores data in a single AZ, data stored in this storage class will be lost in the event of AZ destruction . 99.999999999% durability & 99.5% availability S3 Glacier suitable for low cost data archiving, where data access is infrequent provides retrieval time of minutes to several hours Expedited \u2013 1 to 5 minutes Standard \u2013 3 to 5 hours Bulk \u2013 5 to 12 hour s 99.999999999% durability & 99.9% availability Minimum storage duration of 90 days S3 Glacier Deep Archive (S3 Glacier Deep Archive) provides lowest cost data archiving, where data access is infrequent 99.999999999% durability & 99.9% availability provides retrieval time of several (12-48) hours Standard \u2013 12 hours Bulk \u2013 48 hours Minimum storage duration of 180 days supports long-term retention and digital preservation for data that may be accessed once or twice a year","title":"S3 Storage Classes"},{"location":"chap2/2s3_cs/#lifecycle-management-policies","text":"transition to move objects to different storage classes and Glacier expiration to remove objects and object versions can be applied to both current and non-current objects, in case, versioning is enabled .","title":"Lifecycle Management policies"},{"location":"chap2/2s3_cs/#data-consistency-model","text":"provides strong read-after-write consistency for PUT and DELETE requests of objects in the S3 bucket in all AWS Regions updates to a single key are atomic does not currently support object locking for concurrent writes","title":"Data Consistency Model"},{"location":"chap2/2s3_cs/#s3-security","text":"IAM policies \u2013 grant users within your own AWS account permission to access S3 resources Bucket and Object ACL \u2013 grant other AWS accounts (not specific users) access to S3 resources Bucket policies \u2013 allows to add or deny permissions across some or all of the objects within a single bucket S3 Access Points simplify data access for any AWS service or customer application that stores data in S3. S3 Glacier Vault Lock helps deploy and enforce compliance controls for individual S3 Glacier vaults with a vault lock policy . S3 VPC Gateway Endpoint enables private connections between a VPC and S3, without requiring that you use an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection . Support SSL encryption of data in transit and data encryption at rest","title":"S3 Security"},{"location":"chap2/2s3_cs/#s3-data-encryption","text":"supports data at rest and data in transit encryption Server-Side Encryption SSE-S3 \u2013 encrypts S3 objects using keys handled & managed by AWS SSE-KMS \u2013 leverage AWS Key Management Service to manage encryption keys. KMS provides control and audit trail over the keys. SSE-C \u2013 when you want to manage your own encryption keys. AWS does not store the encryption key. Requires HTTPS. Client-Side Encryption Client library such as the S3 Encryption Client Clients must encrypt data themselves before sending it to S3 Clients must decrypt data themselves when retrieving from S3 Customer fully manages the keys and encryption cycle","title":"S3 Data Encryption"},{"location":"chap2/2s3_cs/#s3-best-practices","text":"use parallel threads and Multipart upload for faster writes use parallel threads and Range Header GET for faster reads for list operations with a large number of objects, it\u2019s better to build a secondary index in DynamoDB use Versioning to protect from unintended overwrites and deletions , but this does not protect against bucket deletion use VPC S3 Endpoints with VPC to transfer data using Amazon internal network","title":"S3 Best Practices"},{"location":"chap2/2s3_cs/#2instance-store","text":"provides temporary or ephemeral block-level storage for an EC2 instance is physically attached to the Instance deliver very high random I/O performance , which is a good option when storage with very low latency is needed cannot be dynamically resized data persists when an instance is rebooted data does not persists if the underlying disk drive fails instance stops i.e. if the EBS backed instance with instance store volumes attached is stopped instance terminates can be attached to an EC2 instance only when the instance is launched is ideal for the temporary storage of information that changes frequently , such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.","title":"2\u3001Instance Store"},{"location":"chap2/2s3_cs/#3elastic-block-store-ebs","text":"is virtual network-attached block storage provides highly available , reliable, durable, block-level storage volumes that can be attached to a running instance provides high durability and are redundant in an AZ , as the data is automatically replicated within that AZ to prevent data loss due to any single hardware component failure persists and is independent of EC2 lifecycle multiple volumes can be attached to a single EC2 instance can be detached & attached to another EC2 instance in that same AZ only volumes are Zonal i.e. created in a specific AZ and CAN\u2019T span across AZs snapshots for making volume available to different AZ, create a snapshot of the volume and restore it to a new volume in any AZ within the region for making the volume available to different Region, the snapshot of the volume can be copied to a different region and restored as a volume PIOPS is designed to run transactions applications that require high and consistent IO for e.g. Relation database, NoSQL , etc PIOPS is designed to run transactions applications that require high and consistent IO for e.g. Relation database, NoSQL, etc Multi-Attach enables attaching a single Provisioned IOPS SSD (io1 or io2) volume to multiple instances that are in the same AZ.","title":"3\u3001Elastic Block Store \u2013 EBS"},{"location":"chap2/2s3_cs/#4ebs-encryption","text":"allow encryption using the EBS encryption feature. All data stored at rest, disk I/O, and snapshots created from the volume are encrypted. uses 256-bit AES algorithms (AES-256) and an Amazon-managed KMS Snapshots of encrypted EBS volumes are automatically encrypted.","title":"4\u3001EBS Encryption"},{"location":"chap2/2s3_cs/#5ebs-snapshots","text":"helps create backups of EBS volumes are incremental occur asynchronously , consume the instance IOPS are regional and CANNOT span across regions can be copied across regions to make it easier to leverage multiple regions for geographical expansion, data center migration, and disaster recovery can be shared by making them public or with specific AWS accounts by modifying the access permissions of the snapshots support EBS encryption Snapshots of encrypted volumes are automatically encrypted Volumes created from encrypted snapshots are automatically encrypted All data in flight between the instance and the volume is encrypted Volumes created from an unencrypted snapshot owned or have access to can be encrypted on the fly. Encrypted snapshot owned or having access to, can be encrypted with a different key during the copy process. can be automated using AWS Data Lifecycle Manager","title":"5\u3001EBS Snapshots"},{"location":"chap2/2s3_cs/#6ebs-vs-instance-store","text":"","title":"6\u3001EBS vs Instance Store"},{"location":"chap2/2s3_cs/#6-1-aws-ebs-vs-instance-store-overview","text":"EC2 instances support two types for block level storage Elastic Block Store (EBS) Instance Store (Ephemeral store) EC2 Instances can be launched using either Elastic Block Store (EBS) or Instance Store volume as root volumes and additional volumes . EC2 instances can be launched by choosing between AMIs backed by EC2 instance store and AMIs backed by EBS . However, AWS recommends using EBS backed AMIs because they launch faster and use persistent storage.","title":"6-1 AWS EBS vs Instance Store Overview"},{"location":"chap2/2s3_cs/#6-2-instance-store-ephemeral-storage","text":"An Instance store backed instance is an EC2 instance using an Instance store as root device volume created from a template stored in S3 . Instance store volumes access storage from disks that are physically attached to the host computer . When an Instance stored instance is launched, the image that is used to boot the instance is copied to the root volume ( typically sda1 ). Instance store provides temporary block-level storage for instances. Data on an instance store volume persists only during the life of the associated instance; if an instance is stopped or terminated, any data on instance store volumes is lost.","title":"6-2 Instance Store (Ephemeral storage)"},{"location":"chap2/2s3_cs/#6-3-key-points-for-instance-store-backed-instance","text":"Boot time is slower than EBS backed volumes and usually less than 5 min Can be selected as Root Volume and attached as additional volumes Instance store backed Instances can be of a maximum 10GiB volume size Instance store volume can be attached as additional volumes only when the instance is being launched and cannot be attached once the Instance is up and running Instance store backed Instances cannot be stopped , as when stopped and started AWS does not guarantee the instance would be launched in the same host, and hence the data is lost Data on Instance store volume is LOST in the following scenarios: Failure of an underlying drive Stopping an EBS-backed instance where instance stores are attached as additional volumes Termination of the Instance Data on Instance store volume is NOT LOST when the instance is rebooted For EC2 instance store-backed instances AWS recommends to distribute the data on the instance stores across multiple AZs back up critical data from the instance store volumes to persistent storage on a regular basis. AMI creation requires usage on AMI tools and needs to be executed from within the running instance Instance store backed Instances cannot be upgraded","title":"6-3 Key points for Instance store backed Instance"},{"location":"chap2/2s3_cs/#6-3-elastic-block-store-ebs","text":"An \u201cEBS-backed\u201d instance means that the root device for an instance launched from the AMI is an EBS volume created from an EBS snapshot An EBS volume behaves like a raw, unformatted, external block device that can be attached to a single instance and is not physically attached to the Instance host computer (more like network-attached storage). Volume persists independently from the running life of an instance . After an EBS volume is attached to an instance, you can use it like any other physical hard drive. EBS volume can be detached from one instance and attached to another instance EBS volumes can be created as encrypted volumes using the EBS encryption feature","title":"6-3 Elastic Block Store (EBS)"},{"location":"chap2/2s3_cs/#6-4-key-points-for-ebs-backed-instance","text":"Boot time is very fast usually less than a min Can be selected as Root Volume and attached as additional volumes EBS backed Instances can be of maximum 64TiB volume size depending upon the OS EBS volume can be attached as additional volumes when the Instance is launched and even when the Instance is up and running Data on the EBS volume is LOST for EBS Root volume, if Delete On Termination flag is enabled , which is the default. Attached EBS volumes, if the Delete On Termination flag is enabled. It\u2019s disabled, by default. Data on EBS volume is NOT LOST in the following scenarios: Reboot on the Instance Stopping an EBS-backed instance Termination of the Instance for the additional EBS volumes. Additional EBS volumes are detached with their data intact When an EBS-backed instance is in a stopped state, various instance\u2013 and volume-related tasks can be done for e.g. you can modify the properties of the instance, you can change the size of your instance or update the kernel it is using, you can attach your root volume to a different running instance for debugging or any other purpose EBS volumes are AZ scoped and tied to a single AZ in which created EBS volumes are automatically replicated within that zone to prevent data loss due to the failure of any single hardware component AMI creation is easy using a Single command EBS backed Instances can be upgraded for instance type, Kernel, RAM disk, and user data","title":"6-4 Key points for EBS backed Instance"},{"location":"chap2/2s3_cs/#6-4-ebs-vs-instance-store-comparision","text":"","title":"6-4 EBS vs Instance Store Comparision"},{"location":"chap2/2s3_cs/#6-5-boot-times","text":"EBS-backed AMIs launch faster than EC2 instance store-backed AMIs. When an EC2 instance store-backed AMI is launched, all the parts have to be retrieved from S3 before the instance is available. With an EBS-backed AMI is launched, parts are lazily loaded and only the parts required to boot the instance need to be retrieved from the snapshot before the instance is available. However, the performance of an instance that uses an EBS volume for its root device is slower for a short time while the remaining parts are retrieved from the snapshot and loaded into the volume. When you stop and restart the instance, it launches quickly, because the state is stored in an EBS volume. EC2 EBS-backed (EBS root) instance is stopped, what happens to the data on any ephemeral store volumes ? Data is automatically saved in an EBS volume. Data is unavailable until the instance is restarted. Data will be deleted and will no longer be accessible . Data is automatically saved as an EBS snapshot. When an EC2 instance that is backed by an S3-based AMI is terminated, what happens to the data on the root volume? Data is automatically saved as an EBS snapshot. Data is automatically saved as an EBS volume. Data is unavailable until the instance is restarted. Data is automatically deleted. Which of the following will occur when an EC2 instance in a VPC (Virtual Private Cloud) with an associated Elastic IP is stopped and started? (Choose 2 answers) The Elastic IP will be dissociated from the instance All data on instance-store devices will be lost All data on EBS (Elastic Block Store) devices will be lost The ENI (Elastic Network Interface) is detached The underlying host for the instance is changed Which of the following provides the fastest storage medium? Amazon S3 Amazon EBS using Provisioned IOPS (PIOPS) SSD Instance (ephemeral) store (SSD Instance Storage provides 100,000 IOPS on some instance types, much faster than any network-attached storage) AWS Storage Gateway","title":"6-5 Boot Times"},{"location":"chap2/2s3_cs/#7glacier","text":"suitable for archiving data, where data access is infrequent and a retrieval time of several hours (3 to 5 hours) is acceptable ( Not true anymore with enhancements from AWS ) provides a high durability by storing archive in multiple facilities and multiple devices at a very low cost storage performs regular, systematic data integrity checks and is built to be automatically self healing aggregate files into bigger files before sending them to Glacier and use range retrievals to retrieve partial file and reduce costs improve speed and reliability with multipart upload automatically encrypts the data using AES-256 upload or download data to Glacier via SSL encrypted endpoints 8\u3001EFS fully-managed, easy to set up, scale, and cost-optimize file storage can automatically scale from gigabytes to petabytes of data without needing to provision storage provides managed NFS (network file system) that can be mounted on and accessed by multiple EC2 in multiple AZs simultaneously highly durable, highly scalable and highly available. stores data redundantly across multiple Availability Zones grows and shrinks automatically as files are added and removed, so you there is no need to manage storage procurement or provisioning. expensive (3x gp2), but you pay per use uses the Network File System version 4 (NFS v4) protocol is compatible with all Linux-based AMIs for EC 2, POSIX file system (~Linux) that has a standard file API does not support Windows AMI does not support Windows AMI offers the ability to encrypt data at rest using KMS and in transit. can be accessed from on-premises using an AWS Direct Connect or AWS VPN connection between the on-premises datacenter and VPC . can be accessed concurrently from servers in the on-premises datacenter as well as EC2 instances in the Amazon VPC Performance mode General purpose (default) latency-sensitive use cases (web server, CMS, etc\u2026) Max I/O higher latency, throughput, highly parallel (big data, media processing) Storage Tiers Standard for frequently accessed files ideal for active file system workloads and you pay only for the file system storage you use per month Infrequent access (EFS-IA) a lower cost storage class that\u2019s cost-optimized for files infrequently accessed i.e. not accessed every day cost to retrieve files, lower price to store EFS Lifecycle Management with choosing an age-off policy allows moving files to EFS IA Lifecycle Management automatically moves the data to the EFS IA storage class according to the lifecycle policy. for e.g., you can move files automatically into EFS IA fourteen days of not being accessed . EFS is a shared POSIX system for Linux systems and does not work for Windows","title":"7\u3001Glacier"},{"location":"chap2/2s3_cs/#8amazon-fsx-for-windows","text":"is a fully managed, highly reliable, and scalable Windows file system share drive supports SMB protocol & Windows NTFS supports Microsoft Active Directory integration, ACLs, user quotas built on SSD, scale up to 10s of GB/s, millions of IOPS, 100s PB of data is accessible from Windows, Linux, and MacOS compute instances can be accessed from the on-premise infrastructure can be configured to be Multi-AZ (high availability) supports encryption of data at rest and in transit provides data deduplication, which enables further cost optimization by removing redundant data. data is backed-up daily to S3","title":"8\u3001Amazon FSx for Windows"},{"location":"chap2/2s3_cs/#9amazon-fsx-for-lustre","text":"provides easy and cost effective way to launch and run the world\u2019s most popular high-performance file system . is a type of parallel distributed file system, for large-scale computing Lustre is derived from \u201cLinux\u201d and \u201ccluster\u201d Machine Learning, High Performance Computing (HPC) esp. Video Processing, Financial Modeling, Electronic Design Automation scales up to 100s GB/s, millions of IOPS, sub-ms latencies seamless integration with S3, it transparently presents S3 objects as files and allows you to write changed data back to S3 . can \u201cread S3\u201d as a file system (through FSx) can write the output of the computations back to S3 (through FSx) supports encryption of data at rest and in transit can be used from on-premise servers","title":"9\u3001Amazon FSx for Lustre"},{"location":"chap2/2s3_cs/#10cloudfront","text":"provides low latency and high data transfer speeds for distribution of static, dynamic web or streaming content to web users delivers the content through a worldwide network of data centers called Edge Locations keeps persistent connections with the origin servers so that the files can be fetched from the origin servers as quickly as possible . dramatically reduces the number of network hops that users\u2019 requests must pass through supports multiple origin server options , like AWS hosted service for e.g. S3, EC2, ELB or an on premise server , which stores the original, definitive version of the objects single distribution can have multiple origin s and Path pattern in a cache behavior determines which requests are routed to the origin","title":"10\u3001CloudFront"},{"location":"chap2/2s3_cs/#supports-web-download-distribution-and-rtmp-streaming-distribution","text":"Web distribution supports static, dynamic web content, on demand using progressive download & HLS and live streaming video content RTMP supports streaming of media files using Adobe Media Server and the Adobe Real-Time Messaging Protocol (RTMP) ONLY","title":"supports Web Download distribution and RTMP Streaming distribution"},{"location":"chap2/2s3_cs/#supports-https-using-either","text":"dedicated IP address , which is expensive as dedicated IP address is assigned to each CloudFront edge location Server Name Indication (SNI), which is free but supported by modern browsers only with the domain name available in the request header","title":"supports HTTPS using either"},{"location":"chap2/2s3_cs/#for-e2e-https-connection","text":"Viewers -> CloudFront needs either self signed certificate, or certificate issued by CA or ACM CloudFront -> Origin needs certificate issued by ACM for ELB and by CA for other origins","title":"For E2E HTTPS connection,"},{"location":"chap2/2s3_cs/#security","text":"Origin Access Identity (OAI) can be used to restrict the content from S3 origin to be accessible from CloudFront only supports Geo restriction (Geo-Blocking) to whitelist or blacklist countries that can access the content Signed URLs for RTMP distribution as signed cookies aren\u2019t supported to restrict access to individual files, for e.g., an installation download for your application . users using a client, for e.g. a custom HTTP client , that doesn\u2019t support cookies","title":"Security"},{"location":"chap2/2s3_cs/#signed-cookies","text":"provide access to multiple restricted files, for e.g., video part files in HLS format or all of the files in the subscribers\u2019 area of a website. don\u2019t want to change the current URLs Integrates with AWS WAF , a web application firewall that helps protect web applications from attacks by allowing rules configured based on IP addresses, HTTP headers, and custom URI supports GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE to get object & object headers, add, update, and delete objects only caches responses to GET and HEAD requests and, optionally, OPTIONS requests does not cache responses to PUT, POST, PATCH, DELETE request methods and these requests are proxied back to the origin","title":"Signed Cookies"},{"location":"chap2/2s3_cs/#object-removal-from-cache","text":"would be removed upon expiry (TTL) from the cache, by default 24 hrs can be invalidated explicitly , but has a cost associated, however might continue to see the old version until it expires from those caches objects can be invalidated only for Web distribution change object name, versioning , to serve different version supports adding or modifying custom headers before the request is sent to origin which can be used to validate if user is accessing the content from CDN identifying CDN from which the request was forwarded from, in case of multiple CloudFront distribution for viewers not supporting CORS to return the Access-Control-Allow-Origin header for every request supports Partial GET requests using range header to download object in smaller units improving the efficiency of partial downloads and recovery from partially failed transfers supports compression to compress and serve compressed files when viewer requests include Accept-Encoding: gzip in the request header supports different price class to include all regions, to include only least expensive regions and other regions to exclude most expensive regions supports access logs which contain detailed information about every user request for both web and RTMP distribution","title":"object removal from cache"},{"location":"chap2/2s3_cs/#11aws-importexport","text":"accelerates moving large amounts of data into and out of AWS using portable storage devices for transport and transfers data directly using Amazon\u2019s high speed internal network, bypassing the internet. suitable for use cases with large datasets low bandwidth connections first time migration of data Importing data to several types of AWS storage, including EBS snapshots, S3 buckets, and Glacier vaults . Exporting data out from S3 only, with versioning enabled only the latest version is exported Import data can be encrypted (optional but recommended) while export is always encrypted using Truecrypt Amazon will wipe the device if specified, however it will not destroy the device","title":"11\u3001AWS Import/Export"},{"location":"chap2/3IS_cs/","text":"L3 AWS Security & Identity Service Cheat Sheet 1\u3001IAM securely control access to AWS services and resources helps create and manage user identities and grant permissions for those users to access AWS resources helps create groups for multiple users with similar permissions not appropriate for application authentication is Global and does not need to be migrated to a different region helps define Policies, in JSON format all permissions are implicitly denied by default most restrictive policy wins 1-1 IAM Role helps grants and delegate access to users and services without the need of creating permanent credentials IAM users or AWS services can assume a role to obtain temporary security credentials that can be used to make AWS API calls needs Trust policy to define who and Permission policy to define what the user or service can access used with Security Token Service (STS), a lightweight web service that provides temporary, limited privilege credentials for IAM users or for authenticated federated users IAM role scenarios Service access for e.g. EC2 to access S3 or DynamoDB Cross Account access for users with user within the same account with user within an AWS account owned the same owner with user from a Third Party AWS account with External ID for enhanced security Identity Providers & Federation Web Identity Federation, where the user can be authenticated using external authentication Identity providers like Amazon, Google or any OpenId IdP using AssumeRoleWithWebIdentity Identity Provider using SAML 2.0, where the user can be authenticated using on premises Active Directory , Open Ldap or any SAML 2.0 compliant IdP using AssumeRoleWithSAML For other Identity Providers, use Identity Broker to authenticate and provide temporary Credentials using AssumeRole (recommended) or GetFederationToken 1-2 IAM Best Practices Do not use Root account for anything other than billing Create Individual IAM users Use groups to assign permissions to IAM users Grant least privilege Use IAM roles for applications on EC2 Delegate using roles instead of sharing credentials Rotate credentials regularly Use Policy conditions for increased granularity Use CloudTrail to keep a history of activity Enforce a strong IAM password policy for IAM users Remove all unused users and credentials 2\u3001CloudHSM provides secure cryptographic key storage to customers by making hardware security modules (HSMs) available in the AWS cloud single tenant, dedicated physical device to securely generate, store, and manage cryptographic keys used for data encryption are inside the VPC (not EC2-classic) & isolated from the rest of the network can use VPC peering to connect to CloudHSM from multiple VPCs integrated with Amazon Redshift and Amazon RDS for Oracle EBS volume encryption, S3 object encryption and key management can be done with CloudHSM but requires custom application scripting is NOT fault tolerant and would need to build a cluster as if one fails all the keys are lost expensive , prefer AWS Key Management Service (KMS) if cost is a criteria 3\u3001AWS Directory Services gives applications in AWS access to Active Directory services different from SAML + AD , where the access is granted to AWS services through Temporary Credentials least expensive but does not support Microsoft AD advance features provides a Samba 4 Microsoft Active Directory compatible standalone directory service on AWS No single point of Authentication or Authorization, as a separate copy is maintained trust relationships cannot be setup between Simple AD and other Active Directory domains Don\u2019t use it, if the requirement is to leverage access and control through centralized authentication service 3-1 AD Connector acts just as an hosted proxy service for instances in AWS to connect to on-premises Active Directory enables consistent enforcement of existing security policies, such as password expiration, password history, and account lockouts, whether users are accessing resources on-premises or in the AWS cloud needs VPN connectivity (or Direct Connect) integrates with existing RADIUS-based MFA solutions to enabled multi-factor authentication does not cache data which might lead to latency 3-2 Read-only Domain Controllers (RODCs) works out as a Read-only Active Directory holds a copy of the Active Directory Domain Service (AD DS) database and respond to authentication requests they cannot be written to and are typically deployed in locations where physical security cannot be guaranteed helps maintain a single point to authentication & authorization controls, however needs to be synced 3-3 Writable Domain Controllers are expensive to setup operate in a multi-master model; changes can be made on any writable server in the forest, and those changes are replicated to servers throughout the entire forest 4 AWS WAF is a web application firewall that helps monitor the HTTP/HTTPS traffic and allows controlling access to the content . helps protect web applications from attacks by allowing rules configuration that allow, block, or monitor (count) web requests based on defined conditions. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection and cross-site scripting. helps define Web ACLs, which is a combination of Rules that is a combinations of Conditions and Action to block or allow integrated with CloudFront, Application Load Balancer (ALB), API Gateway services commonly used to deliver content and applications supports custom origins outside of AWS, when integrated with CloudFront 5 AWS Shield is a managed service that provides protection against Distributed Denial of Service (DDoS) attacks for applications running on AWS provides protection for all AWS customers against common and most frequently occurring infrastructure (layer 3 and 4) attacks like SYN/UDP floods, reflection attacks , and others to support high availability of applications on AWS. provides AWS Shield Advanced with additional protections against more sophisticated and larger attacks for applications running on EC2, ELB, CloudFront, AWS Global Accelerator, and Route 53 6 AWS GuardDuty offers threat detection that enables continuous monitoring and protect the AWS accounts and workloads. analyzes continuous streams of meta-data generated from AWS account and network activity found in AWS CloudTrail Events , VPC Flow Logs, and DNS Logs. integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately. integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately . 7 AWS Inspector is an automated security assessment service that helps test the network accessibility of EC2 instances and the security state of the applications running on the instances. helps automate security vulnerability assessments throughout the development and deployment pipeline or against static production systems 8 AWS Artifact is a self-service audit artifact retrieval portal that provides customers with on-demand access to AWS\u2019 compliance documentation and agreements can use AWS Artifact Reports to download AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and System and Organization Control (SOC) reports . AWS Shield AWS Systems Manager AWS GuardDuty AWS Config","title":"L3 AWS Security & Identity Service Cheat Sheet"},{"location":"chap2/3IS_cs/#l3-aws-security-identity-service-cheat-sheet","text":"","title":"L3 AWS Security &amp; Identity Service Cheat Sheet"},{"location":"chap2/3IS_cs/#1iam","text":"securely control access to AWS services and resources helps create and manage user identities and grant permissions for those users to access AWS resources helps create groups for multiple users with similar permissions not appropriate for application authentication is Global and does not need to be migrated to a different region helps define Policies, in JSON format all permissions are implicitly denied by default most restrictive policy wins","title":"1\u3001IAM"},{"location":"chap2/3IS_cs/#1-1-iam-role","text":"helps grants and delegate access to users and services without the need of creating permanent credentials IAM users or AWS services can assume a role to obtain temporary security credentials that can be used to make AWS API calls needs Trust policy to define who and Permission policy to define what the user or service can access used with Security Token Service (STS), a lightweight web service that provides temporary, limited privilege credentials for IAM users or for authenticated federated users IAM role scenarios Service access for e.g. EC2 to access S3 or DynamoDB Cross Account access for users with user within the same account with user within an AWS account owned the same owner with user from a Third Party AWS account with External ID for enhanced security Identity Providers & Federation Web Identity Federation, where the user can be authenticated using external authentication Identity providers like Amazon, Google or any OpenId IdP using AssumeRoleWithWebIdentity Identity Provider using SAML 2.0, where the user can be authenticated using on premises Active Directory , Open Ldap or any SAML 2.0 compliant IdP using AssumeRoleWithSAML For other Identity Providers, use Identity Broker to authenticate and provide temporary Credentials using AssumeRole (recommended) or GetFederationToken","title":"1-1 IAM Role"},{"location":"chap2/3IS_cs/#1-2-iam-best-practices","text":"Do not use Root account for anything other than billing Create Individual IAM users Use groups to assign permissions to IAM users Grant least privilege Use IAM roles for applications on EC2 Delegate using roles instead of sharing credentials Rotate credentials regularly Use Policy conditions for increased granularity Use CloudTrail to keep a history of activity Enforce a strong IAM password policy for IAM users Remove all unused users and credentials","title":"1-2 IAM Best Practices"},{"location":"chap2/3IS_cs/#2cloudhsm","text":"provides secure cryptographic key storage to customers by making hardware security modules (HSMs) available in the AWS cloud single tenant, dedicated physical device to securely generate, store, and manage cryptographic keys used for data encryption are inside the VPC (not EC2-classic) & isolated from the rest of the network can use VPC peering to connect to CloudHSM from multiple VPCs integrated with Amazon Redshift and Amazon RDS for Oracle EBS volume encryption, S3 object encryption and key management can be done with CloudHSM but requires custom application scripting is NOT fault tolerant and would need to build a cluster as if one fails all the keys are lost expensive , prefer AWS Key Management Service (KMS) if cost is a criteria","title":"2\u3001CloudHSM"},{"location":"chap2/3IS_cs/#3aws-directory-services","text":"gives applications in AWS access to Active Directory services different from SAML + AD , where the access is granted to AWS services through Temporary Credentials least expensive but does not support Microsoft AD advance features provides a Samba 4 Microsoft Active Directory compatible standalone directory service on AWS No single point of Authentication or Authorization, as a separate copy is maintained trust relationships cannot be setup between Simple AD and other Active Directory domains Don\u2019t use it, if the requirement is to leverage access and control through centralized authentication service","title":"3\u3001AWS Directory Services"},{"location":"chap2/3IS_cs/#3-1-ad-connector","text":"acts just as an hosted proxy service for instances in AWS to connect to on-premises Active Directory enables consistent enforcement of existing security policies, such as password expiration, password history, and account lockouts, whether users are accessing resources on-premises or in the AWS cloud needs VPN connectivity (or Direct Connect) integrates with existing RADIUS-based MFA solutions to enabled multi-factor authentication does not cache data which might lead to latency","title":"3-1 AD Connector"},{"location":"chap2/3IS_cs/#3-2-read-only-domain-controllers-rodcs","text":"works out as a Read-only Active Directory holds a copy of the Active Directory Domain Service (AD DS) database and respond to authentication requests they cannot be written to and are typically deployed in locations where physical security cannot be guaranteed helps maintain a single point to authentication & authorization controls, however needs to be synced","title":"3-2 Read-only Domain Controllers (RODCs)"},{"location":"chap2/3IS_cs/#3-3-writable-domain-controllers","text":"are expensive to setup operate in a multi-master model; changes can be made on any writable server in the forest, and those changes are replicated to servers throughout the entire forest","title":"3-3 Writable Domain Controllers"},{"location":"chap2/3IS_cs/#4-aws-waf","text":"is a web application firewall that helps monitor the HTTP/HTTPS traffic and allows controlling access to the content . helps protect web applications from attacks by allowing rules configuration that allow, block, or monitor (count) web requests based on defined conditions. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection and cross-site scripting. helps define Web ACLs, which is a combination of Rules that is a combinations of Conditions and Action to block or allow integrated with CloudFront, Application Load Balancer (ALB), API Gateway services commonly used to deliver content and applications supports custom origins outside of AWS, when integrated with CloudFront","title":"4 AWS WAF"},{"location":"chap2/3IS_cs/#5-aws-shield","text":"is a managed service that provides protection against Distributed Denial of Service (DDoS) attacks for applications running on AWS provides protection for all AWS customers against common and most frequently occurring infrastructure (layer 3 and 4) attacks like SYN/UDP floods, reflection attacks , and others to support high availability of applications on AWS. provides AWS Shield Advanced with additional protections against more sophisticated and larger attacks for applications running on EC2, ELB, CloudFront, AWS Global Accelerator, and Route 53","title":"5 AWS Shield"},{"location":"chap2/3IS_cs/#6-aws-guardduty","text":"offers threat detection that enables continuous monitoring and protect the AWS accounts and workloads. analyzes continuous streams of meta-data generated from AWS account and network activity found in AWS CloudTrail Events , VPC Flow Logs, and DNS Logs. integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately. integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately .","title":"6 AWS GuardDuty"},{"location":"chap2/3IS_cs/#7-aws-inspector","text":"is an automated security assessment service that helps test the network accessibility of EC2 instances and the security state of the applications running on the instances. helps automate security vulnerability assessments throughout the development and deployment pipeline or against static production systems","title":"7 AWS Inspector"},{"location":"chap2/3IS_cs/#8-aws-artifact","text":"is a self-service audit artifact retrieval portal that provides customers with on-demand access to AWS\u2019 compliance documentation and agreements can use AWS Artifact Reports to download AWS security and compliance documents, such as AWS ISO certifications, Payment Card Industry (PCI), and System and Organization Control (SOC) reports . AWS Shield AWS Systems Manager AWS GuardDuty AWS Config","title":"8 AWS Artifact"},{"location":"chap2/4Ns_cs/","text":"L4 AWS Networking Services Cheat Sheet 1 Virtual Private Cloud \u2013 VPC helps define a logically isolated dedicated virtual network within the AWS provides control of IP addressing using CIDR block from a minimum of /28 to maximum of /16 block size supports IPv4 and IPv6 addressing can be extended by associating secondary IPv4 CIDR blocks to VPC 1-1 Components Internet gateway (IGW) provides access to the Internet Virtual gateway (VGW) provides access to on-premises data center through VPN and Direct Connect connections VPC can have only one IGW and VGW Route tables determine where network traffic from subnet is directed Ability to create subnet with VPC CIDR block A Network Address Translation (NAT) server provides outbound Internet access for EC2 instances in private subnets Elastic IP addresses are static, persistent public IP addresses Instances launched in the VPC will have a Private IP address and can have a Public or a Elastic IP address associated with it Security Groups and NACLs help define security Flow logs \u2013 Capture information about the IP traffic going to and from network interfaces in your VPC 1-2 Tenancy option for instances shared , by default, allows instances to be launched on shared tenancy dedicated allows instances to be launched on a dedicated hardware 1-3 Route Tables defines rules, termed as routes, which determine where network traffic from the subnet would be routed Each VPC has a Main Route table, and can have multiple custom route tables created Every route table contains a local route that enables communication within a VPC which cannot be modified or deleted Route priority is decided by matching the most specific route in the route table that matches the traffic 1-4 Subnets map to AZs and do not span across AZs have a CIDR range that is a portion of the whole VPC. CIDR ranges cannot overlap between subnets within the VPC. AWS reserves 5 IP addresses in each subnet \u2013 first 4 and last one Each subnet is associated with a route table which define its behavior Public subnets \u2013 inbound/outbound Internet connectivity via IGW Private subnets \u2013 outbound Internet connectivity via an NAT or VGW Protected subnets \u2013 no outbound connectivity and used for regulated workloads 1-5 Elastic Network Interface (ENI) a default ENI, eth0, is attached to an instance which cannot be detached with one or more secondary detachable ENIs (eth1-ethn) has primary private, one or more secondary private, public, Elastic IP address, security groups, MAC address and source/destination check flag attributes associated AN ENI in one subnet can be attached to an instance in the same or another subnet, in the same AZ and the same VPC Security group membership of an ENI can be changed with pre allocated Mac Address can be used for applications with special licensing requirements 1-6 Security Groups vs Network Access Control Lists Stateful vs Stateless At instance level vs At subnet level Only allows Allow rule vs Allows both Allow and Deny rules Evaluated as a Whole vs Evaluated in defined Order 1-7 Elastic IP is a static IP address designed for dynamic cloud computing. is associated with AWS account , and not a particular instance can be remapped from one instance to an other instance is charged for non usage , if not linked for any instance or instance associated is in stopped state 1-8 NAT allows internet access to instances in private subnet performs the function of both address translation and port address translation (PAT) needs source/destination check flag to be disabled as it is not actual destination of the traffic NAT gateway is a AWS managed NAT service that provides better availability, higher bandwidth, and requires less administrative effort are not supported for IPv6 traffic 1-9 Egress-Only Internet Gateways outbound communication over IPv6 from instances in the VPC to the Internet, and prevents the Internet from initiating an IPv6 connection with your instances supports IPv6 traffic only 1-10 Shared VPCs allows multiple AWS accounts to create their application resources, such as EC2 instances, RDS databases, Redshift clusters, and AWS Lambda functions, into shared, centrally-managed VPCs 1-11 VPC Peering allows routing of traffic between the peer VPCs using private IP addresses and no IGW or VGW required No single point of failure and bandwidth bottlenecks supports inter-region VPC peering IP space or CIDR blocks cannot overlap cannot be transitive , one-to-one relationship between two VPC Only one between any two VPCs and have to be explicitly peered Private DNS values cannot be resolved Security groups from peered VPC can now be referred, however the VPC should be in the same region. 1-12 VPC Endpoints enables you to privately connect VPC to supported AWS services and VPC endpoint services powered by PrivateLink does not require a public IP address, access over the Internet, NAT device, a VPN connection or Direct Connect traffic between VPC & AWS service does not leave the Amazon network are virtual devices. are horizontally scaled, redundant, and highly available VPC components that allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic. Gateway Endpoints is a gateway that is a target for a specified route in the route table, used for traffic destined to a supported AWS service. only S3 and DynamoDB are currently supported Interface Endpoints is an elastic network interface with a private IP address that serves as an entry point for traffic destined to a supported service services supported API Gateway AWS CloudFormation, CloudWatch, CloudWatch Events, CloudWatch Logs AWS CodeBuild AWS CodeCommit AWS Config, EC2 API Elastic Load Balancing API, Elastic Container Registry, Elastic Container Service AWS Key Management Service, Kinesis Data Streams, SageMaker and, SageMaker Runtime, SageMaker Notebook Instance AWS Secrets Manager AWS Security Token Service AWS Service Catalog, SNS, SQS AWS Systems Manager 2 CloudFront provides low latency and high data transfer speeds for distribution of static, dynamic web, or streaming content to web users. delivers the content through a worldwide network of data centers called Edge Locations or Point of Presence (PoPs) keeps persistent connections with the origin servers so that the files can be fetched from the origin servers as quickly as possible. dramatically reduces the number of network hops that users\u2019 requests must pass through supports multiple origin server options, like AWS hosted service for e.g. S3, EC2, ELB, or an on-premise server, which stores the original, definitive version of the objects single distribution can have multiple origins and Path pattern in a cache behavior determines which requests are routed to the origin Web distribution supports static, dynamic web content, on-demand using progressive download & HLS, and live streaming video content supports HTTPS using either dedicated IP address, which is expensive as a dedicated IP address is assigned to each CloudFront edge location Server Name Indication (SNI), which is free but supported by modern browsers only with the domain name available in the request header For E2E HTTPS connection, Viewers -> CloudFront needs either a self-signed certificate or a certificate issued by CA or ACM CloudFront -> Origin needs certificate issued by ACM for ELB and by CA for other origins Security Origin Access Identity (OAI) can be used to restrict the content from S3 origin to be accessible from CloudFront only supports Geo restriction (Geo-Blocking) to whitelist or blacklist countries that can access the content Signed URLs to restrict access to individual files, for e.g., an installation download for your application. users using a client, for e.g. a custom HTTP client, that doesn\u2019t support cookies Signed Cookies provide access to multiple restricted files , for e.g., video part files in HLS format or all of the files in the subscribers\u2019 area of a website . don\u2019t want to change the current URLs integrates with AWS WAF, a web application firewall that helps protect web applications from attacks by allowing rules configured based on IP addresses , HTTP headers, and custom URI strings supports GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE to get object & object headers, add, update, and delete objects only caches responses to GET and HEAD requests and, optionally, OPTIONS requests does not cache responses to PUT, POST, PATCH, DELETE request methods and these requests are proxied back to the origin object removal from the cache would be removed upon expiry (TTL) from the cache, by default 24 hrs can be invalidated explicitly, but has a cost associated, however, might continue to see the old version until it expires from those caches objects can be invalidated only for Web distribution use versioning or change object name, to serve a different version supports adding or modifying custom headers before the request is sent to origin which can be used to validate if a user is accessing the content from CDN identifying CDN from which the request was forwarded, in case of multiple CloudFront distributions for viewers not supporting CORS to return the Access-Control-Allow-Origin header for every request supports Partial GET requests using range header to download objects in smaller units improving the efficiency of partial downloads and recovery from partially failed transfers supports compression to compress and serve compressed files when viewer requests include Accept-Encoding: gzip in the request header supports different price classes to include all regions , or only the least expensive regions and other regions without the most expensive regions 3 Direct Connect & VPN 3-1 VPN provide secure IPSec connections from on-premise computers or services to AWS over the Internet is quick to setup, is cheap however it depends on the Internet speed 3-2 Direct Connect is a network service that provides an alternative to using Internet to utilize AWS services by using private dedicated network connection provides Virtual Interfaces Private VIF to access instances within an VPC via VGW Public VIF to access non VPC services requires time to setup probably months , and should not be considered as an option if turnaround time is less does not provide redundancy , use either second direct connection or IPSec VPN connection Virtual Private Gateway is on the AWS side and Customer Gateway is on the Customer side route propagation is enabled on VGW and not on CGW 3-2 Direct Connect vs VPN IPSec Expensive to Setup and Takes time vs Cheap & Immediate Dedicated private connections vs Internet Reduced data transfer rate vs Internet data transfer cost Consistent performance vs Internet inherent variability Do not provide Redundancy vs Provides Redundancy 4 Route 53 Highly available and scalable DNS & Domain Registration Service Reliable and cost-effective way to route end users to Internet applications Supports multi-region and backup architectures for High availability . ELB , limited to region, does not support multi region HA architecture supports private Intranet facing DNS service internal resource record sets only work for requests originating from within the VPC and currently cannot extend to on-premise Global propagation of any changes made to the DN records within ~ 1min Route 53 to create an alias resource record set that points to ELB, S3, CloudFront. An alias resource record set is a Route 53 extension to DNS. It\u2019s similar to a CNAME resource record set, but supports both for root domain \u2013 zone apex e.g. example.com, and for subdomains for e.g. www.example.com. CNAME resource record sets can be created only for subdomains and cannot be mapped to the zone apex record Route 53 Split-view (Split-horizon) DNS enables you to access an internal version of your website using the same domain name that is used publicly 4-1 Routing polic y Simple routing \u2013 simple round robin policy Weighted round robin \u2013 assign weights to resource records sets to specify the proportion for e.g. 80%:20% Latency based routing \u2013 helps improve global applications as request are sent to server from the location with minimal latency , is based on the latency and cannot guarantee users from the same geographic will be served from the same location for any compliance reasons Geolocation routing \u2013 Specify geographic locations by continent , country, state limited to US, is based on IP accuracy Failover routing \u2013 failover to a backup site if the primary site fails and becomes unreachable Weighted, Latency and Geolocation can be used for Active-Active while Failover routing can be used for Active-Passive multi region architecture","title":"L4 AWS Networking Services Cheat Sheet"},{"location":"chap2/4Ns_cs/#l4-aws-networking-services-cheat-sheet","text":"","title":"L4 AWS Networking Services Cheat Sheet"},{"location":"chap2/4Ns_cs/#1-virtual-private-cloud-vpc","text":"helps define a logically isolated dedicated virtual network within the AWS provides control of IP addressing using CIDR block from a minimum of /28 to maximum of /16 block size supports IPv4 and IPv6 addressing can be extended by associating secondary IPv4 CIDR blocks to VPC","title":"1 Virtual Private Cloud \u2013 VPC"},{"location":"chap2/4Ns_cs/#1-1-components","text":"Internet gateway (IGW) provides access to the Internet Virtual gateway (VGW) provides access to on-premises data center through VPN and Direct Connect connections VPC can have only one IGW and VGW Route tables determine where network traffic from subnet is directed Ability to create subnet with VPC CIDR block A Network Address Translation (NAT) server provides outbound Internet access for EC2 instances in private subnets Elastic IP addresses are static, persistent public IP addresses Instances launched in the VPC will have a Private IP address and can have a Public or a Elastic IP address associated with it Security Groups and NACLs help define security Flow logs \u2013 Capture information about the IP traffic going to and from network interfaces in your VPC","title":"1-1 Components"},{"location":"chap2/4Ns_cs/#1-2-tenancy-option-for-instances","text":"shared , by default, allows instances to be launched on shared tenancy dedicated allows instances to be launched on a dedicated hardware","title":"1-2 Tenancy option for instances"},{"location":"chap2/4Ns_cs/#1-3-route-tables","text":"defines rules, termed as routes, which determine where network traffic from the subnet would be routed Each VPC has a Main Route table, and can have multiple custom route tables created Every route table contains a local route that enables communication within a VPC which cannot be modified or deleted Route priority is decided by matching the most specific route in the route table that matches the traffic","title":"1-3 Route Tables"},{"location":"chap2/4Ns_cs/#1-4-subnets","text":"map to AZs and do not span across AZs have a CIDR range that is a portion of the whole VPC. CIDR ranges cannot overlap between subnets within the VPC. AWS reserves 5 IP addresses in each subnet \u2013 first 4 and last one Each subnet is associated with a route table which define its behavior Public subnets \u2013 inbound/outbound Internet connectivity via IGW Private subnets \u2013 outbound Internet connectivity via an NAT or VGW Protected subnets \u2013 no outbound connectivity and used for regulated workloads","title":"1-4 Subnets"},{"location":"chap2/4Ns_cs/#1-5-elastic-network-interface-eni","text":"a default ENI, eth0, is attached to an instance which cannot be detached with one or more secondary detachable ENIs (eth1-ethn) has primary private, one or more secondary private, public, Elastic IP address, security groups, MAC address and source/destination check flag attributes associated AN ENI in one subnet can be attached to an instance in the same or another subnet, in the same AZ and the same VPC Security group membership of an ENI can be changed with pre allocated Mac Address can be used for applications with special licensing requirements","title":"1-5 Elastic Network Interface (ENI)"},{"location":"chap2/4Ns_cs/#1-6-security-groups-vs-network-access-control-lists","text":"Stateful vs Stateless At instance level vs At subnet level Only allows Allow rule vs Allows both Allow and Deny rules Evaluated as a Whole vs Evaluated in defined Order","title":"1-6 Security Groups vs Network Access Control Lists"},{"location":"chap2/4Ns_cs/#1-7-elastic-ip","text":"is a static IP address designed for dynamic cloud computing. is associated with AWS account , and not a particular instance can be remapped from one instance to an other instance is charged for non usage , if not linked for any instance or instance associated is in stopped state","title":"1-7 Elastic IP"},{"location":"chap2/4Ns_cs/#1-8-nat","text":"allows internet access to instances in private subnet performs the function of both address translation and port address translation (PAT) needs source/destination check flag to be disabled as it is not actual destination of the traffic NAT gateway is a AWS managed NAT service that provides better availability, higher bandwidth, and requires less administrative effort are not supported for IPv6 traffic","title":"1-8 NAT"},{"location":"chap2/4Ns_cs/#1-9-egress-only-internet-gateways","text":"outbound communication over IPv6 from instances in the VPC to the Internet, and prevents the Internet from initiating an IPv6 connection with your instances supports IPv6 traffic only","title":"1-9 Egress-Only Internet Gateways"},{"location":"chap2/4Ns_cs/#1-10-shared-vpcs","text":"allows multiple AWS accounts to create their application resources, such as EC2 instances, RDS databases, Redshift clusters, and AWS Lambda functions, into shared, centrally-managed VPCs","title":"1-10 Shared VPCs"},{"location":"chap2/4Ns_cs/#1-11-vpc-peering","text":"allows routing of traffic between the peer VPCs using private IP addresses and no IGW or VGW required No single point of failure and bandwidth bottlenecks supports inter-region VPC peering IP space or CIDR blocks cannot overlap cannot be transitive , one-to-one relationship between two VPC Only one between any two VPCs and have to be explicitly peered Private DNS values cannot be resolved Security groups from peered VPC can now be referred, however the VPC should be in the same region.","title":"1-11 VPC Peering"},{"location":"chap2/4Ns_cs/#1-12-vpc-endpoints","text":"enables you to privately connect VPC to supported AWS services and VPC endpoint services powered by PrivateLink does not require a public IP address, access over the Internet, NAT device, a VPN connection or Direct Connect traffic between VPC & AWS service does not leave the Amazon network are virtual devices. are horizontally scaled, redundant, and highly available VPC components that allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic. Gateway Endpoints is a gateway that is a target for a specified route in the route table, used for traffic destined to a supported AWS service. only S3 and DynamoDB are currently supported Interface Endpoints is an elastic network interface with a private IP address that serves as an entry point for traffic destined to a supported service services supported API Gateway AWS CloudFormation, CloudWatch, CloudWatch Events, CloudWatch Logs AWS CodeBuild AWS CodeCommit AWS Config, EC2 API Elastic Load Balancing API, Elastic Container Registry, Elastic Container Service AWS Key Management Service, Kinesis Data Streams, SageMaker and, SageMaker Runtime, SageMaker Notebook Instance AWS Secrets Manager AWS Security Token Service AWS Service Catalog, SNS, SQS AWS Systems Manager","title":"1-12 VPC Endpoints"},{"location":"chap2/4Ns_cs/#2-cloudfront","text":"provides low latency and high data transfer speeds for distribution of static, dynamic web, or streaming content to web users. delivers the content through a worldwide network of data centers called Edge Locations or Point of Presence (PoPs) keeps persistent connections with the origin servers so that the files can be fetched from the origin servers as quickly as possible. dramatically reduces the number of network hops that users\u2019 requests must pass through supports multiple origin server options, like AWS hosted service for e.g. S3, EC2, ELB, or an on-premise server, which stores the original, definitive version of the objects single distribution can have multiple origins and Path pattern in a cache behavior determines which requests are routed to the origin Web distribution supports static, dynamic web content, on-demand using progressive download & HLS, and live streaming video content supports HTTPS using either dedicated IP address, which is expensive as a dedicated IP address is assigned to each CloudFront edge location Server Name Indication (SNI), which is free but supported by modern browsers only with the domain name available in the request header For E2E HTTPS connection, Viewers -> CloudFront needs either a self-signed certificate or a certificate issued by CA or ACM CloudFront -> Origin needs certificate issued by ACM for ELB and by CA for other origins","title":"2 CloudFront"},{"location":"chap2/4Ns_cs/#security","text":"Origin Access Identity (OAI) can be used to restrict the content from S3 origin to be accessible from CloudFront only supports Geo restriction (Geo-Blocking) to whitelist or blacklist countries that can access the content Signed URLs to restrict access to individual files, for e.g., an installation download for your application. users using a client, for e.g. a custom HTTP client, that doesn\u2019t support cookies Signed Cookies provide access to multiple restricted files , for e.g., video part files in HLS format or all of the files in the subscribers\u2019 area of a website . don\u2019t want to change the current URLs integrates with AWS WAF, a web application firewall that helps protect web applications from attacks by allowing rules configured based on IP addresses , HTTP headers, and custom URI strings supports GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE to get object & object headers, add, update, and delete objects only caches responses to GET and HEAD requests and, optionally, OPTIONS requests does not cache responses to PUT, POST, PATCH, DELETE request methods and these requests are proxied back to the origin","title":"Security"},{"location":"chap2/4Ns_cs/#object-removal-from-the-cache","text":"would be removed upon expiry (TTL) from the cache, by default 24 hrs can be invalidated explicitly, but has a cost associated, however, might continue to see the old version until it expires from those caches objects can be invalidated only for Web distribution use versioning or change object name, to serve a different version","title":"object removal from the cache"},{"location":"chap2/4Ns_cs/#supports-adding-or-modifying-custom-headers-before-the-request-is-sent-to-origin-which-can-be-used-to","text":"validate if a user is accessing the content from CDN identifying CDN from which the request was forwarded, in case of multiple CloudFront distributions for viewers not supporting CORS to return the Access-Control-Allow-Origin header for every request supports Partial GET requests using range header to download objects in smaller units improving the efficiency of partial downloads and recovery from partially failed transfers supports compression to compress and serve compressed files when viewer requests include Accept-Encoding: gzip in the request header supports different price classes to include all regions , or only the least expensive regions and other regions without the most expensive regions","title":"supports adding or modifying custom headers before the request is sent to origin which can be used to"},{"location":"chap2/4Ns_cs/#3-direct-connect-vpn","text":"","title":"3 Direct Connect &amp; VPN"},{"location":"chap2/4Ns_cs/#3-1-vpn","text":"provide secure IPSec connections from on-premise computers or services to AWS over the Internet is quick to setup, is cheap however it depends on the Internet speed","title":"3-1 VPN"},{"location":"chap2/4Ns_cs/#3-2-direct-connect","text":"is a network service that provides an alternative to using Internet to utilize AWS services by using private dedicated network connection provides Virtual Interfaces Private VIF to access instances within an VPC via VGW Public VIF to access non VPC services requires time to setup probably months , and should not be considered as an option if turnaround time is less does not provide redundancy , use either second direct connection or IPSec VPN connection Virtual Private Gateway is on the AWS side and Customer Gateway is on the Customer side route propagation is enabled on VGW and not on CGW","title":"3-2 Direct Connect"},{"location":"chap2/4Ns_cs/#3-2-direct-connect-vs-vpn-ipsec","text":"Expensive to Setup and Takes time vs Cheap & Immediate Dedicated private connections vs Internet Reduced data transfer rate vs Internet data transfer cost Consistent performance vs Internet inherent variability Do not provide Redundancy vs Provides Redundancy","title":"3-2 Direct Connect vs VPN IPSec"},{"location":"chap2/4Ns_cs/#4-route-53","text":"Highly available and scalable DNS & Domain Registration Service Reliable and cost-effective way to route end users to Internet applications Supports multi-region and backup architectures for High availability . ELB , limited to region, does not support multi region HA architecture supports private Intranet facing DNS service internal resource record sets only work for requests originating from within the VPC and currently cannot extend to on-premise Global propagation of any changes made to the DN records within ~ 1min Route 53 to create an alias resource record set that points to ELB, S3, CloudFront. An alias resource record set is a Route 53 extension to DNS. It\u2019s similar to a CNAME resource record set, but supports both for root domain \u2013 zone apex e.g. example.com, and for subdomains for e.g. www.example.com. CNAME resource record sets can be created only for subdomains and cannot be mapped to the zone apex record Route 53 Split-view (Split-horizon) DNS enables you to access an internal version of your website using the same domain name that is used publicly","title":"4 Route 53"},{"location":"chap2/4Ns_cs/#4-1-routing-policy","text":"Simple routing \u2013 simple round robin policy Weighted round robin \u2013 assign weights to resource records sets to specify the proportion for e.g. 80%:20% Latency based routing \u2013 helps improve global applications as request are sent to server from the location with minimal latency , is based on the latency and cannot guarantee users from the same geographic will be served from the same location for any compliance reasons Geolocation routing \u2013 Specify geographic locations by continent , country, state limited to US, is based on IP accuracy Failover routing \u2013 failover to a backup site if the primary site fails and becomes unreachable Weighted, Latency and Geolocation can be used for Active-Active while Failover routing can be used for Active-Passive multi region architecture","title":"4-1 Routing policy"},{"location":"chap2/5CS_cs/","text":"L5 AWS Compute Services Cheat Sheet 1 Elastic Cloud Compute \u2013 EC2 provides scalable computing capacity 1-1 Features Virtual computing environments, known as EC2 instances Preconfigured templates for EC2 instances, known as Amazon Machine Images (AMIs) , that package the bits needed for the server (including the operating system and additional software) Various configurations of CPU, memory, storage, and networking capacity for your instances, known as Instance types Secure login information for your instances using key pairs (public-private keys where private is kept by user) Storage volumes for temporary data that\u2019s deleted when you stop or terminate your instance, known as Instance store volumes Persistent storage volumes for data using Elastic Block Store (EBS) Multiple physical locations for your resources, such as instances and EBS volumes, known as Regions and Availability Zones A firewall to specify the protocols, ports, and source IP ranges that can reach your instances using Security Groups Static IP addresses, known as Elastic IP addresses Metadata, known as tags , can be created and assigned to EC2 resources Virtual networks that are logically isolated from the rest of the AWS cloud, and can optionally connect to on-premises network, known as Virtual private clouds (VPCs) 1-2 Amazon Machine Image \u2013 AMI template from which EC2 instances can be launched quickly does NOT span across regions , and needs to be copied can be shared with other specific AWS accounts or made public 1-3 Instance Types T for applications needing general usage T2 instances are Burstable Performance Instances that provide a baseline level of CPU performance with the ability to burst above the baseline. T2 instances accumulate CPU Credits when they are idle, and consume CPU Credits when they are active. T2 Unlimited Instances can sustain high CPU performance for as long as a workload needs it at an additional cost. R for applications needing more RAM or Memory C for applications needing more Compute M for applications needing more Medium or Moderate performance on both Memory and CPU I for applications needing more IOPS G for applications needing more GPU 2 Instance Purchasing Option 2-1 On-Demand Instances pay for instances and compute capacity that you use by the hour no long-term commitments or up-front payments 2-2 Reserved Instances provides lower hourly running costs by providing a billing discount capacity reservation is applied to instances suited if consistent, heavy, predictable usage provides benefits with Consolidate Billing can be modified to switch Availability Zones or the instance size within the same instance type, given the instance size footprint (Normalization factor) remains the same pay for the entire term regardless of the usage is not a physical instance that is launched, but rather a billing discount applied to the use of On-Demand Instances 2-3 Scheduled Reserved Instances enable capacity reservations purchase that recurs on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. enable capacity reservations purchase that recurs on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. good choice for workloads that do not run continuously, but do run on a regular schedule 2-4 Spot Instances cost-effective choice but does NOT guarantee availability applications flexible in the timing when they can run and also able to handle interruption by storing the state externally provides a two-minute warning if the instance is to be terminated to save any unsaved work Spot blocks can also be launched with a required duration, which are not interrupted due to changes in the Spot price Spot Fleet is a collection, or fleet, of Spot Instances, and optionally On-Demand Instances, which attempts to launch the number of Spot and On-Demand Instances to meet the specified target capacity 2-5 Dedicated Instances is a tenancy option that enables instances to run in VPC on hardware that\u2019s isolated, dedicated to a single customer 2-6 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use Light, Medium, and Heavy Utilization Reserved Instances are no longer available for purchase and were part of the Previous Generation AWS EC2 purchasing model 3 Enhanced Networking results in higher bandwidth, higher packet per second (PPS) performance, lower latency, consistency, scalability, and lower jitter supported using Single Root \u2013 I/O Virtualization (SR-IOV) only on supported instance types is supported only with a VPC (not EC2 Classic), HVM virtualization type and available by default on Amazon AMI but can be installed on other AMIs as well 4 Placement Group 4-1 Cluster Placement Group provide low latency, High-Performance Computing via 10Gbps network is a logical grouping on instances within a Single AZ don\u2019t span availability zones, can span multiple subnets but subnets must be in the same AZ can span across peered VPCs for the same Availability Zones An existing instance can be moved to a placement group , or moved from one placement group to another, or removed from a placement group, given it is in the stopped state. for capacity errors, stop and start the instances in the placement group use homogenous instance types which support enhanced networking and launch all the instances at once 4-3 Spread Placement Groups is a group of instances that are each placed on distinct underlying hardware i.e. each instance on a distinct rack across AZ recommended for applications that have a small number of critical instances that should be kept separate from each othe r. reduces the risk of simultaneous failures that might occur when instances share the same underlying hardware. 4-4 Partition Placement Groups is a group of instances spread across partitions i.e. group of instances spread across racks across AZs reduces the likelihood of correlated hardware failures for the application. can be used to spread deployment of large distributed and replicated workloads, such as HDFS, HBase, and Cassandra, across distinct hardware 5 EC2 Monitoring CloudWatch provides monitoring for EC2 instances Status monitoring helps quickly determine whether EC2 has detected any problems that might prevent instances from running applications. Status monitoring includes System Status checks \u2013 indicate issues with the underlying hardware Instance Status checks \u2013 indicate issues with the underlying instance. Elastic Load Balancer 6 Elastic Load Balancer Managed load balancing service and scales automatically distributes incoming application traffic across multiple EC2 instances is distributed system that is fault tolerant and actively monitored by AWS scales it as per the demand are engineered to not be a single point of failure need to Pre-Warm ELB if the demand is expected to shoot especially during load testing. AWS documentation does not mention it now. supports routing traffic to instances in multiple AZs in the same region performs Health Checks to route traffic only to the healthy instances support Listeners with HTTP, HTTPS, SSL, TCP protocols has an associated IPv4 and dual stack DNS name can offload the work of encryption and decryption (SSL termination) so that the EC2 instances can focus on their main work supports Cross Zone load balancing to help route traffic evenly across all EC2 instances regardless of the AZs they reside in to help identify the IP address of a client supports Proxy Protocol header for TCP/SSL connections supports X-Forward headers for HTTP/HTTPS connections supports Stick Sessions (session affinity) to bind a user\u2019s session to a specific application instance, it is not fault tolerant, if an instance is lost the information is lost requires HTTP/HTTPS listener and does not work with TCP requires SSL termination on ELB as it users the headers supports Connection draining to help complete the in-flight requests in case an instance is deregistered For High Availability, it is recommended to attach one subnet per AZ for at least two AZs , even if the instances are in a single subnet. supports Static/Elastic IP (NLB only) IPv4 & IPv6 support however VPC does not support IPv6. VPC now supports IPV6. HTTPS listener does not support Client Side Certificate For SSL termination at backend instances or support for Client Side Certificate use TCP for connections from the client to the ELB, use the SSL protocol for connections from the ELB to the back-end application, and deploy certificates on the back-end instances handling requests Uses Server Name Indication to supports multiple SSL certificates 7 Application Load Balancer supports HTTP and HTTPS (Secure HTTP) protocols supports HTTP/2 , which is enabled natively. Clients that support HTTP/2 can connect over TLS supports WebSockets and Secure WebSockets natively supports Request tracing , by default. request tracing can be used to track HTTP requests from clients to targets or other services. Load balancer upon receiving a request from a client, adds or updates the X-Amzn-Trace-Id header before sending the request to the target supports containerized applications . Using Dynamic port mapping , ECS can select an unused port when scheduling a task and register the task with a target group using this port. supports Sticky Sessions (Session Affinity) using load balancer generated cookies , to route requests from the same client to the same target supports SSL termination, to decrypt the request on ALB before sending it to the underlying targets . supports layer 7 specific features like X-Forwarded-For headers to help determine the actual client IP, port and protocol automatically scales its request handling capacity in response to incoming application traffic. supports hy brid load balancing, to route traffic to instances in VPC and an on-premises location provides High Availability , by allowing more than one AZ to be specified integrates with ACM to provision and bind a SSL/TLS certificate to the load balancer thereby making the entire SSL offload process very easy supports multiple certificates for the same domain to a secure listener supports IPv6 addressing, for an Internet facing load balancer supports Cross-zone load balancing , and cannot be disabled. supports Security Groups to control the traffic allowed to and from the load balancer . provides Access Logs, to record all requests sent the load balancer , and store the logs in S3 for later analysis in compressed format provides Delete Protection , to prevent the ALB from accidental deletion supports Connection Idle Timeout \u2013 A LB maintains two connections for each request one with the Client (front end) and one with the target instance (back end) . If no data has been sent or received by the time that the idle timeout period elapses, ALB closes the front-end connection integrates with CloudWatch to provide metrics such as request counts , error counts, error types, and request latency integrates with AWS WAF, a web application firewall that helps protect web applications from attacks by allowing rules configuration based on IP addresses, HTTP headers, and custom URI strings integrates with CloudTrail to receive a history of ALB API calls made on the AWS account back-end server authentication is NOT supported does not provide Static, Elastic IP addresses 8 Network Load Balancer handles volatile workloads and scale to millions of requests per second, without the need of pre-warming offers extremely low latencies for latency-sensitive applications provides static IP/Elastic IP addresses for the load balancer allows registering targets by IP address, including targets outside the VPC (on-premises) for the load balancer . supports containerized applications . Using Dynamic port mapping, ECS can select an unused port when scheduling a task and register the task with a target group using this port. monitors the health of its registered targets and routes the traffic only to healthy targets enable cross-zone loading balancing only after creating the NLB preserves client side source IP allowing the back-end to see client IP address. Target groups can be created with target type as instance ID or IP address. If targets registered by instance ID, the source IP addresses of the clients are preserved and provided to the applications. If register targets registered by IP address, the source IP addresses are the private IP addresses of the load balancer nodes. supports both network and application target health checks . supports long-lived TCP connections ideal for WebSocket type of applications supports Zonal Isolation , which is designed for application architectures in a single zone and can be enabled in a single AZ to support architectures that require zonal isolation does not support stick sessions 9 Auto Scaling ensures correct number of EC2 instances are always running to handle the load by scaling up or down automatically as demand changes cannot span multiple regions . attempts to distribute instances evenly between the AZs that are enabled for the Auto Scaling group performs checks either using EC2 status checks or can use ELB health checks to determine the health of an instance and terminates the instance if unhealthy, to launch a new instance can be scaled using manual scaling, scheduled scaling or demand based scaling cooldown period helps ensure instances are not launched or terminated before the previous scaling activity takes effect to allow the newly launched instances to start handling traffic and reduce load 10 AWS Auto Scaling & ELB Auto Scaling & ELB can be used for High Availability and Redundancy by spanning Auto Scaling groups across multiple AZs within a region and then setting up ELB to distribute incoming traffic across those AZs With Auto Scaling, use ELB health check with the instances to ensure that traffic is routed only to the healthy instances","title":"L5 AWS Compute Services Cheat Sheet"},{"location":"chap2/5CS_cs/#l5-aws-compute-services-cheat-sheet","text":"","title":"L5 AWS Compute Services Cheat Sheet"},{"location":"chap2/5CS_cs/#1-elastic-cloud-compute-ec2","text":"provides scalable computing capacity","title":"1 Elastic Cloud Compute \u2013 EC2"},{"location":"chap2/5CS_cs/#1-1-features","text":"Virtual computing environments, known as EC2 instances Preconfigured templates for EC2 instances, known as Amazon Machine Images (AMIs) , that package the bits needed for the server (including the operating system and additional software) Various configurations of CPU, memory, storage, and networking capacity for your instances, known as Instance types Secure login information for your instances using key pairs (public-private keys where private is kept by user) Storage volumes for temporary data that\u2019s deleted when you stop or terminate your instance, known as Instance store volumes Persistent storage volumes for data using Elastic Block Store (EBS) Multiple physical locations for your resources, such as instances and EBS volumes, known as Regions and Availability Zones A firewall to specify the protocols, ports, and source IP ranges that can reach your instances using Security Groups Static IP addresses, known as Elastic IP addresses Metadata, known as tags , can be created and assigned to EC2 resources Virtual networks that are logically isolated from the rest of the AWS cloud, and can optionally connect to on-premises network, known as Virtual private clouds (VPCs)","title":"1-1 Features"},{"location":"chap2/5CS_cs/#1-2-amazon-machine-image-ami","text":"template from which EC2 instances can be launched quickly does NOT span across regions , and needs to be copied can be shared with other specific AWS accounts or made public","title":"1-2 Amazon Machine Image \u2013 AMI"},{"location":"chap2/5CS_cs/#1-3-instance-types","text":"T for applications needing general usage T2 instances are Burstable Performance Instances that provide a baseline level of CPU performance with the ability to burst above the baseline. T2 instances accumulate CPU Credits when they are idle, and consume CPU Credits when they are active. T2 Unlimited Instances can sustain high CPU performance for as long as a workload needs it at an additional cost. R for applications needing more RAM or Memory C for applications needing more Compute M for applications needing more Medium or Moderate performance on both Memory and CPU I for applications needing more IOPS G for applications needing more GPU","title":"1-3 Instance Types"},{"location":"chap2/5CS_cs/#2-instance-purchasing-option","text":"","title":"2 Instance Purchasing Option"},{"location":"chap2/5CS_cs/#2-1-on-demand-instances","text":"pay for instances and compute capacity that you use by the hour no long-term commitments or up-front payments","title":"2-1 On-Demand Instances"},{"location":"chap2/5CS_cs/#2-2-reserved-instances","text":"provides lower hourly running costs by providing a billing discount capacity reservation is applied to instances suited if consistent, heavy, predictable usage provides benefits with Consolidate Billing can be modified to switch Availability Zones or the instance size within the same instance type, given the instance size footprint (Normalization factor) remains the same pay for the entire term regardless of the usage is not a physical instance that is launched, but rather a billing discount applied to the use of On-Demand Instances","title":"2-2 Reserved Instances"},{"location":"chap2/5CS_cs/#2-3-scheduled-reserved-instances","text":"enable capacity reservations purchase that recurs on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. enable capacity reservations purchase that recurs on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. good choice for workloads that do not run continuously, but do run on a regular schedule","title":"2-3 Scheduled Reserved Instances"},{"location":"chap2/5CS_cs/#2-4-spot-instances","text":"cost-effective choice but does NOT guarantee availability applications flexible in the timing when they can run and also able to handle interruption by storing the state externally provides a two-minute warning if the instance is to be terminated to save any unsaved work Spot blocks can also be launched with a required duration, which are not interrupted due to changes in the Spot price Spot Fleet is a collection, or fleet, of Spot Instances, and optionally On-Demand Instances, which attempts to launch the number of Spot and On-Demand Instances to meet the specified target capacity","title":"2-4 Spot Instances"},{"location":"chap2/5CS_cs/#2-5-dedicated-instances","text":"is a tenancy option that enables instances to run in VPC on hardware that\u2019s isolated, dedicated to a single customer","title":"2-5 Dedicated Instances"},{"location":"chap2/5CS_cs/#2-6-dedicated-host","text":"is a physical server with EC2 instance capacity fully dedicated to your use Light, Medium, and Heavy Utilization Reserved Instances are no longer available for purchase and were part of the Previous Generation AWS EC2 purchasing model","title":"2-6 Dedicated Host"},{"location":"chap2/5CS_cs/#3-enhanced-networking","text":"results in higher bandwidth, higher packet per second (PPS) performance, lower latency, consistency, scalability, and lower jitter supported using Single Root \u2013 I/O Virtualization (SR-IOV) only on supported instance types is supported only with a VPC (not EC2 Classic), HVM virtualization type and available by default on Amazon AMI but can be installed on other AMIs as well","title":"3 Enhanced Networking"},{"location":"chap2/5CS_cs/#4-placement-group","text":"","title":"4 Placement Group"},{"location":"chap2/5CS_cs/#4-1-cluster-placement-group","text":"provide low latency, High-Performance Computing via 10Gbps network is a logical grouping on instances within a Single AZ don\u2019t span availability zones, can span multiple subnets but subnets must be in the same AZ can span across peered VPCs for the same Availability Zones An existing instance can be moved to a placement group , or moved from one placement group to another, or removed from a placement group, given it is in the stopped state. for capacity errors, stop and start the instances in the placement group use homogenous instance types which support enhanced networking and launch all the instances at once","title":"4-1 Cluster Placement Group"},{"location":"chap2/5CS_cs/#4-3-spread-placement-groups","text":"is a group of instances that are each placed on distinct underlying hardware i.e. each instance on a distinct rack across AZ recommended for applications that have a small number of critical instances that should be kept separate from each othe r. reduces the risk of simultaneous failures that might occur when instances share the same underlying hardware.","title":"4-3 Spread Placement Groups"},{"location":"chap2/5CS_cs/#4-4-partition-placement-groups","text":"is a group of instances spread across partitions i.e. group of instances spread across racks across AZs reduces the likelihood of correlated hardware failures for the application. can be used to spread deployment of large distributed and replicated workloads, such as HDFS, HBase, and Cassandra, across distinct hardware","title":"4-4 Partition Placement Groups"},{"location":"chap2/5CS_cs/#5-ec2-monitoring","text":"CloudWatch provides monitoring for EC2 instances Status monitoring helps quickly determine whether EC2 has detected any problems that might prevent instances from running applications. Status monitoring includes System Status checks \u2013 indicate issues with the underlying hardware Instance Status checks \u2013 indicate issues with the underlying instance. Elastic Load Balancer","title":"5 EC2 Monitoring"},{"location":"chap2/5CS_cs/#6-elastic-load-balancer","text":"Managed load balancing service and scales automatically distributes incoming application traffic across multiple EC2 instances is distributed system that is fault tolerant and actively monitored by AWS scales it as per the demand are engineered to not be a single point of failure need to Pre-Warm ELB if the demand is expected to shoot especially during load testing. AWS documentation does not mention it now. supports routing traffic to instances in multiple AZs in the same region performs Health Checks to route traffic only to the healthy instances support Listeners with HTTP, HTTPS, SSL, TCP protocols has an associated IPv4 and dual stack DNS name can offload the work of encryption and decryption (SSL termination) so that the EC2 instances can focus on their main work supports Cross Zone load balancing to help route traffic evenly across all EC2 instances regardless of the AZs they reside in to help identify the IP address of a client supports Proxy Protocol header for TCP/SSL connections supports X-Forward headers for HTTP/HTTPS connections supports Stick Sessions (session affinity) to bind a user\u2019s session to a specific application instance, it is not fault tolerant, if an instance is lost the information is lost requires HTTP/HTTPS listener and does not work with TCP requires SSL termination on ELB as it users the headers supports Connection draining to help complete the in-flight requests in case an instance is deregistered For High Availability, it is recommended to attach one subnet per AZ for at least two AZs , even if the instances are in a single subnet. supports Static/Elastic IP (NLB only) IPv4 & IPv6 support however VPC does not support IPv6. VPC now supports IPV6. HTTPS listener does not support Client Side Certificate For SSL termination at backend instances or support for Client Side Certificate use TCP for connections from the client to the ELB, use the SSL protocol for connections from the ELB to the back-end application, and deploy certificates on the back-end instances handling requests Uses Server Name Indication to supports multiple SSL certificates","title":"6 Elastic Load Balancer"},{"location":"chap2/5CS_cs/#7-application-load-balancer","text":"supports HTTP and HTTPS (Secure HTTP) protocols supports HTTP/2 , which is enabled natively. Clients that support HTTP/2 can connect over TLS supports WebSockets and Secure WebSockets natively supports Request tracing , by default. request tracing can be used to track HTTP requests from clients to targets or other services. Load balancer upon receiving a request from a client, adds or updates the X-Amzn-Trace-Id header before sending the request to the target supports containerized applications . Using Dynamic port mapping , ECS can select an unused port when scheduling a task and register the task with a target group using this port. supports Sticky Sessions (Session Affinity) using load balancer generated cookies , to route requests from the same client to the same target supports SSL termination, to decrypt the request on ALB before sending it to the underlying targets . supports layer 7 specific features like X-Forwarded-For headers to help determine the actual client IP, port and protocol automatically scales its request handling capacity in response to incoming application traffic. supports hy brid load balancing, to route traffic to instances in VPC and an on-premises location provides High Availability , by allowing more than one AZ to be specified integrates with ACM to provision and bind a SSL/TLS certificate to the load balancer thereby making the entire SSL offload process very easy supports multiple certificates for the same domain to a secure listener supports IPv6 addressing, for an Internet facing load balancer supports Cross-zone load balancing , and cannot be disabled. supports Security Groups to control the traffic allowed to and from the load balancer . provides Access Logs, to record all requests sent the load balancer , and store the logs in S3 for later analysis in compressed format provides Delete Protection , to prevent the ALB from accidental deletion supports Connection Idle Timeout \u2013 A LB maintains two connections for each request one with the Client (front end) and one with the target instance (back end) . If no data has been sent or received by the time that the idle timeout period elapses, ALB closes the front-end connection integrates with CloudWatch to provide metrics such as request counts , error counts, error types, and request latency integrates with AWS WAF, a web application firewall that helps protect web applications from attacks by allowing rules configuration based on IP addresses, HTTP headers, and custom URI strings integrates with CloudTrail to receive a history of ALB API calls made on the AWS account back-end server authentication is NOT supported does not provide Static, Elastic IP addresses","title":"7 Application Load Balancer"},{"location":"chap2/5CS_cs/#8-network-load-balancer","text":"handles volatile workloads and scale to millions of requests per second, without the need of pre-warming offers extremely low latencies for latency-sensitive applications provides static IP/Elastic IP addresses for the load balancer allows registering targets by IP address, including targets outside the VPC (on-premises) for the load balancer . supports containerized applications . Using Dynamic port mapping, ECS can select an unused port when scheduling a task and register the task with a target group using this port. monitors the health of its registered targets and routes the traffic only to healthy targets enable cross-zone loading balancing only after creating the NLB preserves client side source IP allowing the back-end to see client IP address. Target groups can be created with target type as instance ID or IP address. If targets registered by instance ID, the source IP addresses of the clients are preserved and provided to the applications. If register targets registered by IP address, the source IP addresses are the private IP addresses of the load balancer nodes. supports both network and application target health checks . supports long-lived TCP connections ideal for WebSocket type of applications supports Zonal Isolation , which is designed for application architectures in a single zone and can be enabled in a single AZ to support architectures that require zonal isolation does not support stick sessions","title":"8 Network Load Balancer"},{"location":"chap2/5CS_cs/#9-auto-scaling","text":"ensures correct number of EC2 instances are always running to handle the load by scaling up or down automatically as demand changes cannot span multiple regions . attempts to distribute instances evenly between the AZs that are enabled for the Auto Scaling group performs checks either using EC2 status checks or can use ELB health checks to determine the health of an instance and terminates the instance if unhealthy, to launch a new instance can be scaled using manual scaling, scheduled scaling or demand based scaling cooldown period helps ensure instances are not launched or terminated before the previous scaling activity takes effect to allow the newly launched instances to start handling traffic and reduce load","title":"9 Auto Scaling"},{"location":"chap2/5CS_cs/#10-aws-auto-scaling-elb","text":"Auto Scaling & ELB can be used for High Availability and Redundancy by spanning Auto Scaling groups across multiple AZs within a region and then setting up ELB to distribute incoming traffic across those AZs With Auto Scaling, use ELB health check with the instances to ensure that traffic is routed only to the healthy instances","title":"10 AWS Auto Scaling &amp; ELB"},{"location":"chap2/6DS_cs/","text":"L6 AWS Database Services Cheat Sheet 1 Relational Database Service \u2013 RDS provides Relational Database service supports MySQL, MariaDB, PostgreSQL, Oracle, Microsoft SQL Server, and the new, MySQL-compatible Amazon Aurora DB engine as it is a managed service, shell (root ssh) access is not provided manages backups, software patching, automatic failure detection, and recovery supports use initiated manual backups and snapshots daily automated backups with database transaction logs enables Point in Time recovery up to the last five minutes of database usage snapshots are user-initiated storage volume snapshot of DB instance, backing up the entire DB instance and not just individual databases that can be restored as a independent RDS instance 1-1 RDS Security support encryption at rest using KMS as well as encryption in transit using SSL endpoints supports IAM database authentication, which prevents the need to store static user credentials in the database, because authentication is managed externally using IAM . supports Encryption only during creation of an RDS DB instance existing unencrypted DB cannot be encrypted and y ou need to create a snapshot, created a encrypted copy of the snapshot and restore as encrypted DB supports Secret Manager for storing and rotating secrets for encrypted database logs, snapshots, backups, read replicas are all encrypted as well cross region replicas and snapshots does not work across region (Note \u2013 this is possible now with latest AWS enhancement) 1-2 Multi-AZ deployment provides high availability and automatic failover support and is NOT a scaling solution maintains a synchronous standby replica in a different AZ transaction success is returned only if the commit is successful both on the primary and the standby DB Oracle, PostgreSQL, MySQL, and MariaDB DB instances use Amazon technology , while SQL Server DB instances use SQL Server Mirroring snapshots and backups are taken from standby & eliminate I/O freezes during automatic failover, its seamless and RDS switches to the standby instance and updates the DNS record to point to standby failover can be forced with the Reboot with failover option 1-3 Read Replicas uses the PostgreSQL, MySQL, and MariaDB DB engines\u2019 built-in replication functionality to create a separate Read Only instance updates are asynchronously copied to the Read Replica, and data might be stale can help scale applications and reduce read only load requires automatic backups enabled replicates all databases in the source DB instance for disaster recovery, can be promoted to a full fledged database can be created in a different region for disaster recovery, migration and low latency across regions can\u2019t create encrypted read replicas from unencrypted DB or read replica RDS does not support all the features of underlying databases, and if required the database instance can be launched on an EC2 instance 1-4 RDS Components DB parameter groups contains engine configuration values that can be applied to one or more DB instances of the same instance type for e.g. SSL, max connections etc. Default DB parameter group cannot be modified, create a custom one and attach to the DB Supports static and dynamic parameters changes to dynamic parameters are applied immediately (irrespective of apply immediately setting) changes to static parameters are NOT applied immediately and require a manual reboot . 1-5 RDS Monitoring & Notification integrates with CloudWatch and CloudTrail CloudWatch provides metrics about CPU utilization from the hypervisor for a DB instance, and Enhanced Monitoring gathers its metrics from an agent on the instance Performance Insights is a database performance tuning and monitoring feature that helps illustrate the database\u2019s performance and help analyze any issues that affect it supports RDS Event Notification which uses the SNS to provide notification when an RDS event like creation, deletion or snapshot creation etc occurs 2 Aurora is a relational database engine that combines the speed and reliability of high-end commercial databases with the simplicity and cost-effectiveness of open source databases is a managed services and handles time-consuming tasks such as provisioning, patching, backup, recovery, failure detection and repair is a proprietary technology from AWS (not open sourced) provides PostgreSQL and MySQL compatibility is \u201cAWS cloud optimized\u201d and claims 5x performance improvement over MySQL on RDS, over 3x the performance of PostgreSQL on RDS scales storage automatically in increments of 10GB, up to 64 TB with no impact to database performance. Storage is striped across 100s of volumes. no need to provision storage in advance . provides self-healing storage . Data blocks and disks are continuously scanned for errors and repaired automatically. provides instantaneous failover replicates each chunk of my the database volume six ways across three Availability Zones i.e. 6 copies of the data across 3 AZ requires 4 copies out of 6 needed for writes requires 3 copies out of 6 need for reads costs more than RDS (20% more) \u2013 but is more efficient 2-1 Read Replicas can have 15 replicas while MySQL has 5, and the replication process is faster (sub 10 ms replica lag) share the same data volume as the primary instance in the same AWS Region, there is virtually no replication lag supports Automated failover for master in less than 30 seconds supports Cross Region Replication using either physical or logical replication . 2-2 Security supports Encryption at rest using KMS supports Encryption in flight using SSL (same process as MySQL or Postgres) Automated backups, snapshots and replicas are also encrypted Possibility to authenticate using IAM token (same method as RDS) supports protecting the instance with security groups does not support SSH access to the underlying servers 2-3 Aurora Serverless provides automated database Client instantiation and on-demand autoscaling based on actual usage provides a relatively simple, cost-effective option for infrequent, intermittent, or unpredictable workloads automatically starts up, shuts down, and scales capacity up or down based on the application\u2019s needs. No capacity planning needed Pay per second, can be more cost-effective 2-3 Aurora Global Database allows a single Aurora database to span multiple AWS regions. provides Physical replication , which uses dedicated infrastructure that leaves the databases entirely available to serve the application supports 1 Primary Region (read / write) replicates across up to 5 secondary (read-only) regions, replication lag is less than 1 second supports up to 16 Read Replicas per secondary region recommended for low-latency global reads and disaster recovery with an RTO of < 1 minute failover is not automated and if the primary region becomes unavailable, a secondary region can be manually removed from an Aurora Global Database and promote it to take full reads and writes. Application needs to be updated to point to the newly promoted region. 2-4 Aurora Backtrack Backtracking \u201crewinds\u201d the DB cluster to the specified time Backtracking performs in place restore and does not create a new instance. There is a minimal downtime associated with it. 2-5 Aurora Clone feature allows quick and cost-effective creation of Aurora Cluster duplicates supports parallel or distributed query using Aurora Parallel Query , which refers to the ability to push down and distribute the computational load of a single query across thousands of CPUs in Aurora\u2019s storage layer. 3 DynamoDB fully managed NoSQL database service synchronously replicates data across three facilities in an AWS Region , giving high availability and data durability runs exclusively on SSDs to provide high I/O performance provides provisioned table reads and writes automatically partitions, reallocates and re-partitions the data and provisions additional server capacity as data or throughput changes creates and maintains indexes for the primary key attributes for efficient access of data in the table 3-1 supports Secondary Indexes allows querying attributes other then the primary key attributes without impacting performance. are automatically maintained as sparse objects 3-2 Local vs Global secondary index shares partition key + different sort key vs different partition + sort key search limited to partition vs across all partition unique attributes vs non unique attributes linked to the base table vs independent separate index only created during the base table creation vs can be created later cannot be deleted after creation vs can be deleted consumes provisioned throughput capacity of the base table vs independent throughput returns all attributes for item vs only projected attributes Eventually or Strongly vs Only Eventually consistent reads size limited to 10Gb per partition vs unlimited 3-3 DynamoDB Consistency provides Eventually consistent (by default) or Strongly Consistent option to be specified during an read operation supports Strongly consistent reads for few operations like Query, GetItem, and BatchGetItem using the ConsistentRead parameter 3-4 DynamoDB Throughput Capacity supports On-demand and Provisioned read/write capacity modes Provisioned mode requires the number of reads and writes per second as required by the application to be specified On-demand mode provides flexible billing option capable of serving thousands of requests per second without capacity planning 3-4 DynamoDB Global Tables DynamoDB Global Tables provide multi-master, cross-region replication capability of DynamoDB to support data access locality and regional fault tolerance for database workloads. 3-4 DynamoDB Streams provides a time-ordered sequence of item-level changes made to data in a table 3-5 DynamoDB Time to Live (TTL) enables a per-item timestamp to determine when an item expiry expired items are deleted from the table without consuming any write throughput. supports cross region replication using DynamoDB streams which leverages Kinesis and provides time-ordered sequence of item-level changes and can help for lower RPO, lower RTO disaster recovery 3-6 DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement \u2013 from milliseconds to microseconds \u2013 even at millions of requests per second. supports triggers to allow execution of custom actions or notifications based on item-level updates Data Pipeline jobs with EMR can be used for disaster recovery with higher RPO, lower RTO requirements 4 ElastiCache managed web service that provides in-memory caching to deploy and run Memcached or Redis protocol-compliant cache clusters 4-1 ElastiCache with Redis like RDS, supports Multi-AZ, Read Replicas and Snapshots Read Replicas are created across AZ within same region using Redis\u2019s asynchronous replication technology Multi-AZ differs from RDS as there is no standby, but if the primary goes down a Read Replica is promoted as primary Read Replicas cannot span across regions , as RDS supports cannot be scaled out and if scaled up cannot be scaled down allows snapshots for backup and restore AOF can be enabled for recovery scenarios , to recover the data in case the node fails or service crashes. But it does not help in case the underlying hardware fails Enabling Redis Multi-AZ as a Better Approach to Fault Tolerance 4-2 ElastiCache with Memcached can be scaled up by increasing size and scaled out by adding nodes nodes can span across multiple AZs within the same region cached data is spread across the nodes , and a node failure will always result in some data loss from the cluster supports auto discovery every node should be homogenous and of same instance type 4-3 ElastiCache Redis vs Memcached complex data objects vs simple key value storage persistent vs non persistent, pure caching automatic failover with Multi-AZ vs Multi-AZ not supported scaling using Read Replicas vs using multiple nodes backup & restore supported vs not supported can be used state management to keep the web application stateless 5 Redshift fully managed, fast and powerful, petabyte scale data warehouse service uses replication and continuous backups to enhance availability and improve data durability and can automatically recover from node and component failures provides Massive Parallel Processing (MPP) by distributing & parallelizing queries across multiple physical resources columnar data storage improving query performance and allowing advance compression techniques only supports Single-AZ deployments and the nodes are available within the same AZ, if the AZ supports Redshift clusters spot instances are vs an option","title":"L6 AWS Database Services Cheat Sheet"},{"location":"chap2/6DS_cs/#l6-aws-database-services-cheat-sheet","text":"","title":"L6 AWS Database Services Cheat Sheet"},{"location":"chap2/6DS_cs/#1-relational-database-service-rds","text":"provides Relational Database service supports MySQL, MariaDB, PostgreSQL, Oracle, Microsoft SQL Server, and the new, MySQL-compatible Amazon Aurora DB engine as it is a managed service, shell (root ssh) access is not provided manages backups, software patching, automatic failure detection, and recovery supports use initiated manual backups and snapshots daily automated backups with database transaction logs enables Point in Time recovery up to the last five minutes of database usage snapshots are user-initiated storage volume snapshot of DB instance, backing up the entire DB instance and not just individual databases that can be restored as a independent RDS instance","title":"1 Relational Database Service \u2013 RDS"},{"location":"chap2/6DS_cs/#1-1-rds-security","text":"support encryption at rest using KMS as well as encryption in transit using SSL endpoints supports IAM database authentication, which prevents the need to store static user credentials in the database, because authentication is managed externally using IAM . supports Encryption only during creation of an RDS DB instance existing unencrypted DB cannot be encrypted and y ou need to create a snapshot, created a encrypted copy of the snapshot and restore as encrypted DB supports Secret Manager for storing and rotating secrets for encrypted database logs, snapshots, backups, read replicas are all encrypted as well cross region replicas and snapshots does not work across region (Note \u2013 this is possible now with latest AWS enhancement)","title":"1-1 RDS Security"},{"location":"chap2/6DS_cs/#1-2-multi-az-deployment","text":"provides high availability and automatic failover support and is NOT a scaling solution maintains a synchronous standby replica in a different AZ transaction success is returned only if the commit is successful both on the primary and the standby DB Oracle, PostgreSQL, MySQL, and MariaDB DB instances use Amazon technology , while SQL Server DB instances use SQL Server Mirroring snapshots and backups are taken from standby & eliminate I/O freezes during automatic failover, its seamless and RDS switches to the standby instance and updates the DNS record to point to standby failover can be forced with the Reboot with failover option","title":"1-2 Multi-AZ deployment"},{"location":"chap2/6DS_cs/#1-3-read-replicas","text":"uses the PostgreSQL, MySQL, and MariaDB DB engines\u2019 built-in replication functionality to create a separate Read Only instance updates are asynchronously copied to the Read Replica, and data might be stale can help scale applications and reduce read only load requires automatic backups enabled replicates all databases in the source DB instance for disaster recovery, can be promoted to a full fledged database can be created in a different region for disaster recovery, migration and low latency across regions can\u2019t create encrypted read replicas from unencrypted DB or read replica RDS does not support all the features of underlying databases, and if required the database instance can be launched on an EC2 instance","title":"1-3 Read Replicas"},{"location":"chap2/6DS_cs/#1-4-rds-components","text":"DB parameter groups contains engine configuration values that can be applied to one or more DB instances of the same instance type for e.g. SSL, max connections etc. Default DB parameter group cannot be modified, create a custom one and attach to the DB Supports static and dynamic parameters changes to dynamic parameters are applied immediately (irrespective of apply immediately setting) changes to static parameters are NOT applied immediately and require a manual reboot .","title":"1-4 RDS Components"},{"location":"chap2/6DS_cs/#1-5-rds-monitoring-notification","text":"integrates with CloudWatch and CloudTrail CloudWatch provides metrics about CPU utilization from the hypervisor for a DB instance, and Enhanced Monitoring gathers its metrics from an agent on the instance Performance Insights is a database performance tuning and monitoring feature that helps illustrate the database\u2019s performance and help analyze any issues that affect it supports RDS Event Notification which uses the SNS to provide notification when an RDS event like creation, deletion or snapshot creation etc occurs","title":"1-5 RDS Monitoring &amp; Notification"},{"location":"chap2/6DS_cs/#2-aurora","text":"is a relational database engine that combines the speed and reliability of high-end commercial databases with the simplicity and cost-effectiveness of open source databases is a managed services and handles time-consuming tasks such as provisioning, patching, backup, recovery, failure detection and repair is a proprietary technology from AWS (not open sourced) provides PostgreSQL and MySQL compatibility is \u201cAWS cloud optimized\u201d and claims 5x performance improvement over MySQL on RDS, over 3x the performance of PostgreSQL on RDS scales storage automatically in increments of 10GB, up to 64 TB with no impact to database performance. Storage is striped across 100s of volumes. no need to provision storage in advance . provides self-healing storage . Data blocks and disks are continuously scanned for errors and repaired automatically. provides instantaneous failover replicates each chunk of my the database volume six ways across three Availability Zones i.e. 6 copies of the data across 3 AZ requires 4 copies out of 6 needed for writes requires 3 copies out of 6 need for reads costs more than RDS (20% more) \u2013 but is more efficient","title":"2 Aurora"},{"location":"chap2/6DS_cs/#2-1-read-replicas","text":"can have 15 replicas while MySQL has 5, and the replication process is faster (sub 10 ms replica lag) share the same data volume as the primary instance in the same AWS Region, there is virtually no replication lag supports Automated failover for master in less than 30 seconds supports Cross Region Replication using either physical or logical replication .","title":"2-1 Read Replicas"},{"location":"chap2/6DS_cs/#2-2-security","text":"supports Encryption at rest using KMS supports Encryption in flight using SSL (same process as MySQL or Postgres) Automated backups, snapshots and replicas are also encrypted Possibility to authenticate using IAM token (same method as RDS) supports protecting the instance with security groups does not support SSH access to the underlying servers","title":"2-2 Security"},{"location":"chap2/6DS_cs/#2-3-aurora-serverless","text":"provides automated database Client instantiation and on-demand autoscaling based on actual usage provides a relatively simple, cost-effective option for infrequent, intermittent, or unpredictable workloads automatically starts up, shuts down, and scales capacity up or down based on the application\u2019s needs. No capacity planning needed Pay per second, can be more cost-effective","title":"2-3 Aurora Serverless"},{"location":"chap2/6DS_cs/#2-3-aurora-global-database","text":"allows a single Aurora database to span multiple AWS regions. provides Physical replication , which uses dedicated infrastructure that leaves the databases entirely available to serve the application supports 1 Primary Region (read / write) replicates across up to 5 secondary (read-only) regions, replication lag is less than 1 second supports up to 16 Read Replicas per secondary region recommended for low-latency global reads and disaster recovery with an RTO of < 1 minute failover is not automated and if the primary region becomes unavailable, a secondary region can be manually removed from an Aurora Global Database and promote it to take full reads and writes. Application needs to be updated to point to the newly promoted region.","title":"2-3 Aurora Global Database"},{"location":"chap2/6DS_cs/#2-4-aurora-backtrack","text":"Backtracking \u201crewinds\u201d the DB cluster to the specified time Backtracking performs in place restore and does not create a new instance. There is a minimal downtime associated with it.","title":"2-4 Aurora Backtrack"},{"location":"chap2/6DS_cs/#2-5-aurora-clone-feature","text":"allows quick and cost-effective creation of Aurora Cluster duplicates supports parallel or distributed query using Aurora Parallel Query , which refers to the ability to push down and distribute the computational load of a single query across thousands of CPUs in Aurora\u2019s storage layer.","title":"2-5 Aurora Clone feature"},{"location":"chap2/6DS_cs/#3-dynamodb","text":"fully managed NoSQL database service synchronously replicates data across three facilities in an AWS Region , giving high availability and data durability runs exclusively on SSDs to provide high I/O performance provides provisioned table reads and writes automatically partitions, reallocates and re-partitions the data and provisions additional server capacity as data or throughput changes creates and maintains indexes for the primary key attributes for efficient access of data in the table","title":"3 DynamoDB"},{"location":"chap2/6DS_cs/#3-1-supports-secondary-indexes","text":"allows querying attributes other then the primary key attributes without impacting performance. are automatically maintained as sparse objects","title":"3-1 supports Secondary Indexes"},{"location":"chap2/6DS_cs/#3-2-local-vs-global-secondary-index","text":"shares partition key + different sort key vs different partition + sort key search limited to partition vs across all partition unique attributes vs non unique attributes linked to the base table vs independent separate index only created during the base table creation vs can be created later cannot be deleted after creation vs can be deleted consumes provisioned throughput capacity of the base table vs independent throughput returns all attributes for item vs only projected attributes Eventually or Strongly vs Only Eventually consistent reads size limited to 10Gb per partition vs unlimited","title":"3-2 Local vs Global secondary index"},{"location":"chap2/6DS_cs/#3-3-dynamodb-consistency","text":"provides Eventually consistent (by default) or Strongly Consistent option to be specified during an read operation supports Strongly consistent reads for few operations like Query, GetItem, and BatchGetItem using the ConsistentRead parameter","title":"3-3 DynamoDB Consistency"},{"location":"chap2/6DS_cs/#3-4-dynamodb-throughput-capacity","text":"supports On-demand and Provisioned read/write capacity modes Provisioned mode requires the number of reads and writes per second as required by the application to be specified On-demand mode provides flexible billing option capable of serving thousands of requests per second without capacity planning","title":"3-4 DynamoDB Throughput Capacity"},{"location":"chap2/6DS_cs/#3-4-dynamodb-global-tables","text":"DynamoDB Global Tables provide multi-master, cross-region replication capability of DynamoDB to support data access locality and regional fault tolerance for database workloads.","title":"3-4 DynamoDB Global Tables"},{"location":"chap2/6DS_cs/#3-4-dynamodb-streams","text":"provides a time-ordered sequence of item-level changes made to data in a table","title":"3-4 DynamoDB Streams"},{"location":"chap2/6DS_cs/#3-5-dynamodb-time-to-live-ttl","text":"enables a per-item timestamp to determine when an item expiry expired items are deleted from the table without consuming any write throughput. supports cross region replication using DynamoDB streams which leverages Kinesis and provides time-ordered sequence of item-level changes and can help for lower RPO, lower RTO disaster recovery","title":"3-5 DynamoDB Time to Live (TTL)"},{"location":"chap2/6DS_cs/#3-6-dynamodb-accelerator-dax","text":"is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement \u2013 from milliseconds to microseconds \u2013 even at millions of requests per second. supports triggers to allow execution of custom actions or notifications based on item-level updates Data Pipeline jobs with EMR can be used for disaster recovery with higher RPO, lower RTO requirements","title":"3-6 DynamoDB Accelerator (DAX)"},{"location":"chap2/6DS_cs/#4-elasticache","text":"managed web service that provides in-memory caching to deploy and run Memcached or Redis protocol-compliant cache clusters","title":"4 ElastiCache"},{"location":"chap2/6DS_cs/#4-1-elasticache-with-redis","text":"like RDS, supports Multi-AZ, Read Replicas and Snapshots Read Replicas are created across AZ within same region using Redis\u2019s asynchronous replication technology Multi-AZ differs from RDS as there is no standby, but if the primary goes down a Read Replica is promoted as primary Read Replicas cannot span across regions , as RDS supports cannot be scaled out and if scaled up cannot be scaled down allows snapshots for backup and restore AOF can be enabled for recovery scenarios , to recover the data in case the node fails or service crashes. But it does not help in case the underlying hardware fails Enabling Redis Multi-AZ as a Better Approach to Fault Tolerance","title":"4-1 ElastiCache with Redis"},{"location":"chap2/6DS_cs/#4-2-elasticache-with-memcached","text":"can be scaled up by increasing size and scaled out by adding nodes nodes can span across multiple AZs within the same region cached data is spread across the nodes , and a node failure will always result in some data loss from the cluster supports auto discovery every node should be homogenous and of same instance type","title":"4-2 ElastiCache with Memcached"},{"location":"chap2/6DS_cs/#4-3-elasticache-redis-vs-memcached","text":"complex data objects vs simple key value storage persistent vs non persistent, pure caching automatic failover with Multi-AZ vs Multi-AZ not supported scaling using Read Replicas vs using multiple nodes backup & restore supported vs not supported can be used state management to keep the web application stateless","title":"4-3 ElastiCache Redis vs Memcached"},{"location":"chap2/6DS_cs/#5-redshift","text":"fully managed, fast and powerful, petabyte scale data warehouse service uses replication and continuous backups to enhance availability and improve data durability and can automatically recover from node and component failures provides Massive Parallel Processing (MPP) by distributing & parallelizing queries across multiple physical resources columnar data storage improving query performance and allowing advance compression techniques only supports Single-AZ deployments and the nodes are available within the same AZ, if the AZ supports Redshift clusters spot instances are vs an option","title":"5 Redshift"},{"location":"chap2/7AS_cs/","text":"L7 AWS Analytics Services Cheat Sheet 1 Kinesis Data Streams \u2013 KDS enables real-time processing of streaming data at massive scale provides ordering of records per shard provides an ability to read and/or replay records in the same order allows multiple applications to consume the same data data is replicated across three data centers within a region data is preserved for 24 hours, by default, and can be extended to 7 days data inserted in Kinesis, it can\u2019t be deleted (immutability) but only expires streams can be scaled using multiple shards, based on the partition key each shard provides the capacity of 1MB/sec data input and 2MB/sec data output with 1000 PUT requests per second 1-1 Kinesis vs SQS real-time processing of streaming big data vs reliable, highly scalable hosted queue for storing messages ordered records, as well as the ability to read and/or replay records in the same order vs no guarantee on data ordering (with the standard queues before the FIFO queue feature was released) data storage up to 24 hours, extended to 7 days vs 1 minute to extended to 14 days but cleared if deleted by the consumer supports multiple consumers vs single consumer at a time and requires multiple queues to deliver message to multiple consumers 1-2 Kinesis Producer API PutRecord and PutRecords are synchronous PutRecords uses batching and increases throughput might experience ProvisionedThroughputExceeded Exceptions , when sending more data. Use retries with backoff, resharding or change partition key. KPL producer supports synchronous or asynchronous use cases supports inbuilt batching and retry mechanism Kinesis Agent can help monitor log files and send them to KDS supports third party libraries like Spark, Flume, Kafka connect etc. 1-3 Kinesis Consumers Kinesis SDK Records are polled by consumers from a shard Kinesis Client Library (KCL) Read records from Kinesis produced with the KPL (de-aggregation) supports checkpointing feature to keep track of the application\u2019s state and resume progress using DynamoDB table if KDS application receives provisioned-throughput exceptions, increase the provisioned throughput for the DynamoDB table Kinesis Connector Library \u2013 can be replaced using Firehose or Lambda Third party libraries: Spark, Log4J Appenders, Flume, Kafka Connect \u2026 Kinesis Firehose, AWS Lambda Kinesis Consumer Enhanced Fan-Out supports Multiple Consumer applications for the same Stream provides Low Latency ~70ms Higher costs Default limit of 5 consumers using enhanced fan-out per data stream 1-4 Kinesis Security allows access / authorization control using IAM policies supports Encryption in flight using HTTPS endpoints supports Data encryption at rest either using client side encryption before pushing the data to data streams or server side encryption upports VPC Endpoints to access within VPC 2\u3001Kinesis Data Firehose \u2013 KDF data transfer solution for delivering real time streaming data to destinations such as S3, Redshift, Elasticsearch service, and Splunk . is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration is Near Real Time (min. 60 secs) as it buffers incoming streaming data to a certain size or for a certain period of time before delivering it supports batching, compression, and encryption of the data before loading it, minimizing the amount of storage used at the destination and increasing security supports data compression, minimizing the amount of storage used at the destination . It currently supports GZIP, ZIP, and SNAPPY compression formats. Only GZIP is supported if the data is further loaded to Redshift. supports out of box data transformation as well as custom transformation using Lambda function to transform incoming source data and deliver the transformed data to destinations uses at least once semantics for data delivery. supports multiple producers as datasource , which include Kinesis data stream, KPL, Kinesis Agent, or the Kinesis Data Firehose API using the AWS SDK, CloudWatch Logs, CloudWatch Events, or AWS IoT does NOT support consumers like Spark and KCL supports interface VPC endpoint to keep traffic between the VPC and Kinesis Data Firehose from leaving the Amazon network. 3 Kinesis Data Streams vs Kinesis Data Firehose 4\u3001Kinesis Data Analytics helps analyze streaming data , gain actionable insights, and respond to the business and customer needs in real time. reduces the complexity of building, managing, and integrating streaming applications with other AWS service 5\u3001 Redshift Redshift is a fast, fully managed data warehouse provides simple and cost-effective solution to analyze all the data using standard SQL and the existing Business Intelligence (BI) tools. manages the work needed to set up, operate, and scale a data warehouse, from provisioning the infrastructure capacity to automating ongoing administrative tasks such as backups, and patching . automatically monitors your nodes and drives to help you recover from failures. only supports Single-AZ deployments . replicates all the data within the data warehouse cluster when it is loaded and also continuously backs up your data to S3. attempts to maintain at least three copies of your data (the original and replica on the compute nodes and a backup in S3). supports cross-region snapshot replication to another region for disaster recovery Redshift supports four distribution styles; AUTO, EVEN, KEY, or ALL . KEY distribution uses a single column as distribution key (DISTKEY) and helps place matching values on the same node slice Even distribution distributes the rows across the slices in a round-robin fashion , regardless of the values in any particular column ALL distribution replicates whole table in every compute node. AUTO distribution lets Redshift assigns an optimal distribution style based on the size of the table data 5-1 Redshift supports Compound and Interleaved sort keys Compound key is made up of all of the columns listed in the sort key definition , in the order they are listed and is more efficient when query predicates use a prefix , or query\u2019s filter applies conditions, such as filters and joins, which is a subset of the sort key columns in order. Interleaved sort key gives equal weight to each column in the sort key, so query predicates can use any subset of the columns that make up the sort key, in any order. Not ideal for monotonically increasing attributes Column encodings CANNOT be changed once created. supports query queues for Workload Management, in order to manage concurrency and resource planning. It is a best practice to have separate queues for long running resource-intensive queries and fast queries that don\u2019t require big amounts of memory and CPU Supports Enhanced VPC routing 5-2 Import/Export Data UNLOAD helps copy data from Redshift table to S3 COPY command helps copy data from S3 to Redshift also supports EMR, DynamoDB, remote hosts using SSH parallelized and efficient can decrypt data as it is loaded from S3 DON\u2019T use multiple concurrent COPY commands to load one table from multiple files as Redshift is forced to perform a serialized load, which is much slower. supports data decryption when loading data , if data encrypted supports decompressing data , if data is compressed. Split the Load Data into Multiple Files Load the data in sort key order to avoid needing to vacuum. Use a Manifest File provides Data consistency, to avoid S3 eventual consistency issues helps specify different S3 locations in a more efficient way that with the use of S3 prefixes. 5-3 Redshift Spectrum helps query and retrieve structured and semistructured data from files in S3 without having to load the data into Redshift tables Redshift Spectrum external tables are read-only . You can\u2019t COPY or INSERT to an external table. 6 EMR is a web service that utilizes a hosted Hadoop framework running on the web-scale infrastructure of EC2 and S3 launches all nodes for a given cluster in the same Availability Zone , which improves performance as it provides higher data access rate seamlessly supports Reserved, On-Demand and Spot Instances consists of Master Node for management and Slave nodes, which consists of Core nodes holding data and Task nodes for performing tasks only is fault tolerant for slave node failures and continues job execution if a slave node goes down does not automatically provision another node to take over failed slaves supports Persistent and Transient cluster types Persistent which continue to run Transient which terminates once the job steps are completed supports EMRFS which allows S3 to be used as a durable HA data storage 7 Glue fully-managed ETL service that automates the time-consuming steps of data preparation for analytics is serverless and s upports pay-as-you-go model . recommends and generates ETL code to transform the source data into target schemas, and runs the ETL jobs on a fully managed, scale-out Apache Spark environment to load your data into its destination. helps setup, orchestrate, and monitor complex data flows . natively supports RDS, Redshift, S3 and databases on EC2 instances. supports server side encryption for data at rest and SSL for data in motion. provides development endpoints to edit, debug, and test the code it generates. 7-1 AWS Glue Data Catalog is a central repository to store structural and operational metadata for all the data assets . automatically discovers and profiles the data automatically discover both structured and semi-structured data stored in the data lake on S3, Redshift, and other databases provides a unified view of the data that is available for ETL, querying and reporting using services like Athena, EMR, and Redshift Spectrum.\u3001 Each AWS account has one AWS Glue Data Catalog per region . 7-2 AWS Glue crawler connects to a data store, progresses through a prioritized list of classifiers to extract the schema of the data and other statistics , and then populates the Glue Data Catalog with this metadata can be scheduled to run periodically so that the metadata is always up-to-date and in-sync with the underlying data. 8 QuickSight is a very fast, easy-to-use, cloud-powered business analytics service that makes it easy to build visualizations , perform ad-hoc analysis, and quickly get business insights from their data, anytime, on any device. delivers fast and responsive query performance by using a robust in-memory engine (SPICE) . \u201cSPICE\u201d stands for a Super-fast, Parallel, In-memory Calculation Engine can also be configured to keep the data in SPICE up-to-date as the data in the underlying sources change. automatically replicates data for high availability and enables QuickSight to scale to support users to perform simultaneous fast interactive analysis across a wide variety of AWS data sources. supports Excel files and flat files like CSV, TSV, CLF, ELF on-premises databases like PostgreSQL, SQL Server and MySQL SaaS applications like Salesforce and AWS data sources such as Redshift, RDS, Aurora, Athena, and S3 supports various functions to format and transform the data. supports assorted visualizations that facilitate different analytical approaches: Comparison and distribution \u2013 Bar charts (several assorted variants) Changes over time \u2013 Line graphs, Area line charts Correlation \u2013 Scatter plots, Heat maps Aggregation \u2013 Pie graphs, Tree maps Tabular \u2013 Pivot tables 9\u3001Data Pipeline orchestration service that helps define data-driven workflows to automate and schedule regular data movement and data processing activities integrates with on-premises and cloud-based storage systems allows scheduling, retry, and failure logic for the workflows 10\u3001 Elasticsearch Elasticsearch Service is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud. 10-1 Elasticsearch provides real-time, distributed search and analytics engine ability to provision all the resources for Elasticsearch cluster and launches the cluster easy to use cluster scaling options. Scaling Elasticsearch Service domain by adding or modifying instances , and storage volumes is an online operation that does not require any downtime. provides self-healing clusters, which automatically detects and replaces failed Elasticsearch nodes, reducing the overhead associated with self-managed infrastructures domain snapshots to back up and restore ES domains and replicate domains across AZs enhanced security with IAM, Network, Domain access policies, and fine-grained access control storage volumes for the data using EBS volumes ability to span cluster nodes across multiple AZs in the same region, known as zone awareness , for high availability and redundancy . Elasticsearch Service automatically distributes the primary and replica shards across instances in different AZs. dedicated master nodes to improve cluster stability data visualization using the Kibana tool integration with CloudWatch for monitoring ES domain metrics integration with CloudTrail for auditing configuration API calls to ES domains integration with S3, Kinesis, and DynamoDB for loading streaming data ability to handle structured and Unstructured data supports encryption at rest through KMS, node-to-node encryption over TLS, and the ability to require clients to communicate of HTTPS","title":"L7 AWS Analytics Services Cheat Sheet"},{"location":"chap2/7AS_cs/#l7-aws-analytics-services-cheat-sheet","text":"","title":"L7 AWS Analytics Services Cheat Sheet"},{"location":"chap2/7AS_cs/#1-kinesis-data-streams-kds","text":"enables real-time processing of streaming data at massive scale provides ordering of records per shard provides an ability to read and/or replay records in the same order allows multiple applications to consume the same data data is replicated across three data centers within a region data is preserved for 24 hours, by default, and can be extended to 7 days data inserted in Kinesis, it can\u2019t be deleted (immutability) but only expires streams can be scaled using multiple shards, based on the partition key each shard provides the capacity of 1MB/sec data input and 2MB/sec data output with 1000 PUT requests per second","title":"1 Kinesis Data Streams \u2013 KDS"},{"location":"chap2/7AS_cs/#1-1-kinesis-vs-sqs","text":"real-time processing of streaming big data vs reliable, highly scalable hosted queue for storing messages ordered records, as well as the ability to read and/or replay records in the same order vs no guarantee on data ordering (with the standard queues before the FIFO queue feature was released) data storage up to 24 hours, extended to 7 days vs 1 minute to extended to 14 days but cleared if deleted by the consumer supports multiple consumers vs single consumer at a time and requires multiple queues to deliver message to multiple consumers","title":"1-1 Kinesis vs SQS"},{"location":"chap2/7AS_cs/#1-2-kinesis-producer","text":"API PutRecord and PutRecords are synchronous PutRecords uses batching and increases throughput might experience ProvisionedThroughputExceeded Exceptions , when sending more data. Use retries with backoff, resharding or change partition key. KPL producer supports synchronous or asynchronous use cases supports inbuilt batching and retry mechanism Kinesis Agent can help monitor log files and send them to KDS supports third party libraries like Spark, Flume, Kafka connect etc.","title":"1-2 Kinesis Producer"},{"location":"chap2/7AS_cs/#1-3-kinesis-consumers","text":"Kinesis SDK Records are polled by consumers from a shard Kinesis Client Library (KCL) Read records from Kinesis produced with the KPL (de-aggregation) supports checkpointing feature to keep track of the application\u2019s state and resume progress using DynamoDB table if KDS application receives provisioned-throughput exceptions, increase the provisioned throughput for the DynamoDB table Kinesis Connector Library \u2013 can be replaced using Firehose or Lambda Third party libraries: Spark, Log4J Appenders, Flume, Kafka Connect \u2026 Kinesis Firehose, AWS Lambda Kinesis Consumer Enhanced Fan-Out supports Multiple Consumer applications for the same Stream provides Low Latency ~70ms Higher costs Default limit of 5 consumers using enhanced fan-out per data stream","title":"1-3 Kinesis Consumers"},{"location":"chap2/7AS_cs/#1-4-kinesis-security","text":"allows access / authorization control using IAM policies supports Encryption in flight using HTTPS endpoints supports Data encryption at rest either using client side encryption before pushing the data to data streams or server side encryption upports VPC Endpoints to access within VPC","title":"1-4 Kinesis Security"},{"location":"chap2/7AS_cs/#2kinesis-data-firehose-kdf","text":"data transfer solution for delivering real time streaming data to destinations such as S3, Redshift, Elasticsearch service, and Splunk . is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration is Near Real Time (min. 60 secs) as it buffers incoming streaming data to a certain size or for a certain period of time before delivering it supports batching, compression, and encryption of the data before loading it, minimizing the amount of storage used at the destination and increasing security supports data compression, minimizing the amount of storage used at the destination . It currently supports GZIP, ZIP, and SNAPPY compression formats. Only GZIP is supported if the data is further loaded to Redshift. supports out of box data transformation as well as custom transformation using Lambda function to transform incoming source data and deliver the transformed data to destinations uses at least once semantics for data delivery. supports multiple producers as datasource , which include Kinesis data stream, KPL, Kinesis Agent, or the Kinesis Data Firehose API using the AWS SDK, CloudWatch Logs, CloudWatch Events, or AWS IoT does NOT support consumers like Spark and KCL supports interface VPC endpoint to keep traffic between the VPC and Kinesis Data Firehose from leaving the Amazon network.","title":"2\u3001Kinesis Data  Firehose \u2013 KDF"},{"location":"chap2/7AS_cs/#3-kinesis-data-streams-vs-kinesis-data-firehose","text":"","title":"3 Kinesis Data Streams vs Kinesis Data Firehose"},{"location":"chap2/7AS_cs/#4kinesis-data-analytics","text":"helps analyze streaming data , gain actionable insights, and respond to the business and customer needs in real time. reduces the complexity of building, managing, and integrating streaming applications with other AWS service","title":"4\u3001Kinesis Data Analytics"},{"location":"chap2/7AS_cs/#5-redshift","text":"Redshift is a fast, fully managed data warehouse provides simple and cost-effective solution to analyze all the data using standard SQL and the existing Business Intelligence (BI) tools. manages the work needed to set up, operate, and scale a data warehouse, from provisioning the infrastructure capacity to automating ongoing administrative tasks such as backups, and patching . automatically monitors your nodes and drives to help you recover from failures. only supports Single-AZ deployments . replicates all the data within the data warehouse cluster when it is loaded and also continuously backs up your data to S3. attempts to maintain at least three copies of your data (the original and replica on the compute nodes and a backup in S3). supports cross-region snapshot replication to another region for disaster recovery Redshift supports four distribution styles; AUTO, EVEN, KEY, or ALL . KEY distribution uses a single column as distribution key (DISTKEY) and helps place matching values on the same node slice Even distribution distributes the rows across the slices in a round-robin fashion , regardless of the values in any particular column ALL distribution replicates whole table in every compute node. AUTO distribution lets Redshift assigns an optimal distribution style based on the size of the table data","title":"5\u3001 Redshift"},{"location":"chap2/7AS_cs/#5-1-redshift-supports-compound-and-interleaved-sort-keys","text":"Compound key is made up of all of the columns listed in the sort key definition , in the order they are listed and is more efficient when query predicates use a prefix , or query\u2019s filter applies conditions, such as filters and joins, which is a subset of the sort key columns in order. Interleaved sort key gives equal weight to each column in the sort key, so query predicates can use any subset of the columns that make up the sort key, in any order. Not ideal for monotonically increasing attributes Column encodings CANNOT be changed once created. supports query queues for Workload Management, in order to manage concurrency and resource planning. It is a best practice to have separate queues for long running resource-intensive queries and fast queries that don\u2019t require big amounts of memory and CPU Supports Enhanced VPC routing","title":"5-1 Redshift supports Compound and Interleaved sort keys"},{"location":"chap2/7AS_cs/#5-2-importexport-data","text":"UNLOAD helps copy data from Redshift table to S3 COPY command helps copy data from S3 to Redshift also supports EMR, DynamoDB, remote hosts using SSH parallelized and efficient can decrypt data as it is loaded from S3 DON\u2019T use multiple concurrent COPY commands to load one table from multiple files as Redshift is forced to perform a serialized load, which is much slower. supports data decryption when loading data , if data encrypted supports decompressing data , if data is compressed. Split the Load Data into Multiple Files Load the data in sort key order to avoid needing to vacuum. Use a Manifest File provides Data consistency, to avoid S3 eventual consistency issues helps specify different S3 locations in a more efficient way that with the use of S3 prefixes.","title":"5-2 Import/Export Data"},{"location":"chap2/7AS_cs/#5-3-redshift-spectrum","text":"helps query and retrieve structured and semistructured data from files in S3 without having to load the data into Redshift tables Redshift Spectrum external tables are read-only . You can\u2019t COPY or INSERT to an external table.","title":"5-3 Redshift Spectrum"},{"location":"chap2/7AS_cs/#6-emr","text":"is a web service that utilizes a hosted Hadoop framework running on the web-scale infrastructure of EC2 and S3 launches all nodes for a given cluster in the same Availability Zone , which improves performance as it provides higher data access rate seamlessly supports Reserved, On-Demand and Spot Instances consists of Master Node for management and Slave nodes, which consists of Core nodes holding data and Task nodes for performing tasks only is fault tolerant for slave node failures and continues job execution if a slave node goes down does not automatically provision another node to take over failed slaves supports Persistent and Transient cluster types Persistent which continue to run Transient which terminates once the job steps are completed supports EMRFS which allows S3 to be used as a durable HA data storage","title":"6 EMR"},{"location":"chap2/7AS_cs/#7-glue","text":"fully-managed ETL service that automates the time-consuming steps of data preparation for analytics is serverless and s upports pay-as-you-go model . recommends and generates ETL code to transform the source data into target schemas, and runs the ETL jobs on a fully managed, scale-out Apache Spark environment to load your data into its destination. helps setup, orchestrate, and monitor complex data flows . natively supports RDS, Redshift, S3 and databases on EC2 instances. supports server side encryption for data at rest and SSL for data in motion. provides development endpoints to edit, debug, and test the code it generates.","title":"7 Glue"},{"location":"chap2/7AS_cs/#7-1-aws-glue-data-catalog","text":"is a central repository to store structural and operational metadata for all the data assets . automatically discovers and profiles the data automatically discover both structured and semi-structured data stored in the data lake on S3, Redshift, and other databases provides a unified view of the data that is available for ETL, querying and reporting using services like Athena, EMR, and Redshift Spectrum.\u3001 Each AWS account has one AWS Glue Data Catalog per region .","title":"7-1 AWS Glue Data Catalog"},{"location":"chap2/7AS_cs/#7-2-aws-glue-crawler","text":"connects to a data store, progresses through a prioritized list of classifiers to extract the schema of the data and other statistics , and then populates the Glue Data Catalog with this metadata can be scheduled to run periodically so that the metadata is always up-to-date and in-sync with the underlying data.","title":"7-2 AWS Glue crawler"},{"location":"chap2/7AS_cs/#8-quicksight","text":"is a very fast, easy-to-use, cloud-powered business analytics service that makes it easy to build visualizations , perform ad-hoc analysis, and quickly get business insights from their data, anytime, on any device. delivers fast and responsive query performance by using a robust in-memory engine (SPICE) . \u201cSPICE\u201d stands for a Super-fast, Parallel, In-memory Calculation Engine can also be configured to keep the data in SPICE up-to-date as the data in the underlying sources change. automatically replicates data for high availability and enables QuickSight to scale to support users to perform simultaneous fast interactive analysis across a wide variety of AWS data sources. supports Excel files and flat files like CSV, TSV, CLF, ELF on-premises databases like PostgreSQL, SQL Server and MySQL SaaS applications like Salesforce and AWS data sources such as Redshift, RDS, Aurora, Athena, and S3 supports various functions to format and transform the data. supports assorted visualizations that facilitate different analytical approaches: Comparison and distribution \u2013 Bar charts (several assorted variants) Changes over time \u2013 Line graphs, Area line charts Correlation \u2013 Scatter plots, Heat maps Aggregation \u2013 Pie graphs, Tree maps Tabular \u2013 Pivot tables","title":"8 QuickSight"},{"location":"chap2/7AS_cs/#9data-pipeline","text":"orchestration service that helps define data-driven workflows to automate and schedule regular data movement and data processing activities integrates with on-premises and cloud-based storage systems allows scheduling, retry, and failure logic for the workflows","title":"9\u3001Data Pipeline"},{"location":"chap2/7AS_cs/#10-elasticsearch","text":"Elasticsearch Service is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud.","title":"10\u3001 Elasticsearch"},{"location":"chap2/7AS_cs/#10-1-elasticsearch-provides","text":"real-time, distributed search and analytics engine ability to provision all the resources for Elasticsearch cluster and launches the cluster easy to use cluster scaling options. Scaling Elasticsearch Service domain by adding or modifying instances , and storage volumes is an online operation that does not require any downtime. provides self-healing clusters, which automatically detects and replaces failed Elasticsearch nodes, reducing the overhead associated with self-managed infrastructures domain snapshots to back up and restore ES domains and replicate domains across AZs enhanced security with IAM, Network, Domain access policies, and fine-grained access control storage volumes for the data using EBS volumes ability to span cluster nodes across multiple AZs in the same region, known as zone awareness , for high availability and redundancy . Elasticsearch Service automatically distributes the primary and replica shards across instances in different AZs. dedicated master nodes to improve cluster stability data visualization using the Kibana tool integration with CloudWatch for monitoring ES domain metrics integration with CloudTrail for auditing configuration API calls to ES domains integration with S3, Kinesis, and DynamoDB for loading streaming data ability to handle structured and Unstructured data supports encryption at rest through KMS, node-to-node encryption over TLS, and the ability to require clients to communicate of HTTPS","title":"10-1 Elasticsearch provides"},{"location":"chap2/8AS_cs/","text":"L8 AWS Application Services Cheat Sheet 1\u3001SQS extremely scalable queue service and potentially handles millions of messages helps build fault tolerant, distributed loosely coupled applications stores copies of the messages on multiple servers for redundancy and high availability guarantees At-Least-Once Delivery , but does not guarantee Exact One Time Delivery which might result in duplicate messages ( Not true anymore with the introduction of FIFO queues) does not maintain or guarantee message order , and if needed sequencing information needs to be added to the message itself ( Not true anymore with the introduction of FIFO queues ) supports multiple readers and writers interacting with the same queue as the same time holds message for 4 days , by default, and c an be changed from 1 min \u2013 14 days after which the message is deleted message needs to be explicitly deleted by the consumer once processed allows send, receive and delete batching which helps club up to 10 messages in a single batch while charging price for a single message handles visibility of the message to multiple consumers using Visibility Timeout, where the message once read by a consumer is not visible to the other consumers till the timeout occurs can handle load and performance requirements by scaling the worker instances as the demand changes ( Job Observer pattern ) message sample allowing short and long polling returns immediately vs waits for fixed time for e.g. 20 secs might not return all messages as it samples a subset of servers vs returns all available messages repetitive vs helps save cost with long connection supports delay queues to make messages available after a certain delay, can you used to differentiate from priority queues supports dead letter queues , to redirect messages which failed to process after certain attempts instead of being processed repeatedly 1-1 Design Patterns Job Observer Pattern can help coordinate number of EC2 instances with number of job requests (Queue Size) automatically thus Improving cost effectiveness and performance Priority Queue Pattern can be used to setup different queues with different handling either by delayed queues or low scaling capacity for handling messages in lower priority queues 2\u3001SNS delivery or sending of messages to subscribing endpoints or clients publisher-subscriber model Producers and Consumers communicate asynchronously with subscribers by producing and sending a message to a topic supports Email (plain or JSON), HTTP/HTTPS, SMS, SQS supports Mobile Push Notifications to push notifications directly to mobile devices with services like Amazon Device Messaging (ADM), Apple Push Notification Service (APNS), Google Cloud Messaging (GCM) etc . supported order is not guaranteed and No recall available integrated with Lambda to invoke functions on notifications for Email notifications, use SNS or SES directly, SQS does not work 3\u3001SWF orchestration service to coordinate work across distributed components helps define tasks, stores, assigns tasks to workers , define logic, tracks and monitors the task and maintains workflow state in a durable fashion helps define tasks which can be executed on AWS cloud or on-premises helps coordinating tasks across the application which involves managing intertask dependencies, scheduling, and concurrency in accordance with the logical flow of the application supports built-in retries, timeouts and logging supports manual tasks Characteristics deliver exactly once uses long polling, which reduces number of polls without results Visibility of task state via API Timers, signals, markers, child workflows supports versioning keeps workflow history for a user-specified time AWS SWF vs AWS SQS task-oriented vs message-oriented track of all tasks and events vs needs custom handling 4\u3001SES highly scalable and cost-effective email service uses content filtering technologies to scan outgoing emails to check standards and email content for spam and malware supports full fledged emails to be sent as compared to SNS where only the message is sent in Email ideal for sending bulk emails at scale guarantees first hop eliminates the need to support custom software or applications to do heavy lifting of email transport","title":"L8 AWS Application Services Cheat Sheet"},{"location":"chap2/8AS_cs/#l8-aws-application-services-cheat-sheet","text":"","title":"L8 AWS Application Services Cheat Sheet"},{"location":"chap2/8AS_cs/#1sqs","text":"extremely scalable queue service and potentially handles millions of messages helps build fault tolerant, distributed loosely coupled applications stores copies of the messages on multiple servers for redundancy and high availability guarantees At-Least-Once Delivery , but does not guarantee Exact One Time Delivery which might result in duplicate messages ( Not true anymore with the introduction of FIFO queues) does not maintain or guarantee message order , and if needed sequencing information needs to be added to the message itself ( Not true anymore with the introduction of FIFO queues ) supports multiple readers and writers interacting with the same queue as the same time holds message for 4 days , by default, and c an be changed from 1 min \u2013 14 days after which the message is deleted message needs to be explicitly deleted by the consumer once processed allows send, receive and delete batching which helps club up to 10 messages in a single batch while charging price for a single message handles visibility of the message to multiple consumers using Visibility Timeout, where the message once read by a consumer is not visible to the other consumers till the timeout occurs can handle load and performance requirements by scaling the worker instances as the demand changes ( Job Observer pattern ) message sample allowing short and long polling returns immediately vs waits for fixed time for e.g. 20 secs might not return all messages as it samples a subset of servers vs returns all available messages repetitive vs helps save cost with long connection supports delay queues to make messages available after a certain delay, can you used to differentiate from priority queues supports dead letter queues , to redirect messages which failed to process after certain attempts instead of being processed repeatedly","title":"1\u3001SQS"},{"location":"chap2/8AS_cs/#1-1-design-patterns","text":"Job Observer Pattern can help coordinate number of EC2 instances with number of job requests (Queue Size) automatically thus Improving cost effectiveness and performance Priority Queue Pattern can be used to setup different queues with different handling either by delayed queues or low scaling capacity for handling messages in lower priority queues","title":"1-1 Design Patterns"},{"location":"chap2/8AS_cs/#2sns","text":"delivery or sending of messages to subscribing endpoints or clients publisher-subscriber model Producers and Consumers communicate asynchronously with subscribers by producing and sending a message to a topic supports Email (plain or JSON), HTTP/HTTPS, SMS, SQS supports Mobile Push Notifications to push notifications directly to mobile devices with services like Amazon Device Messaging (ADM), Apple Push Notification Service (APNS), Google Cloud Messaging (GCM) etc . supported order is not guaranteed and No recall available integrated with Lambda to invoke functions on notifications for Email notifications, use SNS or SES directly, SQS does not work","title":"2\u3001SNS"},{"location":"chap2/8AS_cs/#3swf","text":"orchestration service to coordinate work across distributed components helps define tasks, stores, assigns tasks to workers , define logic, tracks and monitors the task and maintains workflow state in a durable fashion helps define tasks which can be executed on AWS cloud or on-premises helps coordinating tasks across the application which involves managing intertask dependencies, scheduling, and concurrency in accordance with the logical flow of the application supports built-in retries, timeouts and logging supports manual tasks Characteristics deliver exactly once uses long polling, which reduces number of polls without results Visibility of task state via API Timers, signals, markers, child workflows supports versioning keeps workflow history for a user-specified time AWS SWF vs AWS SQS task-oriented vs message-oriented track of all tasks and events vs needs custom handling","title":"3\u3001SWF"},{"location":"chap2/8AS_cs/#4ses","text":"highly scalable and cost-effective email service uses content filtering technologies to scan outgoing emails to check standards and email content for spam and malware supports full fledged emails to be sent as compared to SNS where only the message is sent in Email ideal for sending bulk emails at scale guarantees first hop eliminates the need to support custom software or applications to do heavy lifting of email transport","title":"4\u3001SES"},{"location":"chap2/9MT_cs/","text":"L9 AWS Management Tools Cheat Sheet 1\u3001CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources Resources can be updated, deleted, and modified in an orderly, controlled and predictable fashion , in effect applying version control to the AWS infrastructure as code done for software code CloudFormation Template is an architectural diagram , in JSON format, and Stack is the end result of that diagram , which is actually provisioned template can be used to set up the resources consistently and repeatedly over and over across multiple regions and consists of List of AWS resources and their configuration values An optional template file format version number An optional list of template parameters (input values supplied at stack creation time) An optional list of output values like public IP address using the Fn::GetAtt function An optional list of data tables used to lookup static configuration values for e.g., AMI names per AZ supports Chef & Puppet Integration to deploy and configure right down the application layer supports Bootstrap scripts to install packages, files, and services on the EC2 instances by simply describing them in the CF template automatic rollback on error feature is enabled, by default, which will cause all the AWS resources that CF created successfully for a stack up to the point where an error occurred to be deleted provides a WaitCondition resource to block the creation of other resources until a completion signal is received from an external source allows DeletionPolicy attribute to be defined for resources in the template retain to preserve resources like S3 even after stack deletion snapshot to backup resources like RDS after stack deletion DependsOn attribute to specify that the creation of a specific resource follows another Service role is an IAM role that allows AWS CloudFormation to make calls to resources in a stack on the user\u2019s behalf Nested stacks can separate out reusable, common components and create dedicated templates to mix and match different templates but use nested stacks to create a single, unified stack Change Sets presents a summary or preview of the proposed changes that CloudFormation will make when a stack is updated Drift detection enables you to detect whether a stack\u2019s actual configuration differs, or has drifted, from its expected configuration. Termination protection helps prevent a stack from being accidentally deleted. Stack policy can prevent stack resources from being unintentionally updated or deleted during a stack update. StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and Regions with a single operation. 2\u3001Elastic BeanStalk makes it easier for developers to quickly deploy and manage applications in the AWS cloud . automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling and application health monitoring CloudFormation supports ElasticBeanstalk provisions resources to support a web application that handles HTTP(S) requests or a web application that handles background-processing (worker) tasks supports Out Of the Box Apache Tomcat for Java applications Apache HTTP Server for PHP applications Apache HTTP server for Python applications Nginx or Apache HTTP Server for Node.js applications Passenger for Ruby applications MicroSoft IIS 7.5 for .Net applications Single and Multi Container Docker supports custom AMI to be used is designed to support multiple running environments such as one for Dev, QA, Pre-Prod and Production. supports versioning and stores and tracks application versions over time allowing easy rollback to prior version can provision RDS DB instance and connectivity information is exposed to the application by environment variables, but is NOT recommended for production setup as the RDS is tied up with the Elastic Beanstalk lifecycle and if deleted, the RDS instance would be deleted as well 3\u3001OpsWorks is a configuration management service that helps to configure and operate applications in a cloud enterprise by using Chef helps deploy and monitor applications in stacks with multiple layers supports preconfigured layers for Applications, Databases, Load Balancers, Caching OpsWorks Stacks features is a set of lifecycle events \u2013 Setup, Configure, Deploy, Undeploy, and Shutdown \u2013 which automatically runs specified set of recipes at the appropriate time on each instance Layers depend on Chef recipes to handle tasks such as installing packages on instances, deploying apps, running scripts, and so on OpsWorks Stacks runs the recipes for each layer, even if the instance belongs to multiple layers supports Auto Healing and Auto Scaling to monitor instance health, and provision new instances 4\u3001CloudWatch allows monitoring of AWS resources and applications in real time, collect and track pre configured or custom metrics and configure alarms to send notification or make resource changes based on defined rules does not aggregate data across regions stores the log data indefinitely , and the retention can be changed for each log group at any time alarm history is stored for only 14 days can be used an alternative to S3 to store log s with the ability to configure Alarms and generate metrics, however logs cannot be made public Alarms exist only in the created region and the Alarm actions must reside in the same region as well 5\u3001CloudTrail records access to API calls for the AWS account made from AWS management console, SDKs, CLI and higher level AWS service support many AWS services and tracks who did, from where, what & when A region can include global services (like IAM, STS etc), is applicable to all the supported services within that region log files from different regions can be sent to the same S3 bucket can be integrated with SNS to notify logs availability, CloudWatch logs log group for notifications when specific API events occur call history enables security analysis, resource change tracking, trouble shooting and compliance auditing","title":"L9 AWS Management Tools Cheat Sheet"},{"location":"chap2/9MT_cs/#l9-aws-management-tools-cheat-sheet","text":"","title":"L9 AWS Management Tools Cheat Sheet"},{"location":"chap2/9MT_cs/#1cloudformation","text":"gives developers and systems administrators an easy way to create and manage a collection of related AWS resources Resources can be updated, deleted, and modified in an orderly, controlled and predictable fashion , in effect applying version control to the AWS infrastructure as code done for software code CloudFormation Template is an architectural diagram , in JSON format, and Stack is the end result of that diagram , which is actually provisioned template can be used to set up the resources consistently and repeatedly over and over across multiple regions and consists of List of AWS resources and their configuration values An optional template file format version number An optional list of template parameters (input values supplied at stack creation time) An optional list of output values like public IP address using the Fn::GetAtt function An optional list of data tables used to lookup static configuration values for e.g., AMI names per AZ supports Chef & Puppet Integration to deploy and configure right down the application layer supports Bootstrap scripts to install packages, files, and services on the EC2 instances by simply describing them in the CF template automatic rollback on error feature is enabled, by default, which will cause all the AWS resources that CF created successfully for a stack up to the point where an error occurred to be deleted provides a WaitCondition resource to block the creation of other resources until a completion signal is received from an external source allows DeletionPolicy attribute to be defined for resources in the template retain to preserve resources like S3 even after stack deletion snapshot to backup resources like RDS after stack deletion DependsOn attribute to specify that the creation of a specific resource follows another Service role is an IAM role that allows AWS CloudFormation to make calls to resources in a stack on the user\u2019s behalf Nested stacks can separate out reusable, common components and create dedicated templates to mix and match different templates but use nested stacks to create a single, unified stack Change Sets presents a summary or preview of the proposed changes that CloudFormation will make when a stack is updated Drift detection enables you to detect whether a stack\u2019s actual configuration differs, or has drifted, from its expected configuration. Termination protection helps prevent a stack from being accidentally deleted. Stack policy can prevent stack resources from being unintentionally updated or deleted during a stack update. StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and Regions with a single operation.","title":"1\u3001CloudFormation"},{"location":"chap2/9MT_cs/#2elastic-beanstalk","text":"makes it easier for developers to quickly deploy and manage applications in the AWS cloud . automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling and application health monitoring CloudFormation supports ElasticBeanstalk provisions resources to support a web application that handles HTTP(S) requests or a web application that handles background-processing (worker) tasks supports Out Of the Box Apache Tomcat for Java applications Apache HTTP Server for PHP applications Apache HTTP server for Python applications Nginx or Apache HTTP Server for Node.js applications Passenger for Ruby applications MicroSoft IIS 7.5 for .Net applications Single and Multi Container Docker supports custom AMI to be used is designed to support multiple running environments such as one for Dev, QA, Pre-Prod and Production. supports versioning and stores and tracks application versions over time allowing easy rollback to prior version can provision RDS DB instance and connectivity information is exposed to the application by environment variables, but is NOT recommended for production setup as the RDS is tied up with the Elastic Beanstalk lifecycle and if deleted, the RDS instance would be deleted as well","title":"2\u3001Elastic BeanStalk"},{"location":"chap2/9MT_cs/#3opsworks","text":"is a configuration management service that helps to configure and operate applications in a cloud enterprise by using Chef helps deploy and monitor applications in stacks with multiple layers supports preconfigured layers for Applications, Databases, Load Balancers, Caching OpsWorks Stacks features is a set of lifecycle events \u2013 Setup, Configure, Deploy, Undeploy, and Shutdown \u2013 which automatically runs specified set of recipes at the appropriate time on each instance Layers depend on Chef recipes to handle tasks such as installing packages on instances, deploying apps, running scripts, and so on OpsWorks Stacks runs the recipes for each layer, even if the instance belongs to multiple layers supports Auto Healing and Auto Scaling to monitor instance health, and provision new instances","title":"3\u3001OpsWorks"},{"location":"chap2/9MT_cs/#4cloudwatch","text":"allows monitoring of AWS resources and applications in real time, collect and track pre configured or custom metrics and configure alarms to send notification or make resource changes based on defined rules does not aggregate data across regions stores the log data indefinitely , and the retention can be changed for each log group at any time alarm history is stored for only 14 days can be used an alternative to S3 to store log s with the ability to configure Alarms and generate metrics, however logs cannot be made public Alarms exist only in the created region and the Alarm actions must reside in the same region as well","title":"4\u3001CloudWatch"},{"location":"chap2/9MT_cs/#5cloudtrail","text":"records access to API calls for the AWS account made from AWS management console, SDKs, CLI and higher level AWS service support many AWS services and tracks who did, from where, what & when A region can include global services (like IAM, STS etc), is applicable to all the supported services within that region log files from different regions can be sent to the same S3 bucket can be integrated with SNS to notify logs availability, CloudWatch logs log group for notifications when specific API events occur call history enables security analysis, resource change tracking, trouble shooting and compliance auditing","title":"5\u3001CloudTrail"},{"location":"chap3/10route53_policy/","text":"L10 AWS Route 53 Routing Policy 1 AWS Route 53 Routing Policy AWS Route 53 routing policy determines how AWS would respond to the DNS queries and provides multiple Routing policy options 2 Simple Routing Policy Simple routing policy is a simple round robin policy and can be applied when there is a single resource doing the function for the domain for e.g. web server that serves content for the website AWS Route 53 responds to the DNS queries based on the values in the resource record set for e.g. ip address in an A record 3 Weighted Routing Policy Weighted routing policy enables Route 53 to route traffic to different resources in specified proportions (weights) for e.g., 75% one server and 25% to the other during a pilot release Weights can be assigned any number from 0 to 255 Weighted routing policy can be applied when there are multiple resources that perform the same function for e.g., webservers serving the same site Weighted resource record sets let you associate multiple resources with a single DNS name Common use cases include load balancing A/B testing and piloting new versions of software To create a group of weighted resource record sets, two or more resource record sets can be created that have the same combination of DNS name and type, and each resource record set is assigned a unique identifier and a relative weight . When processing a DNS query, Route 53 searches for a resource record set or a group of resource record sets that have the specified name and type. Route 53 selects one from the group. Probability of any one resource record set being selected depends on its weight as a proportion of the total weight for all resource record sets in the group for e.g., suppose for www.example.com has three resource record sets with weights of 1 (20%), 1 (20%), and 3 (60%)(sum = 5). On average, Route 53 selects each of the first two resource record sets one-fifth of the time, and returns the third resource record set three-fifths of the time. 4 Latency-based Routing (LBR) Policy Latency-based Routing Policy enables Route 53 to respond to the DNS query based on which data center gives the user the lowest network latency Latency-based routing policy can be used when there are multiple resources performing the same function and Route 53 needs to be configured to respond to the DNS queries with the resources that provide the fastest response with lowest latency Latency resource record set can be created for the EC2 resource in each region that hosts the application. When Route 53 receives a query for the corresponding domain, it selects the latency resource record set for the EC2 region that gives the user the lowest latency. Route 53 then responds with the value associated with that resource record set for e.g., you might have web servers for example.com in the EC2 data centers in Ireland and in Tokyo. When a user browses to example.com from Singapore, Route 53 will pick up the data center (Tokyo) which has the lowest latency from the users location Latency between hosts on the Internet can change over time as a result of changes in network connectivity and routing. Latency-based routing is based on latency measurements performed over a period of time, and the measurements reflect these changes for e.g. if the latency from the user in Singapore to Ireland improves, the user can be routed to Ireland Latency based routing cannot guarantee users from the same geographic will be served from the same location for any compliance reason Latency resource record sets can be created using any record type that Route 53 supports except NS or SOA 4 Failover Routing Policy Failover routing policy allows active-passive failover configuration, in which one resource takes all traffic when it\u2019s available and the other resource takes all traffic when the first resource isn\u2019t available . Route 53 health checking agents will monitor each location/endpoint of the application to determine the availability. Failover routing policy is applicable for Public hosted zones only 5 Geolocation Routing Policy Geolocation routing policy enables Route 53 to respond to DNS queries based on the geographic location of the users i.e. location from which the DNS queries originate Common use cases include localization of content and presenting some or all of the website in the users language restrict distribution of content to only the locations in which you have distribution rights. balancing load across endpoints in a predictable, easy-to-manage way, so that each user location is consistently routed to the same endpoint. Geolocation routing policy allows geographic locations to be specified by continent, country, or by state (only in US) Geolocation record sets, if created, for overlapping geographic regions for e.g. continent and then for the country within the same continent , priority goes to the smallest geographic region, which allows some queries for a continent to be routed to one resource and queries for selected countries on that continent to a different resource Geolocation works by mapping IP addresses to locations, which might not mapped to a exact geographic location A default resource record set can be created to handle these queries and also the ones which do not have an explicit record set created Route 53 returns a \u201cno answer\u201dresponse for queries from those locations, if a default resource record set if not created Two geolocation resource record sets that specify the same geographic location cannot be created Route 53 supports the edns-client-subnet extension of EDNS0 (EDNS0 adds several optional extensions to the DNS protocol.) to improve the accuracy of geolocation routing 6 AWS Certification Exam Practice Questions You have deployed a web application targeting a global audience across multiple AWS Regions under the domain name example.com. You decide to use Route 53 Latency-Based Routing to serve web requests to users from the region closest to the user. To provide business continuity in the event of server downtime you configure weighted record sets associated with two web servers in separate Availability Zones per region. During a DR test you notice that when you disable all web servers in one of the regions Route 53 does not automatically direct all users to the other region. What could be happening? (Choose 2 answers) Latency resource record sets cannot be used in combination with weighted resource record sets. You did not setup an http health check for one or more of the weighted resource record sets associated with the disabled web servers The value of the weight associated with the latency alias resource record set in the region with the disabled servers is higher than the weight for the other region. One of the two working web servers in the other region did not pass its HTTP health check You did not set \u201cEvaluate Target Health\u201d to \u201cYes\u201d on the latency alias resource record set associated with example.com in the region where you disabled the servers. The compliance department within your multi-national organization requires that all data for your customers that reside in the European Union (EU) must not leave the EU and also data for customers that reside in the US must not leave the US without explicit authorization. What must you do to comply with this requirement for a web based profile management application running on EC2? Run EC2 instances in multiple AWS Availability Zones in single Region and leverage an Elastic Load Balancer with session stickiness to route traffic to the appropriate zone to create their profile ( should be in 2 different regions \u2013 US and Europe ) Run EC2 instances in multiple Regions and leverage Route 53\u2019s Latency Based Routing capabilities to route traffic to the appropriate region to create their profile ( Latency based routing policy would not guarantee the compliance requirement ) Run EC2 instances in multiple Regions and leverage a third party data provider to determine if a user needs to be redirect to the appropriate region to create their profile Run EC2 instances in multiple AWS Availability Zones in a single Region and leverage a third party data provider to determine if a user needs to be redirect to the appropriate zone to create their profile( should be in 2 different regions \u2013 US and Europe ) A US-based company is expanding their web presence into Europe. The company wants to extend their AWS infrastructure from Northern Virginia (us-east-1) into the Dublin (eu-west-1) region. Which of the following options would enable an equivalent experience for users on both continents? Use a public-facing load balancer per region to load-balance web traffic, and enable HTTP health checks. Use a public-facing load balancer per region to load-balance web traffic, and enable sticky sessions. Use Amazon Route 53, and apply a geolocation routing policy to distribute traffic across both regions Use Amazon Route 53, and apply a weighted routing policy to distribute traffic across both regions. You have been asked to propose a multi-region deployment of a web-facing application where a controlled portion of your traffic is being processed by an alternate region. Which configuration would achieve that goal? Route 53 record sets with weighted routing policy Route 53 record sets with latency based routing policy Auto Scaling with scheduled scaling actions set Elastic Load Balancing with health checks enabled Your company is moving towards tracking web page users with a small tracking image loaded on each page. Currently you are serving this image out of us-east, but are starting to get concerned about the time it takes to load the image for users on the west coast. What are the two best ways to speed up serving this image? Choose 2 answers Use Route 53\u2019s Latency Based Routing and serve the image out of us-west-2 as well as us-east-1 Serve the image out through CloudFront Serve the image out of S3 so that it isn\u2019t being served of your web application tier Use EBS PIOPs to serve the image faster out of your EC2 instances Your API requires the ability to stay online during AWS regional failures. Your API does not store any state, it only aggregates data from other sources \u2013 you do not have a database. What is a simple but effective way to achieve this uptime goal? Use a CloudFront distribution to serve up your API. Even if the region your API is in goes down, the edge locations CloudFront uses will be fine. Use an ELB and a cross-zone ELB deployment to create redundancy across datacenters. Even if a region fails, the other AZ will stay online. Create a Route53 Weighted Round Robin record, and if one region goes down, have that region redirect to the other region. Create a Route53 Latency Based Routing Record with Failover and point it to two identical deployments of your stateless API in two different regions. Make sure both regions use Auto Scaling Groups behind ELBs","title":"L10 AWS Route 53 Routing Policy"},{"location":"chap3/10route53_policy/#l10-aws-route-53-routing-policy","text":"","title":"L10 AWS Route 53 Routing Policy"},{"location":"chap3/10route53_policy/#1-aws-route-53-routing-policy","text":"AWS Route 53 routing policy determines how AWS would respond to the DNS queries and provides multiple Routing policy options","title":"1 AWS Route 53 Routing Policy"},{"location":"chap3/10route53_policy/#2-simple-routing-policy","text":"Simple routing policy is a simple round robin policy and can be applied when there is a single resource doing the function for the domain for e.g. web server that serves content for the website AWS Route 53 responds to the DNS queries based on the values in the resource record set for e.g. ip address in an A record","title":"2 Simple Routing Policy"},{"location":"chap3/10route53_policy/#3-weighted-routing-policy","text":"Weighted routing policy enables Route 53 to route traffic to different resources in specified proportions (weights) for e.g., 75% one server and 25% to the other during a pilot release Weights can be assigned any number from 0 to 255 Weighted routing policy can be applied when there are multiple resources that perform the same function for e.g., webservers serving the same site Weighted resource record sets let you associate multiple resources with a single DNS name Common use cases include load balancing A/B testing and piloting new versions of software To create a group of weighted resource record sets, two or more resource record sets can be created that have the same combination of DNS name and type, and each resource record set is assigned a unique identifier and a relative weight . When processing a DNS query, Route 53 searches for a resource record set or a group of resource record sets that have the specified name and type. Route 53 selects one from the group. Probability of any one resource record set being selected depends on its weight as a proportion of the total weight for all resource record sets in the group for e.g., suppose for www.example.com has three resource record sets with weights of 1 (20%), 1 (20%), and 3 (60%)(sum = 5). On average, Route 53 selects each of the first two resource record sets one-fifth of the time, and returns the third resource record set three-fifths of the time.","title":"3 Weighted Routing Policy"},{"location":"chap3/10route53_policy/#4-latency-based-routing-lbr-policy","text":"Latency-based Routing Policy enables Route 53 to respond to the DNS query based on which data center gives the user the lowest network latency Latency-based routing policy can be used when there are multiple resources performing the same function and Route 53 needs to be configured to respond to the DNS queries with the resources that provide the fastest response with lowest latency Latency resource record set can be created for the EC2 resource in each region that hosts the application. When Route 53 receives a query for the corresponding domain, it selects the latency resource record set for the EC2 region that gives the user the lowest latency. Route 53 then responds with the value associated with that resource record set for e.g., you might have web servers for example.com in the EC2 data centers in Ireland and in Tokyo. When a user browses to example.com from Singapore, Route 53 will pick up the data center (Tokyo) which has the lowest latency from the users location Latency between hosts on the Internet can change over time as a result of changes in network connectivity and routing. Latency-based routing is based on latency measurements performed over a period of time, and the measurements reflect these changes for e.g. if the latency from the user in Singapore to Ireland improves, the user can be routed to Ireland Latency based routing cannot guarantee users from the same geographic will be served from the same location for any compliance reason Latency resource record sets can be created using any record type that Route 53 supports except NS or SOA","title":"4 Latency-based Routing (LBR) Policy"},{"location":"chap3/10route53_policy/#4-failover-routing-policy","text":"Failover routing policy allows active-passive failover configuration, in which one resource takes all traffic when it\u2019s available and the other resource takes all traffic when the first resource isn\u2019t available . Route 53 health checking agents will monitor each location/endpoint of the application to determine the availability. Failover routing policy is applicable for Public hosted zones only","title":"4 Failover Routing Policy"},{"location":"chap3/10route53_policy/#5-geolocation-routing-policy","text":"Geolocation routing policy enables Route 53 to respond to DNS queries based on the geographic location of the users i.e. location from which the DNS queries originate Common use cases include localization of content and presenting some or all of the website in the users language restrict distribution of content to only the locations in which you have distribution rights. balancing load across endpoints in a predictable, easy-to-manage way, so that each user location is consistently routed to the same endpoint. Geolocation routing policy allows geographic locations to be specified by continent, country, or by state (only in US) Geolocation record sets, if created, for overlapping geographic regions for e.g. continent and then for the country within the same continent , priority goes to the smallest geographic region, which allows some queries for a continent to be routed to one resource and queries for selected countries on that continent to a different resource Geolocation works by mapping IP addresses to locations, which might not mapped to a exact geographic location A default resource record set can be created to handle these queries and also the ones which do not have an explicit record set created Route 53 returns a \u201cno answer\u201dresponse for queries from those locations, if a default resource record set if not created Two geolocation resource record sets that specify the same geographic location cannot be created Route 53 supports the edns-client-subnet extension of EDNS0 (EDNS0 adds several optional extensions to the DNS protocol.) to improve the accuracy of geolocation routing","title":"5 Geolocation Routing Policy"},{"location":"chap3/10route53_policy/#6-aws-certification-exam-practice-questions","text":"You have deployed a web application targeting a global audience across multiple AWS Regions under the domain name example.com. You decide to use Route 53 Latency-Based Routing to serve web requests to users from the region closest to the user. To provide business continuity in the event of server downtime you configure weighted record sets associated with two web servers in separate Availability Zones per region. During a DR test you notice that when you disable all web servers in one of the regions Route 53 does not automatically direct all users to the other region. What could be happening? (Choose 2 answers) Latency resource record sets cannot be used in combination with weighted resource record sets. You did not setup an http health check for one or more of the weighted resource record sets associated with the disabled web servers The value of the weight associated with the latency alias resource record set in the region with the disabled servers is higher than the weight for the other region. One of the two working web servers in the other region did not pass its HTTP health check You did not set \u201cEvaluate Target Health\u201d to \u201cYes\u201d on the latency alias resource record set associated with example.com in the region where you disabled the servers. The compliance department within your multi-national organization requires that all data for your customers that reside in the European Union (EU) must not leave the EU and also data for customers that reside in the US must not leave the US without explicit authorization. What must you do to comply with this requirement for a web based profile management application running on EC2? Run EC2 instances in multiple AWS Availability Zones in single Region and leverage an Elastic Load Balancer with session stickiness to route traffic to the appropriate zone to create their profile ( should be in 2 different regions \u2013 US and Europe ) Run EC2 instances in multiple Regions and leverage Route 53\u2019s Latency Based Routing capabilities to route traffic to the appropriate region to create their profile ( Latency based routing policy would not guarantee the compliance requirement ) Run EC2 instances in multiple Regions and leverage a third party data provider to determine if a user needs to be redirect to the appropriate region to create their profile Run EC2 instances in multiple AWS Availability Zones in a single Region and leverage a third party data provider to determine if a user needs to be redirect to the appropriate zone to create their profile( should be in 2 different regions \u2013 US and Europe ) A US-based company is expanding their web presence into Europe. The company wants to extend their AWS infrastructure from Northern Virginia (us-east-1) into the Dublin (eu-west-1) region. Which of the following options would enable an equivalent experience for users on both continents? Use a public-facing load balancer per region to load-balance web traffic, and enable HTTP health checks. Use a public-facing load balancer per region to load-balance web traffic, and enable sticky sessions. Use Amazon Route 53, and apply a geolocation routing policy to distribute traffic across both regions Use Amazon Route 53, and apply a weighted routing policy to distribute traffic across both regions. You have been asked to propose a multi-region deployment of a web-facing application where a controlled portion of your traffic is being processed by an alternate region. Which configuration would achieve that goal? Route 53 record sets with weighted routing policy Route 53 record sets with latency based routing policy Auto Scaling with scheduled scaling actions set Elastic Load Balancing with health checks enabled Your company is moving towards tracking web page users with a small tracking image loaded on each page. Currently you are serving this image out of us-east, but are starting to get concerned about the time it takes to load the image for users on the west coast. What are the two best ways to speed up serving this image? Choose 2 answers Use Route 53\u2019s Latency Based Routing and serve the image out of us-west-2 as well as us-east-1 Serve the image out through CloudFront Serve the image out of S3 so that it isn\u2019t being served of your web application tier Use EBS PIOPs to serve the image faster out of your EC2 instances Your API requires the ability to stay online during AWS regional failures. Your API does not store any state, it only aggregates data from other sources \u2013 you do not have a database. What is a simple but effective way to achieve this uptime goal? Use a CloudFront distribution to serve up your API. Even if the region your API is in goes down, the edge locations CloudFront uses will be fine. Use an ELB and a cross-zone ELB deployment to create redundancy across datacenters. Even if a region fails, the other AZ will stay online. Create a Route53 Weighted Round Robin record, and if one region goes down, have that region redirect to the other region. Create a Route53 Latency Based Routing Record with Failover and point it to two identical deployments of your stateless API in two different regions. Make sure both regions use Auto Scaling Groups behind ELBs","title":"6 AWS Certification Exam Practice Questions"},{"location":"chap3/11route53_resolver/","text":"L11 AWS Route 53 Resolver 1 AWS Route 53 \u2013 Resolving DNS Queries Between VPCs and On-premises Network Route 53 Resolver provides automatic DNS resolution within the VPC. By default, Resolver answers DNS queries for VPC domain names such as domain names for EC2 instances or ELB load balancers . Resolver performs recursive lookups against public name servers for all other domain names. However, on-premises instances cannot resolve Route 53 DNS entries and Route 53 cannot resolve on-premises DNS entries DNS resolution between AWS VPC and on-premises network can be configured over a Direct Connect or VPN connection Route 53 Resolver is regional . To use inbound or outbound forwarding, create a Resolver endpoint in the VPC . As part of the definition of an endpoint, specify the IP addresses to forward inbound DNS queries to or the IP addresses that outbound queries to originate from. For each IP address specified, Resolver automatically creates a VPC elastic network interface . 2 Forward DNS queries from resolvers on your network to Route 53 Resolver DNS resolvers on on-premises network can forward DNS queries to Resolver in a specified VPC . This enables DNS resolvers to easily resolve domain names for AWS resources such as EC2 instances or records in a Route 53 private hosted zone. 3 Conditionally forward queries from a VPC to resolvers on your network Route 53 Resolver can be configured to forward queries that it receives from EC2 instances in the VPCs to DNS resolvers on on-premises network. To forward selected queries, Resolver rules can be created that specify the domain names for the DNS queries that you want to forward (such as example.com), and the IP addresses of the DNS resolvers on on-premises network that you want to forward the queries to. If a query matches multiple rules (example.com, acme.example.com), Resolver chooses the rule with the most specific match (acme.example.com) and forwards the query to the IP addresses that you specified in that rule.","title":"L11 AWS Route 53 Resolver"},{"location":"chap3/11route53_resolver/#l11-aws-route-53-resolver","text":"","title":"L11 AWS Route 53 Resolver"},{"location":"chap3/11route53_resolver/#1-aws-route-53-resolving-dns-queries-between-vpcs-and-on-premises-network","text":"Route 53 Resolver provides automatic DNS resolution within the VPC. By default, Resolver answers DNS queries for VPC domain names such as domain names for EC2 instances or ELB load balancers . Resolver performs recursive lookups against public name servers for all other domain names. However, on-premises instances cannot resolve Route 53 DNS entries and Route 53 cannot resolve on-premises DNS entries DNS resolution between AWS VPC and on-premises network can be configured over a Direct Connect or VPN connection Route 53 Resolver is regional . To use inbound or outbound forwarding, create a Resolver endpoint in the VPC . As part of the definition of an endpoint, specify the IP addresses to forward inbound DNS queries to or the IP addresses that outbound queries to originate from. For each IP address specified, Resolver automatically creates a VPC elastic network interface .","title":"1 AWS Route 53 \u2013 Resolving DNS Queries Between VPCs and On-premises Network"},{"location":"chap3/11route53_resolver/#2-forward-dns-queries-from-resolvers-on-your-network-to-route-53-resolver","text":"DNS resolvers on on-premises network can forward DNS queries to Resolver in a specified VPC . This enables DNS resolvers to easily resolve domain names for AWS resources such as EC2 instances or records in a Route 53 private hosted zone.","title":"2 Forward DNS queries from resolvers on your network to Route 53 Resolver"},{"location":"chap3/11route53_resolver/#3-conditionally-forward-queries-from-a-vpc-to-resolvers-on-your-network","text":"Route 53 Resolver can be configured to forward queries that it receives from EC2 instances in the VPCs to DNS resolvers on on-premises network. To forward selected queries, Resolver rules can be created that specify the domain names for the DNS queries that you want to forward (such as example.com), and the IP addresses of the DNS resolvers on on-premises network that you want to forward the queries to. If a query matches multiple rules (example.com, acme.example.com), Resolver chooses the rule with the most specific match (acme.example.com) and forwards the query to the IP addresses that you specified in that rule.","title":"3 Conditionally forward queries from a VPC to resolvers on your network"},{"location":"chap3/12api_gateway/","text":"L12 AWS API Gateway 1 AWS API Gateway AWS API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, and secure APIs at any scale API Gateway handles all of the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, authorization and access control, monitoring, and API version management. API Gateway has no minimum fees or startup costs and charges only for the API calls received and the amount of data transferred out . API Gateway acts as a proxy to the configured backend operations . API Gateway scales automatically to handle the amount of traffic the API receives API Gateway expose HTTPS endpoints only for all the APIs created . It does not support unencrypted (HTTP) endpoints APIs built on API Gateway can accept any payloads sent over HTTP with typical data formats include JSON, XML, query string parameters, and request headers API Gateway can communicate to multiple backends Lambda functions AWS Step functions state machines HTTP endpoints exposed through Elastic Beanstalk, ELB or EC2 servers Non AWS hosted HTTP based operations accessible via public Internet API Gateway endpoints are always public to the Internet and does not run within an VPC. Proxy requests to backend operations also need to be publicly accessible on the Internet . API Gateway helps with several aspects of creating and managing APIs Metering automatically meters traffic to the APIs and and lets you extract utilization data for each API key. define plans that meter, restrict third-party developer access, configure throttling, and quota limits on a per API key basis Security helps removing authorization concerns from the backend code allows leveraging of AWS administration and security tools, such as IAM and Cognito, to authorize access to APIs can verify signed API calls on your behalf using the same methodology AWS uses for its own APIs supports custom authorizers written as Lambda functions and verify incoming bearer tokens automatically protects the backend systems from distributed denial-of-service (DDoS) attacks, whether attacked with counterfeit requests (Layer 7) or SYN floods (Layer 3). Resiliency helps manage traffic with throttling so that backend operations can withstand traffic spikes helps improve the performance of the APIs and the latency end users experience by caching the output of API calls to avoid calling the backend every time. Operations Monitoring integrates with CloudWatch and provides a metrics dashboard to monitor calls to API services integrates with CloudWatch Logs to receive error, access or debug logs provides with backend performance metrics covering API calls, latency data and error rates. Lifecycle Management allows multiple API versions and multiple stages (development, staging, production etc.) for each version simultaneously so that existing applications can continue to call previous versions after new API versions are published. saves the history of the deployments, which allows rollback of a stage to a previous deployment at any point, using APIs or console Designed for Developers allows you to specify a mapping template to generate static content to be returned, helping you mock APIs before the backend is ready helps reduce cross-team development effort and time-to-market for applications and allow dependent teams to begin development while backend processes is still built 2 API Gateway Throttling and Caching Throttling API Gateway provides throttling at multiple levels including global and by service call and limits can be set for standard rates and bursts It tracks the number of requests per second. Any requests over the limit will receive a 429 HTTP response Throttling ensures that API traffic is controlled to help the backend services maintain performance and availability. Caching API Gateway provides API result caching by provisioning an API Gateway cache and specifying its size in gigabytes Caching helps improve performance and reduces the traffic sent to the back end API Gateway handles the request in the following manner If caching is not enabled and throttling limits have not been applied, then all requests pass through to the backend service until the account level throttling limits are reached. If throttling limits specified, then API Gateway will shed necessary amount of requests and send only the defined limit to the back-end If a cache is configured, then API Gateway will return a cached response for duplicate requests for a customizable time, but only if under configured throttling limits API Gateway does not arbitrarily limit or throttle invocations to the backend operations and all requests that are not intercepted by throttling and caching settings are sent to your backend operations. You are running a mobile media application and are considering API Gateway for the client entry point. What benefits would this provide? Choose 2 answers Caching API responses IP blacklisting Intrusion prevention Load balancing Throttling traffic","title":"L12 AWS API Gateway"},{"location":"chap3/12api_gateway/#l12-aws-api-gateway","text":"","title":"L12 AWS API Gateway"},{"location":"chap3/12api_gateway/#1-aws-api-gateway","text":"AWS API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, and secure APIs at any scale API Gateway handles all of the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, authorization and access control, monitoring, and API version management. API Gateway has no minimum fees or startup costs and charges only for the API calls received and the amount of data transferred out . API Gateway acts as a proxy to the configured backend operations . API Gateway scales automatically to handle the amount of traffic the API receives API Gateway expose HTTPS endpoints only for all the APIs created . It does not support unencrypted (HTTP) endpoints APIs built on API Gateway can accept any payloads sent over HTTP with typical data formats include JSON, XML, query string parameters, and request headers API Gateway can communicate to multiple backends Lambda functions AWS Step functions state machines HTTP endpoints exposed through Elastic Beanstalk, ELB or EC2 servers Non AWS hosted HTTP based operations accessible via public Internet API Gateway endpoints are always public to the Internet and does not run within an VPC. Proxy requests to backend operations also need to be publicly accessible on the Internet . API Gateway helps with several aspects of creating and managing APIs","title":"1 AWS API Gateway"},{"location":"chap3/12api_gateway/#metering","text":"automatically meters traffic to the APIs and and lets you extract utilization data for each API key. define plans that meter, restrict third-party developer access, configure throttling, and quota limits on a per API key basis","title":"Metering"},{"location":"chap3/12api_gateway/#security","text":"helps removing authorization concerns from the backend code allows leveraging of AWS administration and security tools, such as IAM and Cognito, to authorize access to APIs can verify signed API calls on your behalf using the same methodology AWS uses for its own APIs supports custom authorizers written as Lambda functions and verify incoming bearer tokens automatically protects the backend systems from distributed denial-of-service (DDoS) attacks, whether attacked with counterfeit requests (Layer 7) or SYN floods (Layer 3).","title":"Security"},{"location":"chap3/12api_gateway/#resiliency","text":"helps manage traffic with throttling so that backend operations can withstand traffic spikes helps improve the performance of the APIs and the latency end users experience by caching the output of API calls to avoid calling the backend every time.","title":"Resiliency"},{"location":"chap3/12api_gateway/#operations-monitoring","text":"integrates with CloudWatch and provides a metrics dashboard to monitor calls to API services integrates with CloudWatch Logs to receive error, access or debug logs provides with backend performance metrics covering API calls, latency data and error rates.","title":"Operations Monitoring"},{"location":"chap3/12api_gateway/#lifecycle-management","text":"allows multiple API versions and multiple stages (development, staging, production etc.) for each version simultaneously so that existing applications can continue to call previous versions after new API versions are published. saves the history of the deployments, which allows rollback of a stage to a previous deployment at any point, using APIs or console","title":"Lifecycle Management"},{"location":"chap3/12api_gateway/#designed-for-developers","text":"allows you to specify a mapping template to generate static content to be returned, helping you mock APIs before the backend is ready helps reduce cross-team development effort and time-to-market for applications and allow dependent teams to begin development while backend processes is still built","title":"Designed for Developers"},{"location":"chap3/12api_gateway/#2-api-gateway-throttling-and-caching","text":"","title":"2 API Gateway Throttling and Caching"},{"location":"chap3/12api_gateway/#throttling","text":"API Gateway provides throttling at multiple levels including global and by service call and limits can be set for standard rates and bursts It tracks the number of requests per second. Any requests over the limit will receive a 429 HTTP response Throttling ensures that API traffic is controlled to help the backend services maintain performance and availability.","title":"Throttling"},{"location":"chap3/12api_gateway/#caching","text":"API Gateway provides API result caching by provisioning an API Gateway cache and specifying its size in gigabytes Caching helps improve performance and reduces the traffic sent to the back end API Gateway handles the request in the following manner If caching is not enabled and throttling limits have not been applied, then all requests pass through to the backend service until the account level throttling limits are reached. If throttling limits specified, then API Gateway will shed necessary amount of requests and send only the defined limit to the back-end If a cache is configured, then API Gateway will return a cached response for duplicate requests for a customizable time, but only if under configured throttling limits API Gateway does not arbitrarily limit or throttle invocations to the backend operations and all requests that are not intercepted by throttling and caching settings are sent to your backend operations. You are running a mobile media application and are considering API Gateway for the client entry point. What benefits would this provide? Choose 2 answers Caching API responses IP blacklisting Intrusion prevention Load balancing Throttling traffic","title":"Caching"},{"location":"chap3/13cloudfront/","text":"L13 AWS CloudFront 1 CloudFront CloudFront is a fully managed, fast content delivery network (CDN) service that speeds up the distribution of static, dynamic web, or streaming content to end-users . CloudFront delivers the content through a worldwide network of data centers called edge locations or Point of Presence (POP) . CloudFront securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds. CloudFront speeds up the distribution of the content by routing each user request to the edge location that can best serve the content thus providing the lowest latency (time delay). CloudFront dramatically reduces the number of network hops that users\u2019 requests must pass through, which helps improves performance, provide lower latency and higher data transfer rates. CloudFront is a good choice for the distribution of frequently accessed static content that benefits from edge delivery \u2013 like popular website images, videos, media files, or software downloads 2 CloudFront Benefits CloudFront eliminates the expense and complexity of operating a network of cache server s in multiple sites across the internet and eliminates the need to over-provision capacity in order to serve potential spikes in traffic. CloudFront also provides increased reliability and availability because copies of objects are held in multiple edge locations around the world. CloudFront keeps persistent connections with the origin servers so that those files can be fetched from the origin servers as quickly as possible. CloudFront also uses techniques such as collapsing simultaneous viewer requests at an edge location for the same file into a single request to the origin server reducing the load on the origin . CloudFront offers the most advanced security capabilities, including field-level encryption and HTTPS support. CloudFront seamlessly integrates with AWS Shield, AWS Web Application Firewall, and Route 53 to protect against multiple types of attacks including network and application layer DDoS attacks . 3 Configuration & Content Delivery 3-1 Configuration Origin servers need to be configured to get the files for distribution . An origin server stores the original, definitive version of the objects and can be an AWS hosted service for e.g. S3, EC2, or an on-premise server Files or objects can be added/uploaded to the Origin servers with public read permissions or permissions restricted to Origin Access Identity (OAI). Create a CloudFront distribution, which tells CloudFront which origin servers to get the files from when users request the files CloudFront sends the distribution configuration to all the edge locations The website can be used with the CloudFront provided domain name or a custom alternate domain name An origin server can be configured to limit access protocols, caching behavior, add headers to the files to add TTL or the expiration time 3-2 Content delivery to Users When a user accesses the website, file, or object \u2013 the DNS routes the request to the CloudFront edge location that can best serve the user\u2019s request with the lowest latency CloudFront returns the object immediately if the requested object is present in the cache at the Edge location If the requested object does not exist in the cache at the edge location, CloudFront requests the object from the origin server and returns it to the user as soon as it starts receiving it When the object reaches its expiration time, for any new request CloudFront checks with the Origin server for any latest versions, if it has the latest it uses the same object. If the Origin server has the latest version the same is retrieved, served to the user, and cached as well 4 Delivery Methods 4-1 Web distributions supports both s tatic and dynamic content for e.g. HTML, CSS, js, images, etc using HTTP or HTTPS . supports multimedia content on-demand using progressive download and Apple HTTP Live Streaming (HLS) . supports a live event, such as a meeting, conference , or concert, in real-time. For live streaming, distribution can be created automatically using an AWS CloudFormation stack. origin servers can be either an S3 bucket or an HTTP server, for e.g., a web server or an AWS ELB, etc. 4-2 CloudFront Origins Each origin is either an S3 bucket, a MediaStore container, a MediaPackage channel, or a custom origin like EC2 instance or an HTTP server For the S3 bucket, use the bucket URL or the static website endpoint URL, and the files either need to be publicly readable or secured using OAI. Origin restrict access, for S3 only , can be configured using Origin Access Identity to prevent direct access to the S3 objects For the HTTP server as the origin, the domain name of the resource needs to be mapped and files must be publicly readable . Distribution can have multiple origins for each bucket with one or more cache behaviors that route requests to each origin . Path pattern in a cache behavior determines which requests are routed to the origin (S3 bucket) that is associated with that cache behavior 5 Cache Behavior Settings 5-1 Path Patterns Path Patterns help define which path the Cache behavior would apply to. A default (*) pattern is created and multiple cache distributions can be added with patterns to take priority over the default path 5-2 Viewer Protocol Policy Viewer Protocol policy can be configured to define the allowed access protocol. Between CloudFront & Viewers, cache distribution can be configured to either allow HTTPS only \u2013 supports HTTPS only HTTP and HTTPS \u2013 supports both HTTP redirected to HTTPS \u2013 HTTP is automatically redirected to HTTPS 5-3 HTTPS Connection Between CloudFront & Origin, cache distribution can be configured to require that CloudFront fetches objects from the origin by using HTTPS or CloudFront uses the protocol that the viewer used to request the objects . For S3 as origin , For website, the protocol has to be HTTP as HTTPS is not supported For S3 bucket, the default Origin protocol policy is Match Viewer and cannot be changed. So When CloudFront is configured to require HTTPS between the viewer and CloudFront, it automatically uses HTTPS to communicate with S3 . CloudFront can also be configured to work with HTTPS for alternate domain names by using:- Serving HTTPS Requests Using Dedicated IP Addresses CloudFront associates the alternate domain name with a dedicated IP address, and the certificate is associated with the IP address. when a request is received from a DNS server for the IP address, CloudFront uses the IP address to identify the distribution and the SSL/TLS certificate to return to the viewer This method works for every HTTPS request, regardless of the browser or other viewer that the user is using. * Additional monthly charge (of about $600/month) is incurred for using a dedicated IP address Serving HTTPS Requests Using SNI SNI custom SSL relies on the SNI extension of the TLS protocol, which allows multiple domains to be served over the same IP address by including the hostname, viewers are trying to connect to With SNI method, CloudFront associates an IP address with the alternate domain name, but the IP address is not dedicated CloudFront can\u2019t determine, based on the IP address, which domain the request is for as the IP address is not dedicated Browsers that support SNI automatically gets the domain name from the request URL & adds it to a new field in the request header. When CloudFront receives an HTTPS request from a browser that supports SNI, it finds the domain name in the request header and responds to the request with the applicable SSL/TLS certificate. Viewer and CloudFront perform SSL negotiation, and CloudFront returns the requested content to the viewer. Older browsers do not support it SNI Custom SSL is available at no additional cost beyond standard CloudFront data transfer and request fees For End-to-End HTTPS connections certificate needs to be applied both between the Viewers and CloudFront & CloudFront and Origin, with the following requirements HTTPS between viewers and CloudFront Certificate that was issued by a trusted certificate authority (CA) such as Comodo, DigiCert, or Symantec; Certificate provided by AWS Certificate Manager (ACM); Self-signed certificate. HTTPS between CloudFront and a custom origin If the origin is not an ELB load balancer, the certificate must be issued by a trusted CA such as Comodo, DigiCert, or Symantec. For ELB load balancer, certificate provided by ACM can be used 6 Allowed HTTP methods CloudFront supports GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE to get, add, update, and delete objects, and to get object headers. GET, HEAD methods to use to get objects, object headers GET, HEAD, OPTIONS methods to use to get objects, object headers or retrieve a list of the options supported from the origin GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE operations can also be performed for e.g. submitting data from a web form, which are directly proxied back to the Origin server CloudFront only caches responses to GET and HEAD requests and, optionally, OPTIONS requests. CloudFront does not cache responses to PUT, POST, PATCH, DELETE request methods and these requests are directed to the origin . PUT, POST HTTP methods also help for accelerated content uploads, as these operations will be sent to the origin e.g. S3 via the CloudFront edge location, improving efficiency, reducing latency, and allowing the application to benefit from the monitored, persistent connections that CloudFront maintains from the edge locations to the origin servers. 7 Field Level Encryption Config CloudFront can enforce secure end-to-end connections to origin servers by using HTTPS . Field-level encryption adds an additional layer of security that helps protect specific data throughout system processing so that only certain applications can see it. 8 CloudFront Edge Caches Control the cache max-age To increase the cache hit ratio, the origin can be configured to add a Cache-Control: max-age directive to the objects. Longer the interval less frequently it would be retrieved from the origin Caching Based on Query String Parameters CloudFront can be configured to cache based on the query parameters None (Improves Caching) \u2013 if the origin returns the same version of an object regardless of the values of query string parameters. Forward all, cache based on whitelist \u2013 if the origin server returns different versions of the objects based on one or more query string parameters. Then specify the parameters that you want CloudFront to use as a basis for caching in the Query String Whitelist field. Forward all, cache based on all \u2013 if the origin server returns different versions of the objects for all query string parameters. Caching performance can be improved by Configure CloudFront to forward only the query strings for which the origin will return unique objects. Using the same case for the parameters\u2019 values for e.g. parameter value A or a, CloudFront would cache the same request twice even if the response or object returned is identical Using the same parameter order for e.g. for request a=x&b=y and b=y&a=x, CloudFront would cache the same request twice even though the response or object returned is identical Caching Based on Cookie Values CloudFront can be configured to cache based on cookie values. By default, it doesn\u2019t consider cookies while caching on edge locations Caching performance can be improved by Configure CloudFront to forward only specified cookies instead of forwarding all cookies for e.g. if the request has 2 cookies with 3 possible values, CloudFront would cache all possible combinations even if the response takes into account a single cookie Cookie names and values are both case sensitive so better to stick with the same case Create separate cache behaviors for static and dynamic content, and configure CloudFront to forward cookies to the origin only for dynamic content for e.g. for CSS files, the cookies do not make sense as the object does not change with the cookie value If possible, create separate cache behaviors for dynamic content for which cookie values are unique for each user (such as a user ID) and dynamic content that varies based on a smaller number of unique values reducing the number of combinations Caching Based on Request Headers CloudFront can be configured to cache based on request headers By default, CloudFront doesn\u2019t consider headers when caching the objects in edge locations. CloudFront configured to cache based on request headers, does not change the headers that CloudFront forwards, only whether CloudFront caches objects based on the header values. Caching performance can be improved by Configure CloudFront to forward and cache based only on specified headers instead of forwarding and caching based on all headers. Try to avoid caching based on request headers that have large numbers of unique values. CloudFront is configured to forward all headers to the origin, CloudFront doesn\u2019t cache the objects associated with this cache behavior. Instead, it sends every request to the origin CloudFront caches based on header values, it doesn\u2019t consider the case of the header name but considers the case of the header value 9 Object Caching & Expiration Object expiration determines how long the objects stay in a CloudFront cache before it fetches it again from Origin Low expiration time helps serve content that changes frequently and high expiration time helps improve performance and reduce the origin load. By default, each object automatically expires after 24 hours After expiration time, CloudFront checks if it still has the latest version If the cache already has the latest version, the origin returns a 304 status code (Not Modified). If the CloudFront cache does not have the latest version, the origin returns a 200 status code (OK), and the latest version of the object If an object in an edge location isn\u2019t frequently requested, CloudFront might evict the object, remove the object before its expiration date, to make room for objects that have been requested more recently. For Web distributions, the default behavior can be changed by for the entire path pattern, cache behavior can be configured by the setting Minimum TTL, Maximum TTL, and Default TTL values for individual objects, the origin can be configured to add a Cache-Control max-age or Cache-Control s-maxage directive, or an Expires header field to the object . AWS recommends using Cache-Control max-age directive over Expires header to control object caching behavior CloudFront uses only the value of Cache-Control max-age , if both the Cache-Control max-age directive and Expires header is specified HTTP Cache-Control or Pragma header fields in a GET request from a viewer can\u2019t be used to force CloudFront to go back to the origin server for the object By default, when the origin returns an HTTP 4xx or 5xx status code, CloudFront caches these error responses for five minutes and then submits the next request for the object to the origin to see whether the requested object is available and the problem has been resolved 10 Restrict Viewer Access 10-1 Serving Private Content To securely serve private content using CloudFront Require the users to access the private content by using special CloudFront signed URLs or signed cookies with following restrictions an end date and time, after which the URL is no longer valid start date time, when the URL becomes valid ip address or range of addresses to access the URLs Require that users access the S3 content only using CloudFront URLs, not S3 URLs. Requiring CloudFront URLs isn\u2019t required, but recommended to prevent users from bypassing the restrictions specified in signed URLs or signed cookies. Signed URLs or Signed Cookies can used with CloudFront using HTTP server as an origin. It requires the content to be publicly accessible and care should be taken to not share the direct URL of the content Restriction for Origin can be applied by For S3, using Origin Access Identity to grant only CloudFront access using Bucket policies or Object ACL, to the content and removing any other access permissions For HTTP server, custom header can be added by CloudFront which can be used at Origin to verify the request has come from CloudFront Trusted Signer To create signed URLs or signed cookies, at least one AWS account (trusted signer) is needed that has an active CloudFront key pair Once AWS account is added as trusted signer to the distribution, CloudFront starts to require that users use signed URLs or signed cookies to access the objects . Private key from the trusted signer\u2019s key pair to sign a portion of the URL or the cookie. When someone requests a restricted object, CloudFront compares the signed portion of the URL or cookie with the unsigned portion to verify that the URL or cookie hasn\u2019t been tampered with. CloudFront also validates the URL or cookie is valid for e.g, that the expiration date and time hasn\u2019t passed. Each Trusted signer AWS accounts used to create CloudFront signed URLs or signed cookies must have its own active CloudFront key pair, which should be frequently rotated A maximum of 5 trusted signers can be assigned for each cache behavior or RTMP distribution 10-2 Signed URLs vs Signed Cookies CloudFront signed URLs and signed cookies help to secure the content and provide control to decide who can access the content. Use signed URLs in the following cases: to restrict access to individual files, for e.g., an installation download for the application. users using a client, for e.g. a custom HTTP client, that doesn\u2019t support cookies Use signed cookies in the following cases : provide access to multiple restricted files, for e.g., all of the video files in HLS format or all of the files in the subscribers\u2019 area of a website. don\u2019t want to change the current URLs. Signed URLs take precedence over signed cookies, if both signed URLs and signed cookies are used to control access to the same files and a viewer uses a signed URL to request a file, CloudFront determines whether to return the file to the viewer based only on the signed URL. 10-3 Canned Policy vs Custom Policy Canned policy or a custom policy is a policy statement, used by the Signed URLs, helps define the restrictions for e.g. expiration date and time CloudFront validates the expiration time at the start of the event. If user is downloading a large object, and the url expires the download would still continue and the same for RTMP distribution. However, if the user is using range GET requests, or while streaming video skips to another position which might trigger an other event, the request would fail. 11 Serving Compressed Files CloudFront can be configured to automatically compress files of certain types and serve the compressed files when viewer requests include Accept-Encodin g in the request header Compressing content, downloads are faster because the files are smaller as well as less expensive as the cost of CloudFront data transfer is based on the total amount of data served . CloudFront can compress objects using the Gzip and Brotli compression formats . If serving from a custom origin, it can be used to configure to compress files with or without CloudFront compression compress file types that CloudFront doesn\u2019t compress. If the origin returns a compressed file, CloudFront detects compression by the Content-Encoding header value and doesn\u2019t compress the file again. CloudFront serves content using compression as below CloudFront distribution is created and configured to compress content. A viewer requests a compressed file by adding the Accept-Encoding header with includes gzip, br, or both to the request . At the edge location, CloudFront checks the cache for a compressed version of the file that is referenced in the request. If the compressed file is already in the cache, CloudFront returns the file to the viewer and skips the remaining steps. If the compressed file is not in the cache, CloudFront forwards the request to the origin server (S3 bucket or a custom origin) Even if CloudFront has an uncompressed version of the file in the cache, it still forwards a request to the origin. Origin server returns an uncompressed version of the requested file CloudFront determines whether the file is compressible: file must be of a type that CloudFront compresses. file size must be between 1,000 and 10,000,000 bytes. response must include a Content-Length header to determine the size within valid compression limits. If the Content-Length header is missing, CloudFront won\u2019t compress the file. value of the Content-Encoding header on the file must not be gzip i.e. the origin has already compressed the file. the response should have a body response HTTP status code should be 200, 403, or 404 If the file is compressible, CloudFront compresses it, returns the compressed file to the viewer, and adds it to the cache. The viewer uncompresses the file. 12 Distribution Details 12-1 Price Class CloudFront has edge locations all over the world and as cost for each edge location varies and the price charged for serving the requests also varies CloudFront edge locations are grouped into geographic regions, and regions have been grouped into price classes Price Class \u2013 includes all the regions Price Class 200 \u2013 Includes All regions except South America and Australia and New Zealand. Price Class 100 \u2013 A third price class includes only the least-expensive regions (North America and Europe regions) Price class can be selected to lower the cost but this would come only at the expense of performance (higher latency), as CloudFront would serve requests only from the selected price class edge locations CloudFront may, sometimes, service requests from a region not included within the price class, however, you would be charged the rate for the least-expensive region in your selected price class 12-2 WAF Web ACL AWS WAF can be used to allow or block requests based on specified criteria, choose the web ACL to associate with this distribution. 12-3 Alternate Domain Names (CNAMEs) CloudFront by default assigns a domain name for the distribution for e.g. d111111abcdef8.cloudfront.net An alternate domain name, also known as a CNAME, can be used to use own custom domain name for links to objects CloudFront supports * wildcard at the beginning of a domain name instead of specifying subdomains individually. However, a wildcard cannot replace part of a subdomain name for e.g. *domain.example.com , or cannot replace a subdomain in the middle of a domain name for e.g. subdomain.*.example.com. 12-4 Distribution State Distribution state indicates whether you want the distribution to be enabled or disabled once it\u2019s deployed. 13 Geo-Restriction \u2013 Geoblocking Geo restriction can help allow or prevent users in selected countries from accessing the content, CloudFront distribution can be configured either to allow users in whitelist of specified countries to access the content or to deny users in a blacklist of specified countries to access the content Geo restriction can be used to restrict access to all of the files that are associated with distribution and to restrict access at the country level CloudFront responds to a request from a viewer in a restricted country with an HTTP status code 403 (Forbidden) Use a third-party geolocation service, if access is to be restricted to a subset of the files that are associated with a distribution or to restrict access at a finer granularity than the country level. 14 CloudFront with S3 CloudFront can be used to distribute the content from an S3 bucket For an RTMP distribution, S3 bucket is the only supported origin and custom origins cannot be used Using CloudFront over S3 has the following benefits can be more cost effective if the objects are frequently accessed as at higher usage, the price for CloudFront data transfer is much lower than the price for S3 data transfer. downloads are faster with CloudFront than with S3 alone because the objects are stored closer to the users When using S3 as the origin for a distribution and the bucket is moved to a different region, CloudFront can take up to an hour to update its records to include the change of region when both of the following are true: Origin Access Identity (OAI) is used to restrict access to the bucket Bucket is moved to an S3 region that requires Signature Version 4 for authentication 15 Origin Access Identity With S3 as origin, objects in S3 must be granted public read permissions and hence the objects are accessible from both S3 as well as CloudFront Even though, CloudFront does not expose the underlying S3 url, it can be known to the user if shared directly or used by applications For using CloudFront signed URLs or signed cookies to provide access to the objects, it would be necessary to prevent users from having direct access to the S3 objects Users accessing S3 objects directly would bypass the controls provided by CloudFront signed URLs or signed cookies, for e.g., control over the date time that a user can no longer access the content and the IP addresses can be used to access content CloudFront access logs are less useful because they\u2019re incomplete. Origin Access Identity (OAI) can be used to prevent users from directly accessing objects from S3 Origin access identity, which is a special CloudFront user, can be created and associated with the distribution. S3 bucket/object permissions needs to be configured to only provide access to the Origin Access Identity When users access the object from CloudFront, it uses the OAI to fetch the content on users behalf, while direct access to the S3 objects is restricted 16 Working with Objects CloudFront can be configured to include custom headers or modify existing headers whenever it forwards a request to the origin, to validate the user is not accessing the origin directly, bypassing CDN identify the CDN from which the request was forwarded, if more than one CloudFront distribution is configured to use the same origin if users use viewers that don\u2019t support CORS, configure CloudFront to forward the Origin header to the origin. That will cause the origin to return the Access-Control-Allow-Origin header for every request 16-1 Adding & Updating Objects Objects just need to be added to the Origin and CloudFront would start distributing them when accessed Objects served by CloudFront the Origin, can be updated either by Overwriting the Original object Create a different version and updating the links exposed to the user For updating objects, its recommended to use versioning for e.g. have files or the entire folders with versions, so the the links can be changed when the objects are updated forcing a refresh With versioning, there is no time wait for an object to expire before CloudFront begins to serve a new version of it there is no difference in consistency in the object served from the edge no cost involved to pay for object invalidation. 16-2 Removing/Invalidating Objects Objects, by default, would be removed upon expiry (TTL) and the latest object would be fetched from the Origin Objects can also be removed from the edge cache before it expires File or Object Versioning to serve a different version of the object that has a different name. Invalidate the object from edge caches. For the next request, CloudFront returns to the Origin to fetch the object Object or File Versioning is recommended over Invalidating objects if the objects need to be updated frequently. enables to control which object a request returns even when the user has a version cached either locally or behind a corporate caching proxy. makes it easier to analyze the results of object changes as CloudFront access logs include the names of the objects provides a way to serve different versions to different users. simplifies rolling forward & back between object revisions. is less expensive, as no charges for invalidating objects. for e.g. change header-v1.jpg to header-v2.jpg Invalidating objects from the cache objects in the cache can be invalidated explicitly before they expire to force a refresh allows to invalidate selected objects allows to invalidate multiple objects for e.g. objects in a directory or all of the objects whose names begin with the same characters, you can include the * wildcard at the end of the invalidation path. the user might continue to see the old version until it expires from those caches. A specified number of invalidation paths can be submitted each month for free. Any invalidation requests more than the allotted no. per month, a fee is charged for each submitted invalidation path The First 1,000 invalidation paths requests submitted per month are free; charges apply for each invalidation path over 1,000 in a month. Invalidation path can be for a single object for e.g. /js/ab.js or for multiple objects for e.g. /js/* and is counted as a single request even if the * wildcard request may invalidate thousands of objects. 16-3 Partial Requests (Range GETs) Partial requests using Range headers in a GET request helps to download the object in smaller units, improving the efficiency of partial downloads and the recovery from partially failed transfers. For a partial GET range request, CloudFront checks the cache in the edge location for the requested range or the entire object and if exists, serves it immediately if the requested range does not exist, it forwards the request to the origin and may request a larger range than the client requested to optimize performance if the origin supports range header, it returns the requested object range and CloudFront returns the same to the viewer if the origin does not support range header, it returns the complete object and CloudFront serves the entire object and caches it for future. CloudFront uses the cached entire object to serve any future range GET header requests 17 CloudFront Security CloudFront provides Encryption in Transit and can be configured to require that viewers use HTTPS to request the files so that connections are encrypted when CloudFront communicates with viewers . CloudFront provides Encryption at Rest uses SSDs which are encrypted for edge location points of presence (POPs), and encrypted EBS volumes for Regional Edge Caches (RECs). Function code and configuration are always stored in an encrypted format on the encrypted SSDs on the edge location POPs, and in other storage locations used by CloudFront. Restricting access to content Configure HTTPS connections Use signed URLs or cookies to restrict access for selected users Restrict access to content in S3 buckets using origin access identity \u2013 OAI, to prevent users from using the direct URL of the file. Set up field-level encryption for specific content fields Use AWS WAF web ACLs to create a web access control list (web ACL) to restrict access to your content. Use geo-restriction, also known as geoblocking, to prevent users in specific geographic locations from accessing content served through a CloudFront distribution. 18 Access Logs CloudFront can be configured to create log files that contain detailed information about every user request that CloudFront receives. Access logs are available for both web and RTMP distributions. With logging enabled, an S3 bucket can be specified where CloudFront would save the files CloudFront delivers access logs for a distribution periodically, up to several times an hour CloudFront usually delivers the log file for that time period to the S3 bucket within an hour of the events that appear in the log. Note, however, that some or all log file entries for a time period can sometimes be delayed by up to 24 hours 19 CloudFront Cost CloudFront charges are based on actual usage of the service in four areas: Data Transfer Out to Internet charges are applied for the volume of data transferred out of the CloudFront edge locations, measured in GB Data transfer out from AWS origin (e.g., S3, EC2, etc.) to CloudFront are no longer charged. This applies to data transfer from all AWS regions to all global CloudFront edge locations HTTP/HTTPS Requests number of HTTP/HTTPS requests made for the content Invalidation Requests per path in the invalidation request A path listed in the invalidation request represents the URL (or multiple URLs if the path contains a wildcard character) of the object you want to invalidate from the CloudFront cache Dedicated IP Custom SSL certificates associated with a CloudFront distribution $600 per month for each custom SSL certificate associated with one or more CloudFront distributions using the Dedicated IP version of custom SSL certificate support, pro-rated by the hour Your company Is moving towards tracking web page users with a small tracking Image loaded on each page Currently you are serving this image out of US-East, but are starting to get concerned about the time It takes to load the image for users on the west coast. What are the two best ways to speed up serving this image? Choose 2 answers Use Route 53\u2019s Latency Based Routing and serve the image out of US-West-2 as well as US-East-1 Serve the image out through CloudFront Serve the image out of S3 so that it isn\u2019t being served oft of your web application tier Use EBS PIOPs to serve the image faster out of your EC2 instances You deployed your company website using Elastic Beanstalk and you enabled log file rotation to S3. An Elastic Map Reduce job is periodically analyzing the logs on S3 to build a usage dashboard that you share with your CIO. You recently improved overall performance of the website using Cloud Front for dynamic content delivery and your website as the origin. After this architectural change, the usage dashboard shows that the traffic on your website dropped by an order of magnitude. How do you fix your usage dashboard\u2019? [PROFESSIONAL] Enable CloudFront to deliver access logs to S3 and use them as input of the Elastic Map Reduce job Turn on Cloud Trail and use trail log tiles on S3 as input of the Elastic Map Reduce job Change your log collection process to use Cloud Watch ELB metrics as input of the Elastic Map Reduce job Use Elastic Beanstalk \u201cRebuild Environment\u201d option to update log delivery to the Elastic Map Reduce job. Use Elastic Beanstalk \u2018Restart App server(s)\u201d option to update log delivery to the Elastic Map Reduce job. An AWS customer runs a public blogging website. The site users upload two million blog entries a month. The average blog entry size is 200 KB. The access rate to blog entries drops to negligible 6 months after publication and users rarely access a blog entry 1 year after publication. Additionally, blog entries have a high update rate during the first 3 months following publication; this drops to no updates after 6 months. The customer wants to use CloudFront to improve his user\u2019s load times. Which of the following recommendations would you make to the customer? [PROFESSIONAL] Duplicate entries into two different buckets and create two separate CloudFront distributions where S3 access is restricted only to Cloud Front identity Create a CloudFront distribution with \u201cUS & Europe\u201d price class for US/Europe users and a different CloudFront distribution with All Edge Locations for the remaining users. Create a CloudFront distribution with S3 access restricted only to the CloudFront identity and partition the blog entry\u2019s location in S3 according to the month it was uploaded to be used with CloudFront behaviors Create a CloudFront distribution with Restrict Viewer Access Forward Query string set to true and minimum TTL of 0. Your company has on-premises multi-tier PHP web application, which recently experienced downtime due to a large burst in web traffic due to a company announcement. Over the coming days, you are expecting similar announcements to drive similar unpredictable bursts, and are looking to find ways to quickly improve your infrastructures ability to handle unexpected increases in traffic. The application currently consists of 2 tiers a web tier, which consists of a load balancer, and several Linux Apache web servers as well as a database tier which hosts a Linux server hosting a MySQL database. Which scenario below will provide full site functionality, while helping to improve the ability of your application in the short timeframe required? [PROFESSIONAL] Offload traffic from on-premises environment Setup a CloudFront distribution and configure CloudFront to cache objects from a custom origin Choose to customize your object cache behavior, and select a TTL that objects should exist in cache. Migrate to AWS Use VM Import/Export to quickly convert an on-premises web server to an AMI create an Auto Scaling group, which uses the imported AMI to scale the web tier based on incoming traffic Create an RDS read replica and setup replication between the RDS instance and on-premises MySQL server to migrate the database. Failover environment: Create an S3 bucket and configure it tor website hosting Migrate your DNS to Route53 using zone (lie import and leverage Route53 DNS failover to failover to the S3 hosted website. Hybrid environment Create an AMI which can be used of launch web serfers in EC2 Create an Auto Scaling group which uses the * AMI to scale the web tier based on incoming traffic Leverage Elastic Load Balancing to balance traffic between on-premises web servers and those hosted in AWS. You are building a system to distribute confidential training videos to employees. Using CloudFront, what method could be used to serve content that is stored in S3, but not publically accessible from S3 directly? Create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket to that OAI. Add the CloudFront account security group \u201camazon-cf/amazon-cf-sg\u201d to the appropriate S3 bucket policy. Create an Identity and Access Management (IAM) User for CloudFront and grant access to the objects in your S3 bucket to that IAM User. Create a S3 bucket policy that lists the CloudFront distribution ID as the Principal and the target bucket as the Amazon Resource Name (ARN). A media production company wants to deliver high-definition raw video for preproduction and dubbing to customer all around the world. They would like to use Amazon CloudFront for their scenario, and they require the ability to limit downloads per customer and video file to a configurable number. A CloudFront download distribution with TTL=0 was already setup to make sure all client HTTP requests hit an authentication backend on Amazon Elastic Compute Cloud (EC2)/Amazon RDS first, which is responsible for restricting the number of downloads. Content is stored in S3 and configured to be accessible only via CloudFront. What else needs to be done to achieve an architecture that meets the requirements? Choose 2 answers [PROFESSIONAL] Enable URL parameter forwarding, let the authentication backend count the number of downloads per customer in RDS, and return the content S3 URL unless the download limit is reached. Enable CloudFront logging into an S3 bucket, leverage EMR to analyze CloudFront logs to determine the number of downloads per customer, and return the content S3 URL unless the download limit is reached. (CloudFront logs are logged periodically and EMR not being real time, hence not suitable) Enable URL parameter forwarding, let the authentication backend count the number of downloads per customer in RDS, and invalidate the CloudFront distribution as soon as the download limit is reached. (Distribution are not invalidated but Objects) Enable CloudFront logging into the S3 bucket, let the authentication backend determine the number of downloads per customer by parsing those logs, and return the content S3 URL unless the download limit is reached. (CloudFront logs are logged periodically and EMR not being real time, hence not suitable) Configure a list of trusted signers, let the authentication backend count the number of download requests per customer in RDS, and return a dynamically signed URL unless the download limit is reached. Your customer is implementing a video on-demand streaming platform on AWS. The requirements are to support for multiple devices such as iOS, Android, and PC as client devices, using a standard client player, using streaming technology (not download) and scalable architecture with cost effectiveness [PROFESSIONAL] Store the video contents to Amazon Simple Storage Service (S3) as an origin server. Configure the Amazon CloudFront distribution with a streaming option to stream the video contents Store the video contents to Amazon S3 as an origin server. Configure the Amazon CloudFront distribution with a download option to stream the video contents (Refer link) Launch a streaming server on Amazon Elastic Compute Cloud (EC2) (for example, Adobe Media Server), and store the video contents as an origin server. Configure the Amazon CloudFront distribution with a download option to stream the video contents Launch a streaming server on Amazon Elastic Compute Cloud (EC2) (for example, Adobe Media Server), and store the video contents as an origin server. Launch and configure the required amount of streaming servers on Amazon EC2 as an edge server to stream the video contents You are an architect for a news -sharing mobile application. Anywhere in the world, your users can see local news on of topics they choose. They can post pictures and videos from inside the application. Since the application is being used on a mobile phone, connection stability is required for uploading content, and delivery should be quick. Content is accessed a lot in the first minutes after it has been posted, but is quickly replaced by new content before disappearing. The local nature of the news means that 90 percent of the uploaded content is then read locally (less than a hundred kilometers from where it was posted). What solution will optimize the user experience when users upload and view content (by minimizing page load times and minimizing upload times)? [PROFESSIONAL] Upload and store the content in a central Amazon Simple Storage Service (S3) bucket, and use an Amazon Cloud Front Distribution for content delivery. Upload and store the content in an Amazon Simple Storage Service (S3) bucket in the region closest to the user, and use multiple Amazon Cloud Front distributions for content delivery. Upload the content to an Amazon Elastic Compute Cloud (EC2) instance in the region closest to the user, send the content to a central Amazon Simple Storage Service (S3) bucket, and use an Amazon Cloud Front distribution for content delivery. Use an Amazon Cloud Front distribution for uploading the content to a central Amazon Simple Storage Service (S3) bucket and for content delivery . To enable end-to-end HTTPS connections from the user\u2018s browser to the origin via CloudFront, which of the following options are valid? Choose 2 answers [PROFESSIONAL] Use self signed certificate in the origin and CloudFront default certificate in CloudFront. (Origin cannot be self signed) Use the CloudFront default certificate in both origin and CloudFront (CloudFront cert cannot be applied to origin) Use 3rd-party CA certificate in the origin and CloudFront default certificate in CloudFront Use 3rd-party CA certificate in both origin and CloudFront Use a self signed certificate in both the origin and CloudFront (Origin cannot be self signed) Your application consists of 10% writes and 90% reads. You currently service all requests through a Route53 Alias Record directed towards an AWS ELB, which sits in front of an EC2 Auto Scaling Group. Your system is getting very expensive when there are large traffic spikes during certain news events, during which many more people request to read similar data all at the same time. What is the simplest and cheapest way to reduce costs and scale with spikes like this? [PROFESSIONAL] Create an S3 bucket and asynchronously replicate common requests responses into S3 objects. When a request comes in for a precomputed response, redirect to AWS S3 Create another ELB and Auto Scaling Group layer mounted on top of the other system, adding a tier to the system. Serve most read requests out of the top layer Create a CloudFront Distribution and direct Route53 to the Distribution. Use the ELB as an Origin and specify Cache Behaviors to proxy cache requests, which can be served late. (CloudFront can server request from cache and multiple cache behavior can be defined based on rules for a given URL pattern based on file extensions, file names, or any portion of a URL. Each cache behavior can include the CloudFront configuration values: origin server name, viewer connection protocol, minimum expiration period, query string parameters, cookies, and trusted signers for private content.) Create a Memcached cluster in AWS ElastiCache. Create cache logic to serve requests, which can be served late from the in-memory cache for increased performance. You are designing a service that aggregates clickstream data in batch and delivers reports to subscribers via email only once per week. Data is extremely spikey, geographically distributed, high-scale, and unpredictable. How should you design this system? Use a large RedShift cluster to perform the analysis, and a fleet of Lambdas to perform record inserts into the RedShift tables. Lambda will scale rapidly enough for the traffic spikes. Use a CloudFront distribution with access log delivery to S3. Clicks should be recorded as query string GETs to the distribution. Reports are built and sent by periodically running EMR jobs over the access logs in S3. (CloudFront is a Gigabit-Scale HTTP(S) global request distribution service and works fine with peaks higher than 10 Gbps or 15,000 RPS. It can handle scale, geo-spread, spikes, and unpredictability. Access Logs will contain the GET data and work just fine for batch analysis and email using EMR. Other streaming options are expensive as not required as the need is to batch analyze) Use API Gateway invoking Lambdas which PutRecords into Kinesis, and EMR running Spark performing GetRecords on Kinesis to scale with spikes. Spark on EMR outputs the analysis to S3, which are sent out via email. Use AWS Elasticsearch service and EC2 Auto Scaling groups. The Autoscaling groups scale based on click throughput and stream into the Elasticsearch domain, which is also scalable. Use Kibana to generate reports periodicall Your website is serving on-demand training videos to your workforce. Videos are uploaded monthly in high resolution MP4 format. Your workforce is distributed globally often on the move and using company-provided tablets that require the HTTP Live Streaming (HLS) protocol to watch a video. Your company has no video transcoding expertise and it required you might need to pay for a consultant. How do you implement the most cost-efficient architecture without compromising high availability and quality of video delivery? [PROFESSIONAL] Elastic Transcoder to transcode original high-resolution MP4 videos to HLS. S3 to host videos with lifecycle Management to archive original flies to Glacier after a few days. CloudFront to serve HLS transcoded videos from S3 A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number or nodes depending on the length of the queue S3 to host videos with Lifecycle Management to archive all files to Glacier after a few days CloudFront to serve HLS transcoding videos from Glacier Elastic Transcoder to transcode original high-resolution MP4 videos to HLS EBS volumes to host videos and EBS snapshots to incrementally backup original rues after a few days. CloudFront to serve HLS transcoded videos from EC2. A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number of nodes depending on the length of the queue. EBS volumes to host videos and EBS snapshots to incrementally backup original files after a few days. CloudFront to serve HLS transcoded videos from EC2","title":"L13 AWS CloudFront"},{"location":"chap3/13cloudfront/#l13-aws-cloudfront","text":"","title":"L13 AWS CloudFront"},{"location":"chap3/13cloudfront/#1-cloudfront","text":"CloudFront is a fully managed, fast content delivery network (CDN) service that speeds up the distribution of static, dynamic web, or streaming content to end-users . CloudFront delivers the content through a worldwide network of data centers called edge locations or Point of Presence (POP) . CloudFront securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds. CloudFront speeds up the distribution of the content by routing each user request to the edge location that can best serve the content thus providing the lowest latency (time delay). CloudFront dramatically reduces the number of network hops that users\u2019 requests must pass through, which helps improves performance, provide lower latency and higher data transfer rates. CloudFront is a good choice for the distribution of frequently accessed static content that benefits from edge delivery \u2013 like popular website images, videos, media files, or software downloads","title":"1 CloudFront"},{"location":"chap3/13cloudfront/#2-cloudfront-benefits","text":"CloudFront eliminates the expense and complexity of operating a network of cache server s in multiple sites across the internet and eliminates the need to over-provision capacity in order to serve potential spikes in traffic. CloudFront also provides increased reliability and availability because copies of objects are held in multiple edge locations around the world. CloudFront keeps persistent connections with the origin servers so that those files can be fetched from the origin servers as quickly as possible. CloudFront also uses techniques such as collapsing simultaneous viewer requests at an edge location for the same file into a single request to the origin server reducing the load on the origin . CloudFront offers the most advanced security capabilities, including field-level encryption and HTTPS support. CloudFront seamlessly integrates with AWS Shield, AWS Web Application Firewall, and Route 53 to protect against multiple types of attacks including network and application layer DDoS attacks .","title":"2 CloudFront Benefits"},{"location":"chap3/13cloudfront/#3-configuration-content-delivery","text":"","title":"3 Configuration &amp; Content Delivery"},{"location":"chap3/13cloudfront/#3-1-configuration","text":"Origin servers need to be configured to get the files for distribution . An origin server stores the original, definitive version of the objects and can be an AWS hosted service for e.g. S3, EC2, or an on-premise server Files or objects can be added/uploaded to the Origin servers with public read permissions or permissions restricted to Origin Access Identity (OAI). Create a CloudFront distribution, which tells CloudFront which origin servers to get the files from when users request the files CloudFront sends the distribution configuration to all the edge locations The website can be used with the CloudFront provided domain name or a custom alternate domain name An origin server can be configured to limit access protocols, caching behavior, add headers to the files to add TTL or the expiration time","title":"3-1 Configuration"},{"location":"chap3/13cloudfront/#3-2-content-delivery-to-users","text":"When a user accesses the website, file, or object \u2013 the DNS routes the request to the CloudFront edge location that can best serve the user\u2019s request with the lowest latency CloudFront returns the object immediately if the requested object is present in the cache at the Edge location If the requested object does not exist in the cache at the edge location, CloudFront requests the object from the origin server and returns it to the user as soon as it starts receiving it When the object reaches its expiration time, for any new request CloudFront checks with the Origin server for any latest versions, if it has the latest it uses the same object. If the Origin server has the latest version the same is retrieved, served to the user, and cached as well","title":"3-2 Content delivery to Users"},{"location":"chap3/13cloudfront/#4-delivery-methods","text":"","title":"4 Delivery Methods"},{"location":"chap3/13cloudfront/#4-1-web-distributions","text":"supports both s tatic and dynamic content for e.g. HTML, CSS, js, images, etc using HTTP or HTTPS . supports multimedia content on-demand using progressive download and Apple HTTP Live Streaming (HLS) . supports a live event, such as a meeting, conference , or concert, in real-time. For live streaming, distribution can be created automatically using an AWS CloudFormation stack. origin servers can be either an S3 bucket or an HTTP server, for e.g., a web server or an AWS ELB, etc.","title":"4-1 Web distributions"},{"location":"chap3/13cloudfront/#4-2-cloudfront-origins","text":"Each origin is either an S3 bucket, a MediaStore container, a MediaPackage channel, or a custom origin like EC2 instance or an HTTP server For the S3 bucket, use the bucket URL or the static website endpoint URL, and the files either need to be publicly readable or secured using OAI. Origin restrict access, for S3 only , can be configured using Origin Access Identity to prevent direct access to the S3 objects For the HTTP server as the origin, the domain name of the resource needs to be mapped and files must be publicly readable . Distribution can have multiple origins for each bucket with one or more cache behaviors that route requests to each origin . Path pattern in a cache behavior determines which requests are routed to the origin (S3 bucket) that is associated with that cache behavior","title":"4-2 CloudFront Origins"},{"location":"chap3/13cloudfront/#5-cache-behavior-settings","text":"","title":"5 Cache Behavior Settings"},{"location":"chap3/13cloudfront/#5-1-path-patterns","text":"Path Patterns help define which path the Cache behavior would apply to. A default (*) pattern is created and multiple cache distributions can be added with patterns to take priority over the default path","title":"5-1 Path Patterns"},{"location":"chap3/13cloudfront/#5-2-viewer-protocol-policy","text":"Viewer Protocol policy can be configured to define the allowed access protocol. Between CloudFront & Viewers, cache distribution can be configured to either allow HTTPS only \u2013 supports HTTPS only HTTP and HTTPS \u2013 supports both HTTP redirected to HTTPS \u2013 HTTP is automatically redirected to HTTPS","title":"5-2 Viewer Protocol Policy"},{"location":"chap3/13cloudfront/#5-3-https-connection","text":"Between CloudFront & Origin, cache distribution can be configured to require that CloudFront fetches objects from the origin by using HTTPS or CloudFront uses the protocol that the viewer used to request the objects . For S3 as origin , For website, the protocol has to be HTTP as HTTPS is not supported For S3 bucket, the default Origin protocol policy is Match Viewer and cannot be changed. So When CloudFront is configured to require HTTPS between the viewer and CloudFront, it automatically uses HTTPS to communicate with S3 . CloudFront can also be configured to work with HTTPS for alternate domain names by using:- Serving HTTPS Requests Using Dedicated IP Addresses CloudFront associates the alternate domain name with a dedicated IP address, and the certificate is associated with the IP address. when a request is received from a DNS server for the IP address, CloudFront uses the IP address to identify the distribution and the SSL/TLS certificate to return to the viewer This method works for every HTTPS request, regardless of the browser or other viewer that the user is using. * Additional monthly charge (of about $600/month) is incurred for using a dedicated IP address Serving HTTPS Requests Using SNI SNI custom SSL relies on the SNI extension of the TLS protocol, which allows multiple domains to be served over the same IP address by including the hostname, viewers are trying to connect to With SNI method, CloudFront associates an IP address with the alternate domain name, but the IP address is not dedicated CloudFront can\u2019t determine, based on the IP address, which domain the request is for as the IP address is not dedicated Browsers that support SNI automatically gets the domain name from the request URL & adds it to a new field in the request header. When CloudFront receives an HTTPS request from a browser that supports SNI, it finds the domain name in the request header and responds to the request with the applicable SSL/TLS certificate. Viewer and CloudFront perform SSL negotiation, and CloudFront returns the requested content to the viewer. Older browsers do not support it SNI Custom SSL is available at no additional cost beyond standard CloudFront data transfer and request fees For End-to-End HTTPS connections certificate needs to be applied both between the Viewers and CloudFront & CloudFront and Origin, with the following requirements HTTPS between viewers and CloudFront Certificate that was issued by a trusted certificate authority (CA) such as Comodo, DigiCert, or Symantec; Certificate provided by AWS Certificate Manager (ACM); Self-signed certificate. HTTPS between CloudFront and a custom origin If the origin is not an ELB load balancer, the certificate must be issued by a trusted CA such as Comodo, DigiCert, or Symantec. For ELB load balancer, certificate provided by ACM can be used","title":"5-3 HTTPS Connection"},{"location":"chap3/13cloudfront/#6-allowed-http-methods","text":"CloudFront supports GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE to get, add, update, and delete objects, and to get object headers. GET, HEAD methods to use to get objects, object headers GET, HEAD, OPTIONS methods to use to get objects, object headers or retrieve a list of the options supported from the origin GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE operations can also be performed for e.g. submitting data from a web form, which are directly proxied back to the Origin server CloudFront only caches responses to GET and HEAD requests and, optionally, OPTIONS requests. CloudFront does not cache responses to PUT, POST, PATCH, DELETE request methods and these requests are directed to the origin . PUT, POST HTTP methods also help for accelerated content uploads, as these operations will be sent to the origin e.g. S3 via the CloudFront edge location, improving efficiency, reducing latency, and allowing the application to benefit from the monitored, persistent connections that CloudFront maintains from the edge locations to the origin servers.","title":"6 Allowed HTTP methods"},{"location":"chap3/13cloudfront/#7-field-level-encryption-config","text":"CloudFront can enforce secure end-to-end connections to origin servers by using HTTPS . Field-level encryption adds an additional layer of security that helps protect specific data throughout system processing so that only certain applications can see it.","title":"7 Field Level Encryption Config"},{"location":"chap3/13cloudfront/#8-cloudfront-edge-caches","text":"Control the cache max-age To increase the cache hit ratio, the origin can be configured to add a Cache-Control: max-age directive to the objects. Longer the interval less frequently it would be retrieved from the origin Caching Based on Query String Parameters CloudFront can be configured to cache based on the query parameters None (Improves Caching) \u2013 if the origin returns the same version of an object regardless of the values of query string parameters. Forward all, cache based on whitelist \u2013 if the origin server returns different versions of the objects based on one or more query string parameters. Then specify the parameters that you want CloudFront to use as a basis for caching in the Query String Whitelist field. Forward all, cache based on all \u2013 if the origin server returns different versions of the objects for all query string parameters. Caching performance can be improved by Configure CloudFront to forward only the query strings for which the origin will return unique objects. Using the same case for the parameters\u2019 values for e.g. parameter value A or a, CloudFront would cache the same request twice even if the response or object returned is identical Using the same parameter order for e.g. for request a=x&b=y and b=y&a=x, CloudFront would cache the same request twice even though the response or object returned is identical Caching Based on Cookie Values CloudFront can be configured to cache based on cookie values. By default, it doesn\u2019t consider cookies while caching on edge locations Caching performance can be improved by Configure CloudFront to forward only specified cookies instead of forwarding all cookies for e.g. if the request has 2 cookies with 3 possible values, CloudFront would cache all possible combinations even if the response takes into account a single cookie Cookie names and values are both case sensitive so better to stick with the same case Create separate cache behaviors for static and dynamic content, and configure CloudFront to forward cookies to the origin only for dynamic content for e.g. for CSS files, the cookies do not make sense as the object does not change with the cookie value If possible, create separate cache behaviors for dynamic content for which cookie values are unique for each user (such as a user ID) and dynamic content that varies based on a smaller number of unique values reducing the number of combinations Caching Based on Request Headers CloudFront can be configured to cache based on request headers By default, CloudFront doesn\u2019t consider headers when caching the objects in edge locations. CloudFront configured to cache based on request headers, does not change the headers that CloudFront forwards, only whether CloudFront caches objects based on the header values. Caching performance can be improved by Configure CloudFront to forward and cache based only on specified headers instead of forwarding and caching based on all headers. Try to avoid caching based on request headers that have large numbers of unique values. CloudFront is configured to forward all headers to the origin, CloudFront doesn\u2019t cache the objects associated with this cache behavior. Instead, it sends every request to the origin CloudFront caches based on header values, it doesn\u2019t consider the case of the header name but considers the case of the header value","title":"8 CloudFront Edge Caches"},{"location":"chap3/13cloudfront/#9-object-caching-expiration","text":"Object expiration determines how long the objects stay in a CloudFront cache before it fetches it again from Origin Low expiration time helps serve content that changes frequently and high expiration time helps improve performance and reduce the origin load. By default, each object automatically expires after 24 hours After expiration time, CloudFront checks if it still has the latest version If the cache already has the latest version, the origin returns a 304 status code (Not Modified). If the CloudFront cache does not have the latest version, the origin returns a 200 status code (OK), and the latest version of the object If an object in an edge location isn\u2019t frequently requested, CloudFront might evict the object, remove the object before its expiration date, to make room for objects that have been requested more recently. For Web distributions, the default behavior can be changed by for the entire path pattern, cache behavior can be configured by the setting Minimum TTL, Maximum TTL, and Default TTL values for individual objects, the origin can be configured to add a Cache-Control max-age or Cache-Control s-maxage directive, or an Expires header field to the object . AWS recommends using Cache-Control max-age directive over Expires header to control object caching behavior CloudFront uses only the value of Cache-Control max-age , if both the Cache-Control max-age directive and Expires header is specified HTTP Cache-Control or Pragma header fields in a GET request from a viewer can\u2019t be used to force CloudFront to go back to the origin server for the object By default, when the origin returns an HTTP 4xx or 5xx status code, CloudFront caches these error responses for five minutes and then submits the next request for the object to the origin to see whether the requested object is available and the problem has been resolved","title":"9 Object Caching &amp; Expiration"},{"location":"chap3/13cloudfront/#10-restrict-viewer-access","text":"","title":"10 Restrict Viewer Access"},{"location":"chap3/13cloudfront/#10-1-serving-private-content","text":"To securely serve private content using CloudFront Require the users to access the private content by using special CloudFront signed URLs or signed cookies with following restrictions an end date and time, after which the URL is no longer valid start date time, when the URL becomes valid ip address or range of addresses to access the URLs Require that users access the S3 content only using CloudFront URLs, not S3 URLs. Requiring CloudFront URLs isn\u2019t required, but recommended to prevent users from bypassing the restrictions specified in signed URLs or signed cookies. Signed URLs or Signed Cookies can used with CloudFront using HTTP server as an origin. It requires the content to be publicly accessible and care should be taken to not share the direct URL of the content Restriction for Origin can be applied by For S3, using Origin Access Identity to grant only CloudFront access using Bucket policies or Object ACL, to the content and removing any other access permissions For HTTP server, custom header can be added by CloudFront which can be used at Origin to verify the request has come from CloudFront Trusted Signer To create signed URLs or signed cookies, at least one AWS account (trusted signer) is needed that has an active CloudFront key pair Once AWS account is added as trusted signer to the distribution, CloudFront starts to require that users use signed URLs or signed cookies to access the objects . Private key from the trusted signer\u2019s key pair to sign a portion of the URL or the cookie. When someone requests a restricted object, CloudFront compares the signed portion of the URL or cookie with the unsigned portion to verify that the URL or cookie hasn\u2019t been tampered with. CloudFront also validates the URL or cookie is valid for e.g, that the expiration date and time hasn\u2019t passed. Each Trusted signer AWS accounts used to create CloudFront signed URLs or signed cookies must have its own active CloudFront key pair, which should be frequently rotated A maximum of 5 trusted signers can be assigned for each cache behavior or RTMP distribution","title":"10-1 Serving Private Content"},{"location":"chap3/13cloudfront/#10-2-signed-urls-vs-signed-cookies","text":"CloudFront signed URLs and signed cookies help to secure the content and provide control to decide who can access the content. Use signed URLs in the following cases: to restrict access to individual files, for e.g., an installation download for the application. users using a client, for e.g. a custom HTTP client, that doesn\u2019t support cookies Use signed cookies in the following cases : provide access to multiple restricted files, for e.g., all of the video files in HLS format or all of the files in the subscribers\u2019 area of a website. don\u2019t want to change the current URLs. Signed URLs take precedence over signed cookies, if both signed URLs and signed cookies are used to control access to the same files and a viewer uses a signed URL to request a file, CloudFront determines whether to return the file to the viewer based only on the signed URL.","title":"10-2 Signed URLs vs Signed Cookies"},{"location":"chap3/13cloudfront/#10-3-canned-policy-vs-custom-policy","text":"Canned policy or a custom policy is a policy statement, used by the Signed URLs, helps define the restrictions for e.g. expiration date and time CloudFront validates the expiration time at the start of the event. If user is downloading a large object, and the url expires the download would still continue and the same for RTMP distribution. However, if the user is using range GET requests, or while streaming video skips to another position which might trigger an other event, the request would fail.","title":"10-3 Canned Policy vs Custom Policy"},{"location":"chap3/13cloudfront/#11-serving-compressed-files","text":"CloudFront can be configured to automatically compress files of certain types and serve the compressed files when viewer requests include Accept-Encodin g in the request header Compressing content, downloads are faster because the files are smaller as well as less expensive as the cost of CloudFront data transfer is based on the total amount of data served . CloudFront can compress objects using the Gzip and Brotli compression formats . If serving from a custom origin, it can be used to configure to compress files with or without CloudFront compression compress file types that CloudFront doesn\u2019t compress. If the origin returns a compressed file, CloudFront detects compression by the Content-Encoding header value and doesn\u2019t compress the file again. CloudFront serves content using compression as below CloudFront distribution is created and configured to compress content. A viewer requests a compressed file by adding the Accept-Encoding header with includes gzip, br, or both to the request . At the edge location, CloudFront checks the cache for a compressed version of the file that is referenced in the request. If the compressed file is already in the cache, CloudFront returns the file to the viewer and skips the remaining steps. If the compressed file is not in the cache, CloudFront forwards the request to the origin server (S3 bucket or a custom origin) Even if CloudFront has an uncompressed version of the file in the cache, it still forwards a request to the origin. Origin server returns an uncompressed version of the requested file CloudFront determines whether the file is compressible: file must be of a type that CloudFront compresses. file size must be between 1,000 and 10,000,000 bytes. response must include a Content-Length header to determine the size within valid compression limits. If the Content-Length header is missing, CloudFront won\u2019t compress the file. value of the Content-Encoding header on the file must not be gzip i.e. the origin has already compressed the file. the response should have a body response HTTP status code should be 200, 403, or 404 If the file is compressible, CloudFront compresses it, returns the compressed file to the viewer, and adds it to the cache. The viewer uncompresses the file.","title":"11 Serving Compressed Files"},{"location":"chap3/13cloudfront/#12-distribution-details","text":"","title":"12 Distribution Details"},{"location":"chap3/13cloudfront/#12-1-price-class","text":"CloudFront has edge locations all over the world and as cost for each edge location varies and the price charged for serving the requests also varies CloudFront edge locations are grouped into geographic regions, and regions have been grouped into price classes Price Class \u2013 includes all the regions Price Class 200 \u2013 Includes All regions except South America and Australia and New Zealand. Price Class 100 \u2013 A third price class includes only the least-expensive regions (North America and Europe regions) Price class can be selected to lower the cost but this would come only at the expense of performance (higher latency), as CloudFront would serve requests only from the selected price class edge locations CloudFront may, sometimes, service requests from a region not included within the price class, however, you would be charged the rate for the least-expensive region in your selected price class","title":"12-1 Price Class"},{"location":"chap3/13cloudfront/#12-2-waf-web-acl","text":"AWS WAF can be used to allow or block requests based on specified criteria, choose the web ACL to associate with this distribution.","title":"12-2 WAF Web ACL"},{"location":"chap3/13cloudfront/#12-3-alternate-domain-names-cnames","text":"CloudFront by default assigns a domain name for the distribution for e.g. d111111abcdef8.cloudfront.net An alternate domain name, also known as a CNAME, can be used to use own custom domain name for links to objects CloudFront supports * wildcard at the beginning of a domain name instead of specifying subdomains individually. However, a wildcard cannot replace part of a subdomain name for e.g. *domain.example.com , or cannot replace a subdomain in the middle of a domain name for e.g. subdomain.*.example.com.","title":"12-3 Alternate Domain Names (CNAMEs)"},{"location":"chap3/13cloudfront/#12-4-distribution-state","text":"Distribution state indicates whether you want the distribution to be enabled or disabled once it\u2019s deployed.","title":"12-4 Distribution State"},{"location":"chap3/13cloudfront/#13-geo-restriction-geoblocking","text":"Geo restriction can help allow or prevent users in selected countries from accessing the content, CloudFront distribution can be configured either to allow users in whitelist of specified countries to access the content or to deny users in a blacklist of specified countries to access the content Geo restriction can be used to restrict access to all of the files that are associated with distribution and to restrict access at the country level CloudFront responds to a request from a viewer in a restricted country with an HTTP status code 403 (Forbidden) Use a third-party geolocation service, if access is to be restricted to a subset of the files that are associated with a distribution or to restrict access at a finer granularity than the country level.","title":"13 Geo-Restriction \u2013 Geoblocking"},{"location":"chap3/13cloudfront/#14-cloudfront-with-s3","text":"CloudFront can be used to distribute the content from an S3 bucket For an RTMP distribution, S3 bucket is the only supported origin and custom origins cannot be used Using CloudFront over S3 has the following benefits can be more cost effective if the objects are frequently accessed as at higher usage, the price for CloudFront data transfer is much lower than the price for S3 data transfer. downloads are faster with CloudFront than with S3 alone because the objects are stored closer to the users When using S3 as the origin for a distribution and the bucket is moved to a different region, CloudFront can take up to an hour to update its records to include the change of region when both of the following are true: Origin Access Identity (OAI) is used to restrict access to the bucket Bucket is moved to an S3 region that requires Signature Version 4 for authentication","title":"14 CloudFront with S3"},{"location":"chap3/13cloudfront/#15-origin-access-identity","text":"With S3 as origin, objects in S3 must be granted public read permissions and hence the objects are accessible from both S3 as well as CloudFront Even though, CloudFront does not expose the underlying S3 url, it can be known to the user if shared directly or used by applications For using CloudFront signed URLs or signed cookies to provide access to the objects, it would be necessary to prevent users from having direct access to the S3 objects Users accessing S3 objects directly would bypass the controls provided by CloudFront signed URLs or signed cookies, for e.g., control over the date time that a user can no longer access the content and the IP addresses can be used to access content CloudFront access logs are less useful because they\u2019re incomplete. Origin Access Identity (OAI) can be used to prevent users from directly accessing objects from S3 Origin access identity, which is a special CloudFront user, can be created and associated with the distribution. S3 bucket/object permissions needs to be configured to only provide access to the Origin Access Identity When users access the object from CloudFront, it uses the OAI to fetch the content on users behalf, while direct access to the S3 objects is restricted","title":"15 Origin Access Identity"},{"location":"chap3/13cloudfront/#16-working-with-objects","text":"CloudFront can be configured to include custom headers or modify existing headers whenever it forwards a request to the origin, to validate the user is not accessing the origin directly, bypassing CDN identify the CDN from which the request was forwarded, if more than one CloudFront distribution is configured to use the same origin if users use viewers that don\u2019t support CORS, configure CloudFront to forward the Origin header to the origin. That will cause the origin to return the Access-Control-Allow-Origin header for every request","title":"16 Working with Objects"},{"location":"chap3/13cloudfront/#16-1-adding-updating-objects","text":"Objects just need to be added to the Origin and CloudFront would start distributing them when accessed Objects served by CloudFront the Origin, can be updated either by Overwriting the Original object Create a different version and updating the links exposed to the user For updating objects, its recommended to use versioning for e.g. have files or the entire folders with versions, so the the links can be changed when the objects are updated forcing a refresh With versioning, there is no time wait for an object to expire before CloudFront begins to serve a new version of it there is no difference in consistency in the object served from the edge no cost involved to pay for object invalidation.","title":"16-1 Adding &amp; Updating Objects"},{"location":"chap3/13cloudfront/#16-2-removinginvalidating-objects","text":"Objects, by default, would be removed upon expiry (TTL) and the latest object would be fetched from the Origin Objects can also be removed from the edge cache before it expires File or Object Versioning to serve a different version of the object that has a different name. Invalidate the object from edge caches. For the next request, CloudFront returns to the Origin to fetch the object Object or File Versioning is recommended over Invalidating objects if the objects need to be updated frequently. enables to control which object a request returns even when the user has a version cached either locally or behind a corporate caching proxy. makes it easier to analyze the results of object changes as CloudFront access logs include the names of the objects provides a way to serve different versions to different users. simplifies rolling forward & back between object revisions. is less expensive, as no charges for invalidating objects. for e.g. change header-v1.jpg to header-v2.jpg Invalidating objects from the cache objects in the cache can be invalidated explicitly before they expire to force a refresh allows to invalidate selected objects allows to invalidate multiple objects for e.g. objects in a directory or all of the objects whose names begin with the same characters, you can include the * wildcard at the end of the invalidation path. the user might continue to see the old version until it expires from those caches. A specified number of invalidation paths can be submitted each month for free. Any invalidation requests more than the allotted no. per month, a fee is charged for each submitted invalidation path The First 1,000 invalidation paths requests submitted per month are free; charges apply for each invalidation path over 1,000 in a month. Invalidation path can be for a single object for e.g. /js/ab.js or for multiple objects for e.g. /js/* and is counted as a single request even if the * wildcard request may invalidate thousands of objects.","title":"16-2 Removing/Invalidating Objects"},{"location":"chap3/13cloudfront/#16-3-partial-requests-range-gets","text":"Partial requests using Range headers in a GET request helps to download the object in smaller units, improving the efficiency of partial downloads and the recovery from partially failed transfers. For a partial GET range request, CloudFront checks the cache in the edge location for the requested range or the entire object and if exists, serves it immediately if the requested range does not exist, it forwards the request to the origin and may request a larger range than the client requested to optimize performance if the origin supports range header, it returns the requested object range and CloudFront returns the same to the viewer if the origin does not support range header, it returns the complete object and CloudFront serves the entire object and caches it for future. CloudFront uses the cached entire object to serve any future range GET header requests","title":"16-3 Partial Requests (Range GETs)"},{"location":"chap3/13cloudfront/#17-cloudfront-security","text":"CloudFront provides Encryption in Transit and can be configured to require that viewers use HTTPS to request the files so that connections are encrypted when CloudFront communicates with viewers . CloudFront provides Encryption at Rest uses SSDs which are encrypted for edge location points of presence (POPs), and encrypted EBS volumes for Regional Edge Caches (RECs). Function code and configuration are always stored in an encrypted format on the encrypted SSDs on the edge location POPs, and in other storage locations used by CloudFront. Restricting access to content Configure HTTPS connections Use signed URLs or cookies to restrict access for selected users Restrict access to content in S3 buckets using origin access identity \u2013 OAI, to prevent users from using the direct URL of the file. Set up field-level encryption for specific content fields Use AWS WAF web ACLs to create a web access control list (web ACL) to restrict access to your content. Use geo-restriction, also known as geoblocking, to prevent users in specific geographic locations from accessing content served through a CloudFront distribution.","title":"17 CloudFront Security"},{"location":"chap3/13cloudfront/#18-access-logs","text":"CloudFront can be configured to create log files that contain detailed information about every user request that CloudFront receives. Access logs are available for both web and RTMP distributions. With logging enabled, an S3 bucket can be specified where CloudFront would save the files CloudFront delivers access logs for a distribution periodically, up to several times an hour CloudFront usually delivers the log file for that time period to the S3 bucket within an hour of the events that appear in the log. Note, however, that some or all log file entries for a time period can sometimes be delayed by up to 24 hours","title":"18 Access Logs"},{"location":"chap3/13cloudfront/#19-cloudfront-cost","text":"CloudFront charges are based on actual usage of the service in four areas: Data Transfer Out to Internet charges are applied for the volume of data transferred out of the CloudFront edge locations, measured in GB Data transfer out from AWS origin (e.g., S3, EC2, etc.) to CloudFront are no longer charged. This applies to data transfer from all AWS regions to all global CloudFront edge locations HTTP/HTTPS Requests number of HTTP/HTTPS requests made for the content Invalidation Requests per path in the invalidation request A path listed in the invalidation request represents the URL (or multiple URLs if the path contains a wildcard character) of the object you want to invalidate from the CloudFront cache Dedicated IP Custom SSL certificates associated with a CloudFront distribution $600 per month for each custom SSL certificate associated with one or more CloudFront distributions using the Dedicated IP version of custom SSL certificate support, pro-rated by the hour Your company Is moving towards tracking web page users with a small tracking Image loaded on each page Currently you are serving this image out of US-East, but are starting to get concerned about the time It takes to load the image for users on the west coast. What are the two best ways to speed up serving this image? Choose 2 answers Use Route 53\u2019s Latency Based Routing and serve the image out of US-West-2 as well as US-East-1 Serve the image out through CloudFront Serve the image out of S3 so that it isn\u2019t being served oft of your web application tier Use EBS PIOPs to serve the image faster out of your EC2 instances You deployed your company website using Elastic Beanstalk and you enabled log file rotation to S3. An Elastic Map Reduce job is periodically analyzing the logs on S3 to build a usage dashboard that you share with your CIO. You recently improved overall performance of the website using Cloud Front for dynamic content delivery and your website as the origin. After this architectural change, the usage dashboard shows that the traffic on your website dropped by an order of magnitude. How do you fix your usage dashboard\u2019? [PROFESSIONAL] Enable CloudFront to deliver access logs to S3 and use them as input of the Elastic Map Reduce job Turn on Cloud Trail and use trail log tiles on S3 as input of the Elastic Map Reduce job Change your log collection process to use Cloud Watch ELB metrics as input of the Elastic Map Reduce job Use Elastic Beanstalk \u201cRebuild Environment\u201d option to update log delivery to the Elastic Map Reduce job. Use Elastic Beanstalk \u2018Restart App server(s)\u201d option to update log delivery to the Elastic Map Reduce job. An AWS customer runs a public blogging website. The site users upload two million blog entries a month. The average blog entry size is 200 KB. The access rate to blog entries drops to negligible 6 months after publication and users rarely access a blog entry 1 year after publication. Additionally, blog entries have a high update rate during the first 3 months following publication; this drops to no updates after 6 months. The customer wants to use CloudFront to improve his user\u2019s load times. Which of the following recommendations would you make to the customer? [PROFESSIONAL] Duplicate entries into two different buckets and create two separate CloudFront distributions where S3 access is restricted only to Cloud Front identity Create a CloudFront distribution with \u201cUS & Europe\u201d price class for US/Europe users and a different CloudFront distribution with All Edge Locations for the remaining users. Create a CloudFront distribution with S3 access restricted only to the CloudFront identity and partition the blog entry\u2019s location in S3 according to the month it was uploaded to be used with CloudFront behaviors Create a CloudFront distribution with Restrict Viewer Access Forward Query string set to true and minimum TTL of 0. Your company has on-premises multi-tier PHP web application, which recently experienced downtime due to a large burst in web traffic due to a company announcement. Over the coming days, you are expecting similar announcements to drive similar unpredictable bursts, and are looking to find ways to quickly improve your infrastructures ability to handle unexpected increases in traffic. The application currently consists of 2 tiers a web tier, which consists of a load balancer, and several Linux Apache web servers as well as a database tier which hosts a Linux server hosting a MySQL database. Which scenario below will provide full site functionality, while helping to improve the ability of your application in the short timeframe required? [PROFESSIONAL] Offload traffic from on-premises environment Setup a CloudFront distribution and configure CloudFront to cache objects from a custom origin Choose to customize your object cache behavior, and select a TTL that objects should exist in cache. Migrate to AWS Use VM Import/Export to quickly convert an on-premises web server to an AMI create an Auto Scaling group, which uses the imported AMI to scale the web tier based on incoming traffic Create an RDS read replica and setup replication between the RDS instance and on-premises MySQL server to migrate the database. Failover environment: Create an S3 bucket and configure it tor website hosting Migrate your DNS to Route53 using zone (lie import and leverage Route53 DNS failover to failover to the S3 hosted website. Hybrid environment Create an AMI which can be used of launch web serfers in EC2 Create an Auto Scaling group which uses the * AMI to scale the web tier based on incoming traffic Leverage Elastic Load Balancing to balance traffic between on-premises web servers and those hosted in AWS. You are building a system to distribute confidential training videos to employees. Using CloudFront, what method could be used to serve content that is stored in S3, but not publically accessible from S3 directly? Create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket to that OAI. Add the CloudFront account security group \u201camazon-cf/amazon-cf-sg\u201d to the appropriate S3 bucket policy. Create an Identity and Access Management (IAM) User for CloudFront and grant access to the objects in your S3 bucket to that IAM User. Create a S3 bucket policy that lists the CloudFront distribution ID as the Principal and the target bucket as the Amazon Resource Name (ARN). A media production company wants to deliver high-definition raw video for preproduction and dubbing to customer all around the world. They would like to use Amazon CloudFront for their scenario, and they require the ability to limit downloads per customer and video file to a configurable number. A CloudFront download distribution with TTL=0 was already setup to make sure all client HTTP requests hit an authentication backend on Amazon Elastic Compute Cloud (EC2)/Amazon RDS first, which is responsible for restricting the number of downloads. Content is stored in S3 and configured to be accessible only via CloudFront. What else needs to be done to achieve an architecture that meets the requirements? Choose 2 answers [PROFESSIONAL] Enable URL parameter forwarding, let the authentication backend count the number of downloads per customer in RDS, and return the content S3 URL unless the download limit is reached. Enable CloudFront logging into an S3 bucket, leverage EMR to analyze CloudFront logs to determine the number of downloads per customer, and return the content S3 URL unless the download limit is reached. (CloudFront logs are logged periodically and EMR not being real time, hence not suitable) Enable URL parameter forwarding, let the authentication backend count the number of downloads per customer in RDS, and invalidate the CloudFront distribution as soon as the download limit is reached. (Distribution are not invalidated but Objects) Enable CloudFront logging into the S3 bucket, let the authentication backend determine the number of downloads per customer by parsing those logs, and return the content S3 URL unless the download limit is reached. (CloudFront logs are logged periodically and EMR not being real time, hence not suitable) Configure a list of trusted signers, let the authentication backend count the number of download requests per customer in RDS, and return a dynamically signed URL unless the download limit is reached. Your customer is implementing a video on-demand streaming platform on AWS. The requirements are to support for multiple devices such as iOS, Android, and PC as client devices, using a standard client player, using streaming technology (not download) and scalable architecture with cost effectiveness [PROFESSIONAL] Store the video contents to Amazon Simple Storage Service (S3) as an origin server. Configure the Amazon CloudFront distribution with a streaming option to stream the video contents Store the video contents to Amazon S3 as an origin server. Configure the Amazon CloudFront distribution with a download option to stream the video contents (Refer link) Launch a streaming server on Amazon Elastic Compute Cloud (EC2) (for example, Adobe Media Server), and store the video contents as an origin server. Configure the Amazon CloudFront distribution with a download option to stream the video contents Launch a streaming server on Amazon Elastic Compute Cloud (EC2) (for example, Adobe Media Server), and store the video contents as an origin server. Launch and configure the required amount of streaming servers on Amazon EC2 as an edge server to stream the video contents You are an architect for a news -sharing mobile application. Anywhere in the world, your users can see local news on of topics they choose. They can post pictures and videos from inside the application. Since the application is being used on a mobile phone, connection stability is required for uploading content, and delivery should be quick. Content is accessed a lot in the first minutes after it has been posted, but is quickly replaced by new content before disappearing. The local nature of the news means that 90 percent of the uploaded content is then read locally (less than a hundred kilometers from where it was posted). What solution will optimize the user experience when users upload and view content (by minimizing page load times and minimizing upload times)? [PROFESSIONAL] Upload and store the content in a central Amazon Simple Storage Service (S3) bucket, and use an Amazon Cloud Front Distribution for content delivery. Upload and store the content in an Amazon Simple Storage Service (S3) bucket in the region closest to the user, and use multiple Amazon Cloud Front distributions for content delivery. Upload the content to an Amazon Elastic Compute Cloud (EC2) instance in the region closest to the user, send the content to a central Amazon Simple Storage Service (S3) bucket, and use an Amazon Cloud Front distribution for content delivery. Use an Amazon Cloud Front distribution for uploading the content to a central Amazon Simple Storage Service (S3) bucket and for content delivery . To enable end-to-end HTTPS connections from the user\u2018s browser to the origin via CloudFront, which of the following options are valid? Choose 2 answers [PROFESSIONAL] Use self signed certificate in the origin and CloudFront default certificate in CloudFront. (Origin cannot be self signed) Use the CloudFront default certificate in both origin and CloudFront (CloudFront cert cannot be applied to origin) Use 3rd-party CA certificate in the origin and CloudFront default certificate in CloudFront Use 3rd-party CA certificate in both origin and CloudFront Use a self signed certificate in both the origin and CloudFront (Origin cannot be self signed) Your application consists of 10% writes and 90% reads. You currently service all requests through a Route53 Alias Record directed towards an AWS ELB, which sits in front of an EC2 Auto Scaling Group. Your system is getting very expensive when there are large traffic spikes during certain news events, during which many more people request to read similar data all at the same time. What is the simplest and cheapest way to reduce costs and scale with spikes like this? [PROFESSIONAL] Create an S3 bucket and asynchronously replicate common requests responses into S3 objects. When a request comes in for a precomputed response, redirect to AWS S3 Create another ELB and Auto Scaling Group layer mounted on top of the other system, adding a tier to the system. Serve most read requests out of the top layer Create a CloudFront Distribution and direct Route53 to the Distribution. Use the ELB as an Origin and specify Cache Behaviors to proxy cache requests, which can be served late. (CloudFront can server request from cache and multiple cache behavior can be defined based on rules for a given URL pattern based on file extensions, file names, or any portion of a URL. Each cache behavior can include the CloudFront configuration values: origin server name, viewer connection protocol, minimum expiration period, query string parameters, cookies, and trusted signers for private content.) Create a Memcached cluster in AWS ElastiCache. Create cache logic to serve requests, which can be served late from the in-memory cache for increased performance. You are designing a service that aggregates clickstream data in batch and delivers reports to subscribers via email only once per week. Data is extremely spikey, geographically distributed, high-scale, and unpredictable. How should you design this system? Use a large RedShift cluster to perform the analysis, and a fleet of Lambdas to perform record inserts into the RedShift tables. Lambda will scale rapidly enough for the traffic spikes. Use a CloudFront distribution with access log delivery to S3. Clicks should be recorded as query string GETs to the distribution. Reports are built and sent by periodically running EMR jobs over the access logs in S3. (CloudFront is a Gigabit-Scale HTTP(S) global request distribution service and works fine with peaks higher than 10 Gbps or 15,000 RPS. It can handle scale, geo-spread, spikes, and unpredictability. Access Logs will contain the GET data and work just fine for batch analysis and email using EMR. Other streaming options are expensive as not required as the need is to batch analyze) Use API Gateway invoking Lambdas which PutRecords into Kinesis, and EMR running Spark performing GetRecords on Kinesis to scale with spikes. Spark on EMR outputs the analysis to S3, which are sent out via email. Use AWS Elasticsearch service and EC2 Auto Scaling groups. The Autoscaling groups scale based on click throughput and stream into the Elasticsearch domain, which is also scalable. Use Kibana to generate reports periodicall Your website is serving on-demand training videos to your workforce. Videos are uploaded monthly in high resolution MP4 format. Your workforce is distributed globally often on the move and using company-provided tablets that require the HTTP Live Streaming (HLS) protocol to watch a video. Your company has no video transcoding expertise and it required you might need to pay for a consultant. How do you implement the most cost-efficient architecture without compromising high availability and quality of video delivery? [PROFESSIONAL] Elastic Transcoder to transcode original high-resolution MP4 videos to HLS. S3 to host videos with lifecycle Management to archive original flies to Glacier after a few days. CloudFront to serve HLS transcoded videos from S3 A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number or nodes depending on the length of the queue S3 to host videos with Lifecycle Management to archive all files to Glacier after a few days CloudFront to serve HLS transcoding videos from Glacier Elastic Transcoder to transcode original high-resolution MP4 videos to HLS EBS volumes to host videos and EBS snapshots to incrementally backup original rues after a few days. CloudFront to serve HLS transcoded videos from EC2. A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number of nodes depending on the length of the queue. EBS volumes to host videos and EBS snapshots to incrementally backup original files after a few days. CloudFront to serve HLS transcoded videos from EC2","title":"19 CloudFront Cost"},{"location":"chap3/14direct_connect/","text":"14 AWS Direct Connect \u2013 DX 1 Direct Connect Overview AWS Direct Connect is a network service that provides an alternative to using the Internet to utilize AWS cloud services AWS Direct Connect links your internal network to an AWS Direct Connect location over a standard 1 gigabit or 10 gigabit Ethernet fiber-optic cable with one end of the cable connected to your router, the other to an AWS Direct Connect router. Direct Connect connection can be established with 1Gbps and 10Gbps ports. Speeds of 50Mbps, 100Mbps, 200Mbps, 300Mbps, 400Mbps, and 500Mbps can be ordered from any APN partners supporting AWS Direct Connect . AWS Direct Connect helps to create virtual interfaces directly to the AWS cloud for e.g, to EC2 & S3 and to Virtual Private Cloud (VPC), bypassing Internet service providers in the network path. AWS Direct Connect location provides access to Amazon Web Services in the region it is associated with, as well as access to other US regions (in case of a Direct Connect in a US region). for e.g. , you can provision a single connection to any AWS Direct Connect location in the US and use it to access public AWS services in all US Regions and AWS GovCloud (US). Each AWS Direct Connect location enables connectivity to all Availability Zones within the geographically nearest AWS region . 2 Direct Connect Advantages 2-1 Reduced Bandwidth Costs All data transferred over the dedicated connection is charged at the reduced AWS Direct Connect data transfer rate rather than Internet data transfer rates. Transferring data to and from AWS directly reduces your bandwidth commitment to your Internet service provider 2-2 Consistent Network Performance Direct Connect provides a dedicated connection and a more consistent network performance experience as compared to the Internet which can widely vary 2-3 AWS Services Compatibility Direct Connect is a network service and works with all of the AWS services like S3, EC2 and VPC 2-4 Private Connectivity to AWS VPC Using Direct Connect Private Virtual Interface a private, dedicated, high bandwidth network connection can be established between your network and VPC 2-5 Elastic Direct Connect can be easily scaled to meet the needs by either using a higher bandwidth connection or by establishing multiple connections. 3 Direct Connect vs IPSec VPN Connections A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution for immediate needs, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC VPN connections are very cheap ($37.20/month as of now) as compared to Direct Connect connection as it requires actual hardware and infrastructure and might go in thousands. 4 Direct Connect Anatomy Amazon maintains AWS Direct Connect PoP across different locations (referred to as Colocation Facilities) which are different from AWS regions Connection from the AWS Direct Connect PoP to the AWS regions is maintained by AWS itself As a consumer, you can either purchase a rack space or use any of the AWS APN Partner which already have the infrastructure within the Colocation Facility and configure a Customer Gateway Connection between the AWS Direct Connect PoP and the Customer gateway within the Colocation Facility is called Cross Connect . Connection from the Customer Gateway to the Customer Data Center can be establish using any Service Provider Network Once a Direct Connect connection is created with AWS, a LOA-CFA (Letter Of Authority \u2013 Connecting Facility Assignment) would be received. LOA-CFA can be handover to the Colocation Facility or the APN Partner to establish the Cross Connect Once the Cross Connect and the connectivity between the CGW and Customer DataCenter is established, Virtual Interfaces can be created AWS Direct Connect requires a VGW to access the AWS VPC Virtual Interfaces Each AWS Direct Connect connection requires a Virtual Interface Each AWS Direct Connect connection can be configured with one or more virtual interfaces . Public Virtual Interface can be created to connect to public resources for e.g. SQS, S3, EC2, Glacier etc which are reachable publicly only . Private virtual interface can be created to connect to the VPC for e.g. instances with private ip address Each virtual interface needs a VLAN ID, interface IP address, ASN, and BGP key. To use your AWS Direct Connect connection with another AWS account, you can create a hosted virtual interface for that account. These hosted virtual interfaces work the same as standard virtual interfaces and can connect to public resources or a VPC. 5 Direct Connect Redundancy Direct Connect connections do not provide redundancy and have multiple single point of failures wrt to the hardware devices as each connection consists of a single dedicated connection between ports on your router and an Amazon router Redundancy can be provided by Establishing a second Direct Connect connection, preferably in a different Colocation Facility using different router and AWS Direct Connect PoP IPsec VPN connection between the Customer DC to the VGW For Multiple ports requested in the same AWS Direct Connect location, Amazon itself makes sure they are provisioned on redundant Amazon routers to prevent impact from an hardware failure 6 Direct Connect LAG A link aggregation group (LAG) is a logical interface that uses the Link Aggregation Control Protocol (LACP) to aggregate multiple connections at a single AWS Direct Connect endpoint, treating them as a single, managed connection. LAG can be created from existing connections, or you can provision new connections. Existing connections (whether standalone or part of another LAG) with the LAG can be associated after LAG creation LAG needs following rules All connections in the LAG must use the same bandwidth . Maximum of four connections in a LAG. Each connection in the LAG counts towards the overall connection limit for the Region. All connections in the LAG must terminate at the same AWS Direct Connect endpoint . 7 AWS Certification Exam You are building a solution for a customer to extend their on-premises data center to AWS. The customer requires a 50-Mbps dedicated and private connection to their VPC. Which AWS product or feature satisfies this requirement? Amazon VPC peering Elastic IP Addresses AWS Direct Connect Amazon VPC virtual private gateway Is there any way to own a direct connection to Amazon Web Services? You can create an encrypted tunnel to VPC, but you don\u2019t own the connection. Yes, it\u2019s called Amazon Dedicated Connection. No, AWS only allows access from the public Internet. Yes, it\u2019s called Direct Connect An organization has established an Internet-based VPN connection between their on-premises data center and AWS. They are considering migrating from VPN to AWS Direct Connect. Which operational concern should drive an organization to consider switching from an Internet-based VPN connection to AWS Direct Connect? AWS Direct Connect provides greater redundancy than an Internet-based VPN connection. AWS Direct Connect provides greater resiliency than an Internet-based VPN connection. AWS Direct Connect provides greater bandwidth than an Internet-based VPN connection . AWS Direct Connect provides greater control of network provider selection than an Internet-based VPN connection. Does AWS Direct Connect allow you access to all Availabilities Zones within a Region? Depends on the type of connection No Yes Only when there\u2019s just one availability zone in a region. If there are more than one, only one availability zone can be accessed directly. A customer has established an AWS Direct Connect connection to AWS. The link is up and routes are being advertised from the customer\u2019s end, however the customer is unable to connect from EC2 instances inside its VPC to servers residing in its datacenter. Which of the following options provide a viable solution to remedy this situation? (Choose 2 answers) Add a route to the route table with an IPSec VPN connection as the target (deals with VPN) Enable route propagation to the Virtual Private Gateway (VGW) Enable route propagation to the customer gateway (CGW) (route propagation is enabled on VGW) Modify the route table of all Instances using the \u2018route\u2019 command. (no route command available) Modify the Instances VPC subnet route table by adding a route back to the customer\u2019s on-premises environment . A company has configured and peered two VPCs: VPC-1 and VPC-2. VPC-1 contains only private subnets, and VPC-2 contains only public subnets. The company uses a single AWS Direct Connect connection and private virtual interface to connect their on-premises network with VPC-1. Which two methods increase the fault tolerance of the connection to VPC-1? Choose 2 answers Establish a hardware VPN over the internet between VPC-2 and the on-premises network. (Peered VPC does not support Edge to Edge Routing) Establish a hardware VPN over the internet between VPC-1 and the on-premises network Establish a new AWS Direct Connect connection and private virtual interface in the same region as VPC-2 (Peered VPC does not support Edge to Edge Routing) Establish a new AWS Direct Connect connection and private virtual interface in a different AWS region than VPC-1 (need to be in the same region as VPC-1) Establish a new AWS Direct Connect connection and private virtual interface in the same AWS region as VPC-1 Your company previously configured a heavily used, dynamically routed VPN connection between your on premises data center and AWS. You recently provisioned a Direct Connect connection and would like to start using the new connection. After configuring Direct Connect settings in the AWS Console, which of the following options will provide the most seamless transition for your users? Delete your existing VPN connection to avoid routing loops configure your Direct Connect router with the appropriate settings and verity network traffic is leveraging Direct Connect. Configure your Direct Connect router with a higher BGP priority than your VPN router, verify network traffic is leveraging Direct Connect and then delete your existing VPN connection. Update your VPC route tables to point to the Direct Connect connection configure your Direct Connect router with the appropriate settings verify network traffic is leveraging Direct Connect and then delete the VPN connection . Configure your Direct Connect router, update your VPC route tables to point to the Direct Connect connection, configure your VPN connection with a higher BGP priority. And verify network traffic is leveraging the Direct Connect connection You are designing the network infrastructure for an application server in Amazon VPC. Users will access all the application instances from the Internet as well as from an on-premises network The on-premises network is connected to your VPC over an AWS Direct Connect link. How would you design routing to meet the above requirements? Configure a single routing Table with a default route via the Internet gateway. Propagate a default route via BGP on the AWS Direct Connect customer router. Associate the routing table with all VPC subnets (propagating default route would cause conflict) Configure a single routing table with a default route via the internet gateway. Propagate specific routes for the on-premises networks via BGP on the AWS Direct Connect customer router. Associate the routing table with all VPC subnets . Configure a single routing table with two default routes: one to the internet via an Internet gateway the other to the on-premises network via the VPN gateway use this routing table across all subnets in your VPC. (there cannot be 2 default routes) Configure two routing tables one that has a default route via the Internet gateway and another that has a default route via the VPN gateway Associate both routing tables with each VPC subnet. (as the instances has to be in public subnet and should have a single routing table associated with them) You are implementing AWS Direct Connect. You intend to use AWS public service end points such as Amazon S3, across the AWS Direct Connect link. You want other Internet traffic to use your existing link to an Internet Service Provider. What is the correct way to configure AWS Direct Connect for access to services such as Amazon S3? Configure a public Interface on your AWS Direct Connect link. Configure a static route via your AWS Direct Connect link that points to Amazon S3. Advertise a default route to AWS using BGP. Create a private interface on your AWS Direct Connect link. Configure a static route via your AWS Direct connect link that points to Amazon S3 Configure specific routes to your network in your VPC. Create a public interface on your AWS Direct Connect link. Redistribute BGP routes into your existing routing infrastructure advertise specific routes for your network to AWS Create a private interface on your AWS Direct connect link. Redistribute BGP routes into your existing routing infrastructure and advertise a default route to AWS. You have been asked to design network connectivity between your existing data centers and AWS. Your application\u2019s EC2 instances must be able to connect to existing backend resources located in your data center. Network traffic between AWS and your data centers will start small, but ramp up to 10s of GB per second over the course of several months. The success of your application is dependent upon getting to market quickly. Which of the following design options will allow you to meet your objectives? Quickly create an internal ELB for your backend applications, submit a DirectConnect request to provision a 1 Gbps cross connect between your data center and VPC, then increase the number or size of your DirectConnect connections as needed. Allocate EIPs and an Internet Gateway for your VPC instances to use for quick, temporary access to your backend applications, then provision a VPN connection between a VPC and existing on -premises equipment. Provision a VPN connection between a VPC and existing on -premises equipment, submit a DirectConnect partner request to provision cross connects between your data center and the DirectConnect location, then cut over from the VPN connection to one or more DirectConnect connections as needed. Quickly submit a DirectConnect request to provision a 1 Gbps cross connect between your data center and VPC, then increase the number or size of your DirectConnect connections as needed. You are tasked with moving a legacy application from a virtual machine running inside your datacenter to an Amazon VPC. Unfortunately this app requires access to a number of on-premises services and no one who configured the app still works for your company. Even worse there\u2019s no documentation for it. What will allow the application running inside the VPC to reach back and access its internal dependencies without being reconfigured? (Choose 3 answers) An AWS Direct Connect link between the VPC and the network housing the internal services An Internet Gateway to allow a VPN connection. (Virtual and Customer gateway is needed) An Elastic IP address on the VPC instance (Don\u2019t need a EIP as private subnets can also interact with on-premises network) An IP address space that does not conflict with the one on-premises (IP address cannot conflict) Entries in Amazon Route 53 that allow the Instance to resolve its dependencies\u2019 IP addresses (Route 53 is not required) A VM Import of the current virtual machine (VM Import to copy the VM to AWS as there is no documentation it can\u2019t be configured from scratch)","title":"L14 AWS Direct Connect \u2013 DX"},{"location":"chap3/14direct_connect/#14-aws-direct-connect-dx","text":"","title":"14 AWS Direct Connect \u2013 DX"},{"location":"chap3/14direct_connect/#1-direct-connect-overview","text":"AWS Direct Connect is a network service that provides an alternative to using the Internet to utilize AWS cloud services AWS Direct Connect links your internal network to an AWS Direct Connect location over a standard 1 gigabit or 10 gigabit Ethernet fiber-optic cable with one end of the cable connected to your router, the other to an AWS Direct Connect router. Direct Connect connection can be established with 1Gbps and 10Gbps ports. Speeds of 50Mbps, 100Mbps, 200Mbps, 300Mbps, 400Mbps, and 500Mbps can be ordered from any APN partners supporting AWS Direct Connect . AWS Direct Connect helps to create virtual interfaces directly to the AWS cloud for e.g, to EC2 & S3 and to Virtual Private Cloud (VPC), bypassing Internet service providers in the network path. AWS Direct Connect location provides access to Amazon Web Services in the region it is associated with, as well as access to other US regions (in case of a Direct Connect in a US region). for e.g. , you can provision a single connection to any AWS Direct Connect location in the US and use it to access public AWS services in all US Regions and AWS GovCloud (US). Each AWS Direct Connect location enables connectivity to all Availability Zones within the geographically nearest AWS region .","title":"1 Direct Connect Overview"},{"location":"chap3/14direct_connect/#2-direct-connect-advantages","text":"","title":"2 Direct Connect Advantages"},{"location":"chap3/14direct_connect/#2-1-reduced-bandwidth-costs","text":"All data transferred over the dedicated connection is charged at the reduced AWS Direct Connect data transfer rate rather than Internet data transfer rates. Transferring data to and from AWS directly reduces your bandwidth commitment to your Internet service provider","title":"2-1 Reduced Bandwidth Costs"},{"location":"chap3/14direct_connect/#2-2-consistent-network-performance","text":"Direct Connect provides a dedicated connection and a more consistent network performance experience as compared to the Internet which can widely vary","title":"2-2 Consistent Network Performance"},{"location":"chap3/14direct_connect/#2-3-aws-services-compatibility","text":"Direct Connect is a network service and works with all of the AWS services like S3, EC2 and VPC","title":"2-3 AWS Services Compatibility"},{"location":"chap3/14direct_connect/#2-4-private-connectivity-to-aws-vpc","text":"Using Direct Connect Private Virtual Interface a private, dedicated, high bandwidth network connection can be established between your network and VPC","title":"2-4 Private Connectivity to AWS VPC"},{"location":"chap3/14direct_connect/#2-5-elastic","text":"Direct Connect can be easily scaled to meet the needs by either using a higher bandwidth connection or by establishing multiple connections.","title":"2-5 Elastic"},{"location":"chap3/14direct_connect/#3-direct-connect-vs-ipsec-vpn-connections","text":"A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution for immediate needs, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC VPN connections are very cheap ($37.20/month as of now) as compared to Direct Connect connection as it requires actual hardware and infrastructure and might go in thousands.","title":"3 Direct Connect vs IPSec VPN Connections"},{"location":"chap3/14direct_connect/#4-direct-connect-anatomy","text":"Amazon maintains AWS Direct Connect PoP across different locations (referred to as Colocation Facilities) which are different from AWS regions Connection from the AWS Direct Connect PoP to the AWS regions is maintained by AWS itself As a consumer, you can either purchase a rack space or use any of the AWS APN Partner which already have the infrastructure within the Colocation Facility and configure a Customer Gateway Connection between the AWS Direct Connect PoP and the Customer gateway within the Colocation Facility is called Cross Connect . Connection from the Customer Gateway to the Customer Data Center can be establish using any Service Provider Network Once a Direct Connect connection is created with AWS, a LOA-CFA (Letter Of Authority \u2013 Connecting Facility Assignment) would be received. LOA-CFA can be handover to the Colocation Facility or the APN Partner to establish the Cross Connect Once the Cross Connect and the connectivity between the CGW and Customer DataCenter is established, Virtual Interfaces can be created AWS Direct Connect requires a VGW to access the AWS VPC","title":"4 Direct Connect Anatomy"},{"location":"chap3/14direct_connect/#virtual-interfaces","text":"Each AWS Direct Connect connection requires a Virtual Interface Each AWS Direct Connect connection can be configured with one or more virtual interfaces . Public Virtual Interface can be created to connect to public resources for e.g. SQS, S3, EC2, Glacier etc which are reachable publicly only . Private virtual interface can be created to connect to the VPC for e.g. instances with private ip address Each virtual interface needs a VLAN ID, interface IP address, ASN, and BGP key. To use your AWS Direct Connect connection with another AWS account, you can create a hosted virtual interface for that account. These hosted virtual interfaces work the same as standard virtual interfaces and can connect to public resources or a VPC.","title":"Virtual Interfaces"},{"location":"chap3/14direct_connect/#5-direct-connect-redundancy","text":"Direct Connect connections do not provide redundancy and have multiple single point of failures wrt to the hardware devices as each connection consists of a single dedicated connection between ports on your router and an Amazon router Redundancy can be provided by Establishing a second Direct Connect connection, preferably in a different Colocation Facility using different router and AWS Direct Connect PoP IPsec VPN connection between the Customer DC to the VGW For Multiple ports requested in the same AWS Direct Connect location, Amazon itself makes sure they are provisioned on redundant Amazon routers to prevent impact from an hardware failure","title":"5 Direct Connect Redundancy"},{"location":"chap3/14direct_connect/#6-direct-connect-lag","text":"A link aggregation group (LAG) is a logical interface that uses the Link Aggregation Control Protocol (LACP) to aggregate multiple connections at a single AWS Direct Connect endpoint, treating them as a single, managed connection. LAG can be created from existing connections, or you can provision new connections. Existing connections (whether standalone or part of another LAG) with the LAG can be associated after LAG creation LAG needs following rules All connections in the LAG must use the same bandwidth . Maximum of four connections in a LAG. Each connection in the LAG counts towards the overall connection limit for the Region. All connections in the LAG must terminate at the same AWS Direct Connect endpoint .","title":"6 Direct Connect LAG"},{"location":"chap3/14direct_connect/#7-aws-certification-exam","text":"You are building a solution for a customer to extend their on-premises data center to AWS. The customer requires a 50-Mbps dedicated and private connection to their VPC. Which AWS product or feature satisfies this requirement? Amazon VPC peering Elastic IP Addresses AWS Direct Connect Amazon VPC virtual private gateway Is there any way to own a direct connection to Amazon Web Services? You can create an encrypted tunnel to VPC, but you don\u2019t own the connection. Yes, it\u2019s called Amazon Dedicated Connection. No, AWS only allows access from the public Internet. Yes, it\u2019s called Direct Connect An organization has established an Internet-based VPN connection between their on-premises data center and AWS. They are considering migrating from VPN to AWS Direct Connect. Which operational concern should drive an organization to consider switching from an Internet-based VPN connection to AWS Direct Connect? AWS Direct Connect provides greater redundancy than an Internet-based VPN connection. AWS Direct Connect provides greater resiliency than an Internet-based VPN connection. AWS Direct Connect provides greater bandwidth than an Internet-based VPN connection . AWS Direct Connect provides greater control of network provider selection than an Internet-based VPN connection. Does AWS Direct Connect allow you access to all Availabilities Zones within a Region? Depends on the type of connection No Yes Only when there\u2019s just one availability zone in a region. If there are more than one, only one availability zone can be accessed directly. A customer has established an AWS Direct Connect connection to AWS. The link is up and routes are being advertised from the customer\u2019s end, however the customer is unable to connect from EC2 instances inside its VPC to servers residing in its datacenter. Which of the following options provide a viable solution to remedy this situation? (Choose 2 answers) Add a route to the route table with an IPSec VPN connection as the target (deals with VPN) Enable route propagation to the Virtual Private Gateway (VGW) Enable route propagation to the customer gateway (CGW) (route propagation is enabled on VGW) Modify the route table of all Instances using the \u2018route\u2019 command. (no route command available) Modify the Instances VPC subnet route table by adding a route back to the customer\u2019s on-premises environment . A company has configured and peered two VPCs: VPC-1 and VPC-2. VPC-1 contains only private subnets, and VPC-2 contains only public subnets. The company uses a single AWS Direct Connect connection and private virtual interface to connect their on-premises network with VPC-1. Which two methods increase the fault tolerance of the connection to VPC-1? Choose 2 answers Establish a hardware VPN over the internet between VPC-2 and the on-premises network. (Peered VPC does not support Edge to Edge Routing) Establish a hardware VPN over the internet between VPC-1 and the on-premises network Establish a new AWS Direct Connect connection and private virtual interface in the same region as VPC-2 (Peered VPC does not support Edge to Edge Routing) Establish a new AWS Direct Connect connection and private virtual interface in a different AWS region than VPC-1 (need to be in the same region as VPC-1) Establish a new AWS Direct Connect connection and private virtual interface in the same AWS region as VPC-1 Your company previously configured a heavily used, dynamically routed VPN connection between your on premises data center and AWS. You recently provisioned a Direct Connect connection and would like to start using the new connection. After configuring Direct Connect settings in the AWS Console, which of the following options will provide the most seamless transition for your users? Delete your existing VPN connection to avoid routing loops configure your Direct Connect router with the appropriate settings and verity network traffic is leveraging Direct Connect. Configure your Direct Connect router with a higher BGP priority than your VPN router, verify network traffic is leveraging Direct Connect and then delete your existing VPN connection. Update your VPC route tables to point to the Direct Connect connection configure your Direct Connect router with the appropriate settings verify network traffic is leveraging Direct Connect and then delete the VPN connection . Configure your Direct Connect router, update your VPC route tables to point to the Direct Connect connection, configure your VPN connection with a higher BGP priority. And verify network traffic is leveraging the Direct Connect connection You are designing the network infrastructure for an application server in Amazon VPC. Users will access all the application instances from the Internet as well as from an on-premises network The on-premises network is connected to your VPC over an AWS Direct Connect link. How would you design routing to meet the above requirements? Configure a single routing Table with a default route via the Internet gateway. Propagate a default route via BGP on the AWS Direct Connect customer router. Associate the routing table with all VPC subnets (propagating default route would cause conflict) Configure a single routing table with a default route via the internet gateway. Propagate specific routes for the on-premises networks via BGP on the AWS Direct Connect customer router. Associate the routing table with all VPC subnets . Configure a single routing table with two default routes: one to the internet via an Internet gateway the other to the on-premises network via the VPN gateway use this routing table across all subnets in your VPC. (there cannot be 2 default routes) Configure two routing tables one that has a default route via the Internet gateway and another that has a default route via the VPN gateway Associate both routing tables with each VPC subnet. (as the instances has to be in public subnet and should have a single routing table associated with them) You are implementing AWS Direct Connect. You intend to use AWS public service end points such as Amazon S3, across the AWS Direct Connect link. You want other Internet traffic to use your existing link to an Internet Service Provider. What is the correct way to configure AWS Direct Connect for access to services such as Amazon S3? Configure a public Interface on your AWS Direct Connect link. Configure a static route via your AWS Direct Connect link that points to Amazon S3. Advertise a default route to AWS using BGP. Create a private interface on your AWS Direct Connect link. Configure a static route via your AWS Direct connect link that points to Amazon S3 Configure specific routes to your network in your VPC. Create a public interface on your AWS Direct Connect link. Redistribute BGP routes into your existing routing infrastructure advertise specific routes for your network to AWS Create a private interface on your AWS Direct connect link. Redistribute BGP routes into your existing routing infrastructure and advertise a default route to AWS. You have been asked to design network connectivity between your existing data centers and AWS. Your application\u2019s EC2 instances must be able to connect to existing backend resources located in your data center. Network traffic between AWS and your data centers will start small, but ramp up to 10s of GB per second over the course of several months. The success of your application is dependent upon getting to market quickly. Which of the following design options will allow you to meet your objectives? Quickly create an internal ELB for your backend applications, submit a DirectConnect request to provision a 1 Gbps cross connect between your data center and VPC, then increase the number or size of your DirectConnect connections as needed. Allocate EIPs and an Internet Gateway for your VPC instances to use for quick, temporary access to your backend applications, then provision a VPN connection between a VPC and existing on -premises equipment. Provision a VPN connection between a VPC and existing on -premises equipment, submit a DirectConnect partner request to provision cross connects between your data center and the DirectConnect location, then cut over from the VPN connection to one or more DirectConnect connections as needed. Quickly submit a DirectConnect request to provision a 1 Gbps cross connect between your data center and VPC, then increase the number or size of your DirectConnect connections as needed. You are tasked with moving a legacy application from a virtual machine running inside your datacenter to an Amazon VPC. Unfortunately this app requires access to a number of on-premises services and no one who configured the app still works for your company. Even worse there\u2019s no documentation for it. What will allow the application running inside the VPC to reach back and access its internal dependencies without being reconfigured? (Choose 3 answers) An AWS Direct Connect link between the VPC and the network housing the internal services An Internet Gateway to allow a VPN connection. (Virtual and Customer gateway is needed) An Elastic IP address on the VPC instance (Don\u2019t need a EIP as private subnets can also interact with on-premises network) An IP address space that does not conflict with the one on-premises (IP address cannot conflict) Entries in Amazon Route 53 that allow the Instance to resolve its dependencies\u2019 IP addresses (Route 53 is not required) A VM Import of the current virtual machine (VM Import to copy the VM to AWS as there is no documentation it can\u2019t be configured from scratch)","title":"7 AWS Certification Exam"},{"location":"chap3/1vpc/","text":"L1 AWS Virtual Private Cloud \u2013 VPC 1 VPC Overview & Components A virtual private cloud (VPC) is a virtual network dedicated to the AWS account. It is logically isolated from other virtual networks in the AWS cloud. VPC allows the user to select IP address range, create subnets, and configure route tables, network gateways, and security settings. 1-1 VPC Sizing VPC needs a set of IP addresses in the form of a Classless Inter-Domain Routing (CIDR) block for e.g, 10.0.0.0/16 , which allows 2^16 (65536) IP address to be available Allowed CIDR block size is between /28 netmask (minimum with 2^4 \u2013 16 available IP address) and /16 netmask (maximum with 2^16 \u2013 65536 IP address) CIDR block from private (non-publicly routable) IP address can be assigned 10.0.0.0 \u2013 10.255.255.255 (10/8 prefix) 172.16.0.0 \u2013 172.31.255.255 (172.16/12 prefix) 192.168.0.0 \u2013 192.168.255.255 (192.168/16 prefix) It\u2019s possible to specify a range of publicly routable IP addresses; however, direct access to the Internet is not currently supported from publicly routable CIDR blocks in a VPC NOTE \u2013 You can now resize VPC Each VPC is separate from any other VPC created with the same CIDR block even if it resides within the same AWS account VPC allows VPC Peering connections with other VPC within the same or different AWS accounts Connection between your VPC and corporate or home network can be established , however the CIDR blocks should be not be overlapping for e.g. VPC with CIDR 10.0.0.0/16 can communicate with 10.1.0.0/16 corporate network but the connections would be dropped if it tries to connect to 10.0.37.0/16 corporate network cause of overlapping ip addresses. 1-2 VPC Peering VPC allows VPC Peering connections with other VPC within the same or different AWS accounts VPC Peering Overview A VPC peering connection is a networking connection between two VPCs that enables routing of traffic between them using private IP addresses. Instances in either VPC can communicate with each other as if they are within the same network VPC peering connection can be established between your own VPCs, or with a VPC in another AWS account in a single different region. AWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is neither a gateway nor a VPN connection, and does not rely on a separate piece of physical hardware. There is no single point of failure for communication or a bandwidth bottleneck VPC Peering Rules & Limitations VPC peering connection cannot be created between VPCs that have matching or overlapping CIDR blocks. VPC Peering is now supported inter-region VPC peering connection are limited on the number active and pending VPC peering connections that you can have per VPC. VPC peering does not support transitive peering relationships . In a VPC peering connection, the VPC does not have access to any other VPCs that the peer VPC may be peered with even if established entirely within your own AWS account VPC peering does not support Edge to Edge Routing Through a Gateway or Private Connection In a VPC peering connection, the VPC does not have access to any other connection that the peer VPC may have and vice versa. Connections that the peer VPC can include A VPN connection or an AWS Direct Connect connection to a corporate network An Internet connection through an Internet gateway An Internet connection in a private subnet through a NAT device A ClassicLink connection to an EC2-Classic instance A VPC endpoint to an AWS service; for example, an endpoint to S3 . Only one VPC peering connection can be established between the same two VPCs at the same time Maximum Transmission Unit (MTU) across a VPC peering connection is 1500 bytes. A placement group can span peered VPCs that are in the same region ; however, you do not get full-bisection bandwidth between instances in peered VPCs Any tags created for the VPC peering connection are only applied in the account or region in which they were created Unicast reverse path forwarding in VPC peering connections is not supported Instance\u2019s Public DNS can now be resolved to its private IP address across peered VPCs VPC Peering Architecture VPC Peering can be applied to create shared services or perform authentication with an on-premises instance This would help creating a single point of contact, as well limiting the VPN connections to a single account or VPC Questions A company has an AWS account that contains three VPCs (Dev, Test, and Prod) in the same region. Test is peered to both Prod and Dev. All VPCs have non-overlapping CIDR blocks. The company wants to push minor code releases from Dev to Prod to speed up time to market. Which of the following options helps the company accomplish this? Create a new peering connection Between Prod and Dev along with appropriate routes. Create a new entry to Prod in the Dev route table using the peering connection as the target. Attach a second gateway to Dev. Add a new entry in the Prod route table identifying the gateway as the target. The VPCs have non-overlapping CIDR blocks in the same account. The route tables contain local routes for all VPCs. VPC allows you to set tenancy option for the Instances launched in it . By default, the tenancy option is shared. If dedicated option selected, all the instances within it are launched on a dedicated hardware overriding the individual instance tenancy setting Deletion of the VPC is possible only after terminating all instances within the VPC, and deleting all the components with the VPC for e.g. subnets, security groups, network ACLs, route tables, Internet gateways, VPC peering connections, and DHCP options 2 IP Addresses Instances launched in the VPC can have Private, Public and Elastic IP address assigned to it and are properties of ENI (Network Interfaces) Private IP Addresses Private IP addresses are not reachable over the Internet, and can be used for communication only between the instances within the VPC All instances are assigned a private IP address , within the IP address range of the subnet, to the default network interface Primary IP address is associated with the network interface for its lifetime, even when the instance is stopped and restarted and is released only when the instance is terminated Additional Private IP addresses, known as secondary private IP address, can be assigned to the instances and these can be reassigned from one network interface to another Public IP address Public IP addresses are reachable over the Internet , and can be used for communication between instances and the Internet, or with other AWS services that have public endpoints Public IP address assignment to the Instance depends if the Public IP Addressing is enabled for the Subnet . Public IP address can also be assigned to the Instance by enabling the Public IP addressing during the creation of the instance , which overrides the subnet\u2019s public IP addressing attribute Public IP address is assigned from AWS pool of IP addresses and it is not associated with the AWS account and hence is released when the instance is stopped and restarted or terminated. Elastic IP address Elastic IP addresses are static, persistent public IP addresses which can be associated and disassociated with the instance, as required Elastic IP address is allocated at an VPC and owned by the account unless released A Network Interface can be assigned either a Public IP or an Elastic IP . If you assign an instance, already having an Public IP, an Elastic IP, the public IP is released Elastic IP addresses can be moved from one instance to another , which can be within the same or different VPC within the same account Elastic IP are charged for non usage i.e. if it is not associated or associated with a stopped instance or an unattached Network Interface 3\u3001Elastic Network Interface (ENI) Each Instance is attached with default elastic network interface ( Primary Network Interface eth0 ) and cannot be detached from the instance ENI can include the following attributes Primary private IP address One or more secondary private IP addresses One Elastic IP address per private IP address One public IP address, which can be auto-assigned to the network interface for eth0 when you launch an instance, but only when you create a network interface for eth0 instead of using an existing ENI One or more security groups A MAC address A source/destination check flag A description ENI\u2019s attributes follow the ENI as it is attached or detached from an instance and reattached to another instance . When an ENI is moved from one instance to another, network traffic is redirected to the new instance. Multiple ENIs can be attached to an instance and is useful for use cases: Create a management network. Use network and security appliances in your VPC. Create dual-homed instances with workloads/roles on distinct subnets. Create a low-budget, high-availability solution. 4 Route Tables Route table defines rules, termed as routes , which determine where network traffic from the subnet would be routed Each VPC has a implicit router to route network traffic Each VPC has a Main Route table, and can have multiple custom route tables created Each Subnet within a VPC must be associated with a single route table at a time, while a route table can have multiple subnets associated with it Subnet, if not explicitly associated to a route table, is implicitly associated with the main route table Every route table contains a local route that enables communication within a VPC which cannot be modified or deleted Route priority is decided by matching the most specific route in the route table that matches the traffic Route tables needs to be updated to defined routes for Internet gateways , Virtual Private gateways, VPC Peering, VPC Endpoints, NAT Device etc. 5 Internet Gateways \u2013 IGW An Internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in the VPC and the Internet . IGW imposes no availability risks or bandwidth constraints on the network traffic. An Internet gateway serves two purposes: To provide a target in the VPC route tables for Internet-routable traffic, To perform network address translation (NAT) for instances that have been NOT been assigned public IP addresses . 5-1 Enabling Internet access to an Instance requires Attaching Internet gateway to the VPC Subnet should have route tables associated with the route pointing to the Internet gateway Instances should have a Public IP or Elastic IP address assigned Security groups and NACLs associated with the Instance should allow relevant traffic 6 NAT NAT device enables instances in a private subnet to connect to the Internet or other AWS services , but prevents the Internet from initiating connections with the instances. NAT devices do not support IPv6 traffic, use an egress-only Internet gateway instead . 6-1 NAT Overview Network Address Translation (NAT) devices, launched in the public subnet, enables instances in a private subnet to connect to the Internet, but prevents the Internet from initiating connections with the instances. Instances in private subnets would need internet connection for performing software updates or trying to access external services NAT device performs the function of both address translation and port address translation (PAT) NAT instance prevents instances to be directly exposed to the Internet and having to be launched in Public subnet and assignment of the Elastic IP address to all, which are limited. NAT device routes the traffic, from the private subnet to the Internet, by replacing the source IP address with its address and for the response traffic it translates the address back to the instances\u2019 private IP addresses. AWS allows NAT configuration in 2 ways NAT Instance NAT Gateway, managed service by AWS 6-2 NAT device Configuration Key Points needs to be launched in the Public Subnet needs to be associated with an Elastic IP address (or public IP address) should have the Source/Destination flag disabled to route traffic from the instances in the private subnet to the Internet and send the response back should have a Security group associated that allows Outbound Internet traffic from instances in the private subnet disallows Inbound Internet traffic from everywhere Instances in the private subnet should have the Route table configured to direct all Internet traffic to the NAT device 6-3 NAT Gateway NAT gateway is a AWS managed NAT service that provides better availability, higher bandwidth, and requires less administrative effort. A NAT gateway supports bursts of up to 10 Gbps of bandwidth . For over 10 Gbps bursts requirement, the workload can be distributed by splitting the resources into multiple subnets, and creating a NAT gateway in each subnet NAT gateway is associated with One Elastic IP address which cannot be disassociated after it\u2019s creation. Each NAT gateway is created in a specific Availability Zone and implemented with redundancy in that zone . A NAT gateway supports the following protocols: TCP, UDP, and ICMP . NAT gateway cannot be associated a security group. Security can be configured for the instances in the private subnets to control the traffic Network ACL can be used to control the traffic to and from the subnet. NACL applies to the NAT gateway\u2019s traffic, which uses ports 1024-65535 NAT gateway when created receives an elastic network interface that\u2019s automatically assigned a private IP address from the IP address range of the subnet. Attributes of this network interface cannot be modified NAT gateway cannot send traffic over VPC endpoints, VPN connections, AWS Direct Connect, or VPC peering connections . Private subnet\u2019s route table should be modified to route the traffic directly to these devices. 6-4 NAT Instance NAT instance can be created by using Amazon Linux AMIs configured to route traffic to Internet. They do not provide the same availability and bandwidth and need to configured as per the application needs. NAT instances must have security groups associated with Inbound traffic enabled from private subnets and Outbound traffic enabled to the Internet NAT instances should have the Source Destination Check attribute disabled, as it is neither the source nor the destination for the traffic and merely acts as a gateway 6-5 High Availability NAT Instance Create One NAT instance per Availability Zone Configure all Private subnet route tables to the same zone NAT instance Use Auto Scaling for NAT availability Use Auto Scaling group per NAT instance with min and max size set of 1. So if NAT instances fail, Auto Scaling will automatically launch an replacement instance NAT instance is highly available with limited downtime Let Auto Scaling monitor health and availability of the NAT instance Bootstrap scripts with the NAT instance to update the Route tables programmatically Keep a close watch on the Network Metrics and scale vertically the NAT instance type to the one with high network performance 6-6 Disabling Source/Destination checks Each EC2 instance performs source/destination checks, by default, and the instance must be the source or destination of any traffic it sends or receives However, as the NAT instance acts as a router between the Internet and the instances in the private subnet it must be able to send and receive traffic when the source or destination is not itself . Therefore, the source/destination checks on the NAT instance should be disabled 6-7 NAT Gateway & Instance Comparison 6-8 NAT Exam After launching an instance that you intend to serve as a NAT (Network Address Translation) device in a public subnet you modify your route tables to have the NAT device be the target of internet bound traffic of your private subnet. When you try and make an outbound connection to the Internet from an instance in the private subnet , you are not successful. Which of the following steps could resolve the issue? Attaching a second Elastic Network interface (ENI) to the NAT instance, and placing it in the private subnet Attaching an Elastic IP address to the instance in the private subnet Attaching a second Elastic Network Interface (ENI) to the instance in the private subnet, and placing it in the public subnet Disabling the Source/Destination Check attribute on the NAT instance (V) You manually launch a NAT AMI in a public subnet. The network is properly configured. Security groups and network access control lists are property configured. Instances in a private subnet can access the NAT. The NAT can access the Internet. However, private instances cannot access the Internet . What additional step is required to allow access from the private instances? Enable Source/Destination Check on the private Instances. Enable Source/Destination Check on the NAT instance. Disable Source/Destination Check on the private instances Disable Source/Destination Check on the NAT instance A user has created a VPC with public and private subnets. The VPC has CIDR 20.0.0.0/16 . The private subnet uses CIDR 20.0.1.0/24 and the public subnet uses CIDR 20.0.0.0/24 . The user is planning to host a web server in the public subnet (port 80. and a DB server in the private subnet (port 3306.. The user is configuring a security group of the NAT instance. Which of the below mentioned entries is not required for the NAT security group ? For Inbound allow Source: 20.0.1.0/24 on port 80 For Outbound allow Destination: 0.0.0.0/0 on port 80 For Inbound allow Source: 20.0.0.0/24 on port 80 For Outbound allow Destination: 0.0.0.0/0 on port 443 A web company is looking to implement an external payment service into their highly available application deployed in a VPC. Their application EC2 instances are behind a public facing ELB. Auto scaling is used to add additional instances as traffic increases. Under normal load the application runs 2 instances in the Auto Scaling group but at peak it can scale 3x in size. The application instances need to communicate with the payment service over the Internet, which requires whitelisting of all public IP addresses used to communicate with it . A maximum of 4 whitelisting IP addresses are allowed at a time and can be added through an API . How should they architect their solution? Route payment requests through two NAT instances setup for High Availability and whitelist the Elastic IP addresses attached to the NAT instances (check) Whitelist the VPC Internet Gateway Public IP and route payment requests through the Internet Gateway. ( Internet gateway is only to route traffic ) Whitelist the ELB IP addresses and route payment requests from the Application servers through the ELB. ( ELB does not have a fixed IP address ) Automatically assign public IP addresses to the application instances in the Auto Scaling group and run a script on boot that adds each instances public IP address to the payment validation whitelist API. ( would exceed the allowed 4 IP addresses ) 7\u3001Egress-only Internet gateway Egress-only Internet gateway works as a NAT gateway, but for IPv6 traffic Egress-only Internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in the VPC to the Internet, and prevents the Internet from initiating an IPv6 connection with the instances. An egress-only Internet gateway is for use with IPv6 traffic only . To enable outbound-only Internet communication over IPv4, use a NAT gateway instead . 8\u3001VPC & Subnet Sizing VPC supports IPv4 and IPv6 addressing, and has different CIDR block size limits for each IPv6 CIDR block can be optionally associated with the VPC VPC IPv4 CIDR block cannot be modified once created i.e. cannot increase or decrease the size of an existing CIDR block. However, secondary CIDR blocks can be associated with the VPC to extend the VPC Limitations allowed block size is between a /28 netmask and /16 netmask. CIDR block must not overlap with any existing CIDR block that\u2019s associated with the VPC. CIDR block must not be the same or larger than the CIDR range of a route in any of the VPC route tables for e.g. for a CIDR block 10.0.0.0/24 , can only associate smaller CIDR blocks like 10.0.0.0/25 9\u3001VPC Security Security within a VPC is provided through Security groups \u2013 Act as a firewall for associated EC2 instances , controlling both inbound and outbound traffic at the instance level Network access control lists (ACLs) \u2013 Act as a firewall for associated subnets, controlling both inbound and outbound traffic at the subnet level Flow logs \u2013 Capture information about the IP traffic going to and from network interfaces in your VPC 10\u3001Security Groups vs NACLs 10-1 AWS VPC Security Overview In a VPC, both Security Groups and Network ACLs (NACLS) together help to build a layered network defence. Security groups \u2013 Act as a virtual firewall for associated instances , controlling both inbound and outbound traffic at the instance level Network access control lists (NACLs) \u2013 Act as a firewall for associated subnets, controlling both inbound and outbound traffic at the subnet level 10-2 Security Groups Acts at an Instance level and not at the subnet level. Each instance within a subnet can be assigned a different set of Security groups An instance can be assigned 5 security groups with each security group having60 rules allows separate rules for inbound and outbound traffic allows adding or removing rules (authorizing or revoking access) for both Inbound (ingress) and Outbound (egress) traffic to the instance Default Security group allows no external inbound traffic but allows inbound traffic from instances with the same security group Default Security group allows all outbound traffic New Security groups start with only an outbound rule that allows all traffic to leave the instances can specify only Allow rules, but not deny rules can grant access to a specific IP, CIDR range, or to another security group in the VPC or in a peer VPC (requires a VPC peering connection) are evaluated as a Whole or Cumulative bunch of rules with the most permissive rule taking precedence for e.g. if you have a rule that allows access to TCP port 22 (SSH) from IP address 203.0.113.1 and another rule that allows access to TCP port 22 from everyone, everyone has access to TCP port 22. are Stateful \u2013 responses to allowed inbound traffic are allowed to flow outbound regardless of outbound rules, and vice versa. Hence an Outbound rule for the response is not needed Instances associated with a security group can\u2019t talk to each other unless rules allowing the traffic are added . are associated with ENI (network interfaces) . are associated with the instance and can be changed, which changes the security groups associated with the primary network interface (eth0) and the changes would be applicable immediately to all the instances associated with the Security group 10-3 Connection Tracking Security groups are Stateful as they use Connection tracking to track information about traffic to and from the instance. Responses to inbound traffic are allowed to flow out of the instance regardless of outbound security group rules , and vice versa. Connection Tracking is maintained only if there is no explicit Outbound rule for an Inbound request (and vice versa) However, if there is an explicit Outbound rule for an Inbound request, the response traffic is allowed on the basis of the Outbound rule and not on the Tracking information Tracking flow e.g. If an instance (host A) initiates traffic to host B and uses a protocol other than TCP, UDP, or ICMP, the instance\u2019s firewall only tracks the IP address & protocol number for the purpose of allowing response traffic from host B. If host B initiates traffic to the instance in a separate request within 600 seconds of the original request or response, the instance accepts it regardless of inbound security group rules, because it\u2019s regarded as response traffic. This can be controlled by modifying the security group\u2019s outbound rules to permit only certain types of outbound traffic. Alternatively, Network ACLs (NACLs) can be used for the subnet, network ACLs are stateless and therefore do not automatically allow response traffic. 10-4 Network Access Control Lists \u2013 NACLs A Network ACLs (NACLs) is an optional layer of security for the VPC that acts as a firewall for controlling traffic in and out of one or more subnets. are not for granular control and are assigned at a Subnet level and is applicable to all the instances in that Subnet has separate inbound and outbound rules, and each rule can either allow or deny traffic Default ACL allows all inbound and outbound traffic . Newly created ACL denies all inbound and outbound traffic A Subnet can be assigned only 1 NACLs and if not associated explicitly would be associated implicitly with the default NACL can associate a network ACL with multiple subnets is a numbered list of rules that are evaluated in order starting with the lowest numbered rule, to determine whether traffic is allowed in or out of any subnet associated with the network ACL for e.g. if you have a Rule No. 100 with Allow All and 110 with Deny All, the Allow All would take precedence and all the traffic will be allowed are Stateless ; responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa) for e.g. if you enable Inbound SSH on port 22 from the specific IP address, you would need to add an Outbound rule for the response as well. 10-4 Security Group vs NACLs Questions Instance A and instance B are running in two different subnets A and B of a VPC. Instance A is not able to ping instance B . What are two possible reasons for this? (Pick 2 correct answers) The routing table of subnet A has no target route to subnet B The security group attached to instance B does not allow inbound ICMP traffic The policy linked to the IAM role on instance A is not configured correctly The NACL on subnet B does not allow outbound ICMP traffic An instance is launched into a VPC subnet with the network ACL configured to allow all inbound traffic and deny all outbound traffic . The instance\u2019s security group is configured to allow SSH from any IP address and deny all outbound traffic . What changes need to be made to allow SSH access to the instance? The outbound security group needs to be modified to allow outbound traffic. The outbound network ACL needs to be modified to allow outbound traffic . Nothing, it can be accessed from any IP address using SSH. Both the outbound security group and outbound network ACL need to be modified to allow outbound traffic. From what services I can block incoming/outgoing IPs? Security Groups DNS ELB VPC subnet IGW NACL What is the difference between a security group in VPC and a network ACL in VPC (chose 3 correct answers) Security group restricts access to a Subnet while ACL restricts traffic to EC2 Security group restricts access to EC2 while ACL restricts traffic to a subnet Security group can work outside the VPC also while ACL only works within a VPC Network ACL performs stateless filtering and Security group provides stateful filtering Security group can only set Allow rule, while ACL can set Deny rule also You are currently hosting multiple applications in a VPC and have logged numerous port scans coming in from a specific IP address block. Your security team has requested that all access from the offending IP address block be denied for the next 24 hours. Which of the following is the best method to quickly and temporarily deny access from the specified IP address block? Create an AD policy to modify Windows Firewall settings on all hosts in the VPC to deny access from the IP address block Modify the Network ACLs associated with all public subnets in the VPC to deny access from the IP address block Add a rule to all of the VPC 5 Security Groups to deny access from the IP address block Modify the Windows Firewall settings on all Amazon Machine Images (AMIs) that your organization uses in that VPC to deny access from the IP address block You have two Elastic Compute Cloud (EC2) instances inside a Virtual Private Cloud (VPC) in the same Availability Zone (AZ) but in different subnets. One instance is running a database and the other instance an application that will interface with the database. You want to confirm that they can talk to each other for your application to work properly. Which two things do we need to confirm in the VPC settings so that these EC2 instances can communicate inside the VPC ? Choose 2 answers A network ACL that allows communication between the two subnets . Both instances are the same instance class and using the same Key-pair. That the default route is set to a NAT instance or Internet Gateway (IGW) for them to communicate. Security groups are set to allow the application host to talk to the database on the right port/protocol A benefits enrollment company is hosting a 3-tier web application running in a VPC on AWS, which includes a NAT (Network Address Translation) instance in the public Web tier. There is enough provisioned capacity for the expected workload tor the new fiscal year benefit enrollment period plus some extra overhead Enrollment proceeds nicely for two days and then the web tier becomes unresponsive, upon investigation using CloudWatch and other monitoring tools it is discovered that there is an extremely large and unanticipated amount of inbound traffic coming from a set of 15 specific IP addresses over port 80 from a country where the benefits company has no customers. The web tier instances are so overloaded that benefit enrollment administrators cannot even SSH into them . Which activity would be useful in defending against this attack? Create a custom route table associated with the web tier and block the attacking IP addresses from the IGW (internet Gateway) Change the EIP (Elastic IP Address) of the NAT instance in the web tier subnet and update the Main Route Table with the new EIP Create 15 Security Group rules to block the attacking IP addresses over port 80 Create an inbound NACL (Network Access control list) associated with the web tier subnet with deny rules to block the attacking IP addresses Which of the following statements describes network ACLs? (Choose 2 answers) Responses to allowed inbound traffic are allowed to flow outbound regardless of outbound rules, and vice versa ( are stateless ) Using network ACLs, you can deny access from a specific IP range Keep network ACL rules simple and use a security group to restrict application level access NACLs are associated with a single Availability Zone ( associated with Subnet ) You are designing security inside your VPC. You are considering the options for establishing separate security zones and enforcing network traffic rules across different zone to limit Instances can communications. How would you accomplish these requirements? Choose 2 answers Configure a security group for every zone. Configure a default allow all rule. Configure explicit deny rules for the zones that shouldn\u2019t be able to communicate with one another ( Security group does not allow deny rules ) Configure you instances to use pre-set IP addresses with an IP address range every security zone. Configure NACL to explicitly allow or deny communication between the different IP address ranges, as required for interzone communication Configure a security group for every zone. Configure allow rules only between zone that need to be able to communicate with one another. Use implicit deny all rule to block any other traffic Configure multiple subnets in your VPC, one for each zone. Configure routing within your VPC in such a way that each subnet only has routes to other subnets with which it needs to communicate, and doesn\u2019t have routes to subnets with which it shouldn\u2019t be able to communicate. ( default routes are unmodifiable ) Your entire AWS infrastructure lives inside of one Amazon VPC. You have an Infrastructure monitoring application running on an Amazon instance in Availability Zone (AZ) A of the region, and another application instance running in AZ B. The monitoring application needs to make use of ICMP ping to confirm network reachability of the instance hosting the application. Can you configure the security groups for these instances to only allow the ICMP ping to pass from the monitoring instance to the application instance and nothing else\u201d If so how? No Two instances in two different AZ\u2019s can\u2019t talk directly to each other via ICMP ping as that protocol is not allowed across subnet (i.e. broadcast) boundaries ( Can communicate ) Yes Both the monitoring instance and the application instance have to be a part of the same security group, and that security group needs to allow inbound ICMP ( Need not have to be part of same security group ) Yes, The security group for the monitoring instance needs to allow outbound ICMP and the application instance\u2019s security group needs to allow Inbound ICMP (is stateful, so just allow outbound ICMP from monitoring and inbound ICMP on monitored instance) Yes, Both the monitoring instance\u2019s security group and the application instance\u2019s security group need to allow both inbound and outbound ICMP ping packets since ICMP is not a connection-oriented protocol (Security groups are stateful) A user has configured a VPC with a new subnet. The user has created a security group. The user wants to configure that instances of the same subnet communicate with each other. How can the user configure this with the security group? There is no need for a security group modification as all the instances can communicate with each other inside the same subnet Configure the subnet as the source in the security group and allow traffic on all the protocols and ports Configure the security group itself as the source and allow traffic on all the protocols and ports The user has to use VPC peering to configure this You are designing a data leak prevention solution for your VPC environment. You want your VPC Instances to be able to access software depots and distributions on the Internet for product updates. The depots and distributions are accessible via third party CDNs by their URLs. You want to explicitly deny any other outbound connections from your VPC instances to hosts on the Internet. Which of the following options would you consider? Configure a web proxy server in your VPC and enforce URL-based rules for outbound access Remove default routes. (Security group and NACL cannot have URLs in the rules nor does the route) Implement security groups and configure outbound rules to only permit traffic to software depots. Move all your instances into private VPC subnets remove default routes from all routing tables and add specific routes to the software depots and distributions only. Implement network access control lists to all specific destinations, with an Implicit deny as a rule. You have an EC2 Security Group with several running EC2 instances. You change the Security Group rules to allow inbound traffic on a new port and protocol, and launch several new instances in the same Security Group. The new rules apply: Immediately to all instances in the security group . Immediately to the new instances only. Immediately to the new instances, but old instances must be stopped and restarted before the new rules apply. To all instances, but it may take several minutes for old instances to see the changes. 11\u3001VPC Flow logs VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in the VPC and can help in monitoring the traffic or troubleshooting any connectivity issues Flow log data is stored using Amazon CloudWatch Logs Flow log can be created for the entire VPC , subnets or each network interface. If enabled, for entire VPC or subnet all the network interfaces are monitored Flow logs do not capture real-time log streams for network interfaces . Flow logs can be created for network interfaces that are created by other AWS services; for example, Elastic Load Balancing, RDS, ElastiCache, Redshift, and WorkSpaces 12 Subnets Subnet spans a single Availability Zone, distinct locations engineered to be isolated from failures in other AZs, and cannot span across AZs Subnet can be configured with an Internet gateway to enable communication over the Internet , or virtual private gateway (VPN) connection to enable communication with your corporate network Subnet can be Public or Private and it depends on whether it has Internet connectivity i.e. is able to route traffic to the Internet through the IGW Instances within the Public Subnet should be assigned a Public IP or Elastic IP address to be able to communicate with the Internet For Subnets not connected to the Internet, but has traffic routed through Virtual Private Gateway only is termed as VPN-only subnet Subnets can be configured to Enable assignment of the Public IP address to all the Instances launched within the Subnet by default , which can be overridden during the creation of the Instance 12-1 Subnet Sizing CIDR block assigned to the Subnet can be the same as the VPC CIDR, in this case you can launch only one subnet within your VPC CIDR block assigned to the Subnet can be a subset of the VPC CIDR, which allows you to launch multiple subnets within the VPC CIDR block assigned to the subnet should not be overlapping CIDR block size allowed is between /28 netmask (minimum with 2^4 \u2013 16 available IP address) and /16 netmask (maximum with 2^16 \u2013 65536 IP address) AWS reserves 5 IPs address (first 4 and last 1 IP address) in each Subnet which are not available for use and cannot be assigned to an instance. for e.g. for a Subnet with a CIDR block 10.0.0.0/24 the following five IPs are reserved 10.0.0.0 : Network address 10.0.0.1 : Reserved by AWS for the VPC router 10.0.0.2 : Reserved by AWS for mapping to Amazon-provided DNS 10.0.0.3 : Reserved by AWS for future use 10.0.0.255 : Network broadcast address. AWS does not support broadcast in a VPC, therefore the address is reserved. 12-2 Subnet Routing Each Subnet is associated with a route table which controls the traffic. 12-3 Subnet Security Subnet security can be configured using Security groups and NACLs Security groups works at instance level, NACLs work at the subnet level 13 Shared VPCs VPC sharing allows multiple AWS accounts to create their application resources , such as EC2 instances, RDS databases, Redshift clusters, and AWS Lambda functions, into shared, centrally-managed VPCs. In this model, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations . After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them . Participants cannot view, modify, or delete resources that belong to other participants or the VPC owner. 14 AWS VPC Endpoints 14-1 VPC Endpoints Overview VPC endpoint enables creation of a private connection between VPC to supported AWS services and VPC endpoint services powered by PrivateLink using its private IP address VPC Endpoint does not require a public IP address , access over the Internet, NAT device, a VPN connection or AWS Direct Connect Traffic between VPC and AWS service does not leave the Amazon network E ndpoints are virtual devices , that are horizontally scaled, redundant, and highly available VPC components that allow communication between instances in the VPC and AWS services without imposing availability risks or bandwidth constraints on your network traffic. Endpoints currently do not support cross-region requests, ensure that the endpoint is created in the same region as your bucket AWS currently supports two types of Endpoints VPC Interface Endpoints VPC Gateway Endpoints VPC Endpoint policy is an IAM resource policy attached to an endpoint for controlling access from the endpoint to the specified service.. Endpoint policy, by default, allows full access to the service. Endpoint policy does not override or replace IAM user policies or service-specific policies (such as S3 bucket policies). 14-2 VPC Gateway Endpoints A VPC Gateway Endpoint is a gateway that is a target for a specified route in the route table , used for traffic destined to a supported AWS service. VPC Gateway Endpoint currently supports S3 and DynamoDB services 14-3 Configuration Endpoint requires the VPC and the service to be accessed via the endpoint Endpoint needs to be associated with the Route table and the route table cannot be modified to remove the route entry. It can only be deleted by removing the Endpoint association with the Route table A route is automatically added to the Route table with a destination that specifies the prefix list of service and the target with the endpoint id . for e.g. A rule with destination pl-68a54001 (com.amazonaws.us-west-2.s3) and a target with this endpoints\u2019 ID (e.g. vpce-12345678) will be added to the route tables Access to the resources in other services can be controlled by endpoint policies Security groups needs to be modified to allow Outbound traffic from the VPC to the service thats specified in the endpoint . Use the service prefix list ID for e.g. com.amazonaws.us-east-1.s3 as the destination in the outbound rule Multiple endpoints can be created in a single VPC, for e.g., to multiple services. Multiple endpoints can be created for a single service, and different route tables used to enforce different access policies from different subnets to the same service. Multiple endpoints to the same service CANNOT be specified in a single route table 14-2 Limitations Endpoint cannot be created between a VPC and an AWS service in a different region . Endpoints support IPv4 traffic only . Endpoint cannot be transferred from one VPC to another, or from one service to another Endpoint connections cannot be extended out of a VPC i.e. resources across the VPN connection, VPC peering connection, AWS Direct Connect connection cannot use the endpoint 14-3 VPC Interface Endpoints VPC Interface endpoint enables connectivity to services powered by AWS PrivateLink. Services include some AWS services for e.g. CloudTrail, CloudWatch etc., services hosted by other AWS customers and partners in their own VPCs (referred to as endpoint services) , and supported AWS Marketplace partner services. 14-4 Limitations For each interface endpoint, only one subnet per Availability Zone can be selected. Each interface endpoint can support a bandwidth of up to 10 Gbps per Availability Zone by default. Additional capacity may be added automatically based on your usage. Network ACL for the subnet can restrict traffic, and needs to be configured properly Interface Endpoint supports TCP traffic only . Endpoints are supported within the same region only . Endpoints support IPv4 traffic only. Endpoints cannot be transferred from one VPC to another, or from one service to another . 14-5 AWS VPC Endpoints Exam You have an application running on an Amazon EC2 instance that uploads 10 GB video objects to amazon S3. Video uploads are taking longer than expected inspite of using multipart upload cause of internet bandwidth, resulting in poor application performance. Which action can help improve the upload performance? Apply an Amazon S3 bucket policy Use Amazon EBS provisioned IOPS Use VPC endpoints for S3 Request a service limit increase What are the services supported by VPC endpoints, using Gateway endpoint type? Choose 2 answers Amazon S3 Amazon EFS Amazon DynamoDB Amazon Glacier Amazon SQS What are the different types of endpoint types supported by VPC endpoints? Choose 2 Answers Gateway Classic Interface Virtual Network An application running on EC2 instances processes sensitive information stored on Amazon S3. The information is accessed over the Internet. The security team is concerned that the Internet connectivity to Amazon S3 is a security risk. Which solution will resolve the security concern? Access the data through an Internet Gateway. Access the data through a VPN connection. Access the data through a NAT Gateway. Access the data through a VPC endpoint for Amazon S3 . You need to design a VPC for a three-tier architecture, web-application consisting of an Elastic Load Balancer (ELB), a fleet of web/application servers, and backend consisting of an RDS database. The entire Infrastructure must be distributed over 2 availability zones. Which VPC configuration works while assuring the least components are exposed to Internet? Two public subnets for ELB, two private subnets for the web-servers, two private subnets for RDS and DynamoDB Two public subnets for ELB and web-servers, two private subnets for RDS and DynamoDB Two public subnets for ELB, two private subnets for the web-servers, two private subnets for RDS and VPC Endpoints for DynamoDB Two public subnets for ELB and web-servers, two private subnets for RDS and VPC Endpoints for DynamoDB 15 AWS VPC VPN \u2013 CloudHub Connections VPC VPN connections are used to extend on-premise data centers to AWS VPC VPN connections provide secure IPSec connections from on-premise computers/services to AWS 15-1 VPC VPN Connections AWS hardware VPN Connectivity can be established by creating an IPSec, hardware VPN connection between the VPC and the remote network . On the AWS side of the VPN connection, a Virtual Private Gateway (VGW) provides two VPN endpoints for automatic failover . On customer side a customer gateway (CGW) needs to be configured, which is the physical device or software application on the remote side of the VPN connection AWS Direct Connect AWS Direct Connect provides a dedicated p rivate connection from a remote network to your VPC . Direct Connect can be combined with an AWS hardware VPN connection to create an IPsec-encrypted connection AWS VPN CloudHub For more than one remote network for e.g. multiple branch offices , multiple AWS hardware VPN connections can be created via the VPC to enable communication between these networks Software VPN VPN connection can be setup by running a software VPN like OpenVPN appliance on an EC2 instance in the VPC AWS does not provide or maintain software VPN appliances ; however, there are range of products provided by partners and open source communities 15-2 Hardware VPN Connection 15-3 VPN Components Virtual Private Gateway \u2013 VGW A virtual private gateway is the VPN concentrator on the AWS side of the VPN connection Customer Gateway \u2013 CGW A customer gateway is a physical device or software application on customer side of the VPN connection. When a VPN connection is created, the VPN tunnel comes up when traffic is generated from the remote side of the VPN connection . VGW is not the initiator; CGW must initiate the tunnels If the VPN connection experiences a period of idle time, usually 10 seconds, depending on the configuration, the tunnel may go dow n. To prevent this, a network monitoring tool to generate keepalive pings; for e.g. by using IP SLA. 15-4 VPN Configuration VPC has an attached virtual private gateway, and the remote network includes a customer gateway, which must be configured to enable the VPN connection. Routing must be setup so that any traffic from the VPC bound for the remote network is routed to the virtual private gateway. Each VPN has two tunnels associated with it that can be configured on the customer router, as is not single point of failure M ultiple VPN connections to a single VPC can be created, and a second CGW can be configured to create a redundant connection to the same external location or to create VPN connections to multiple geographic locations. 15-5 VPN Routing Options For a VPN connection, the route table for the subnets s hould be updated with the type of routing (static of dynamic) that you plan to use . Route tables determine where network traffic is directed. Traffic destined for the VPN connections must be routed to the virtual private gateway. Type of routing can depend on the make and model of your VPN devices . Static Routing If your device does not support BGP, specify static routing . Using static routing, the routes (IP prefixes) can be specified that should be communicated to the virtual private gateway. Devices that don\u2019t support BGP may also perform health checks to assist failover to the second tunnel when needed. BGP dynamic routing If the VPN device supports Border Gateway Protocol (BGP) , specify dynamic routing with the VPN connection. When using a BGP device, static routes need not be specified to the VPN connection because the device uses BGP for auto discovery and to advertise its routes to the virtual private gateway. BGP-capable devices are recommended as the BGP protocol offers robust liveness detection checks that can assist failover to the second VPN tunnel if the first tunnel goes down. Only IP prefixes known to the virtual private gateway, either through BGP advertisement or static route entry, can receive traffic from your VPC. Virtual private gateway does not route any other traffic destined outside of the advertised BGP, static route entries, or its attached VPC CIDR. Only IP prefixes known to the virtual private gateway, either through BGP advertisement or static route entry, can receive traffic from your VPC . Virtual private gateway does not route any other traffic destined outside of the advertised BGP, static route entries, or its attached VPC CIDR. A VPN connection is used to connect the customer network to a VPC. Each VPN connection has two tunnels to help ensure connectivity in case one of the VPN connections becomes unavailable, with each tunnel using a unique virtual private gateway public IP address . Both tunnels should be configured for redundancy . When one tunnel becomes unavailable, for e.g. down for maintenance, network traffic is automatically routed to the available tunnel for that specific VPN connection. To protect against a loss of connectivity in case the customer gateway becomes unavailable, a second VPN connection can be setup to the VPC and virtual private gateway by using a second customer gateway. Customer gateway IP address for the second VPN connection must be publicly accessibl e. By using redundant VPN connections and CGWs, maintenance on one of the customer gateways can be performed while traffic continues to flow over the second customer gateway\u2019s VPN connection. Dynamically routed VPN connections using the Border Gateway Protocol (BGP) are recommended, if available, to exchange routing information between the customer gateways and the virtual private gateways . Statically routed VPN connections require static routes for the network to be entered on the customer gateway side . BGP-advertised and statically entered route information allow gateways on both sides to determine which tunnels are available and reroute traffic if a failure occurs. 15-6 VPN CloudHub VPN CloudHub can be used to provide secure communication between sites, if you have multiple VPN connections VPN CloudHub operates on a simple hub-and-spoke model that can be used with or without a VPC . Design is suitable for customers with multiple branch offices and existing Internet connections who\u2019d like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices VPN CloudHub architecture with blue dashed lines indicates network traffic between remote sites being routed over their VPN connections. A WS VPN CloudHub requires a virtual private gateway with multiple customer gateways . Each customer gateway must use a unique Border Gateway Protocol (BGP) Autonomous System Number (ASN) Customer gateways advertise the appropriate routes (BGP prefixes) over their VPN connections. Routing advertisements are received and re-advertised to each BGP peer, enabling each site to send data to and receive data from the other sites. Routes for each spoke must have unique ASNs and the sites must not have overlapping IP ranges. Each site can also send and receive data from the VPC as if they were using a standard VPN connection . Sites that use AWS Direct Connect connections to the virtual private gateway can also be part of the AWS VPN CloudHub. To configure the AWS VPN CloudHub, multiple customer gateways can be created, each with the unique public IP address of the gateway and the ASN. a VPN connection can be created from each customer gateway to a common virtual private gateway. each VPN connection must advertise its specific BGP routes. This is done using the network statements in the VPN configuration files for the VPN connection. 15-7 VPC VPN \u2013 CloudHub Connections You have in total 5 offices, and the entire employee related information is stored under AWS VPC instances. Now all the offices want to connect the instances in VPC using VPN. Which of the below help you to implement this? you can have redundant customer gateways between your data center and your VPC you can have multiple locations connected to the AWS VPN CloudHub You have to define 5 different static IP addresses in route table. 1 and 2 1,2 and 3 You have in total 15 offices, and the entire employee related information is stored under AWS VPC instances. Now all the offices want to connect the instances in VPC using VPN. What problem do you see in this scenario? You can not create more than 1 VPN connections with single VPC ( Can be created ) You can not create more than 10 VPN connections with single VPC ( soft limit can be extended ) When you create multiple VPN connections, the virtual private gateway can not sends network traffic to the appropriate VPN connection using statically assigned routes. ( Can route the traffic to correct connection ) Statically assigned routes cannot be configured in case of more than 1 VPN with virtual private gateway. ( can be configured ) None of above You have been asked to virtually extend two existing data centers into AWS to support a highly available application that depends on existing, on-premises resources located in multiple data centers and static content that is served from an Amazon Simple Storage Service (S3) bucket. Your design currently includes a dual-tunnel VPN connection between your CGW and VGW. Which component of your architecture represents a potential single point of failure that you should consider changing to make the solution more highly available? Add another VGW in a different Availability Zone and create another dual-tunnel VPN connection. Add another CGW in a different data center and create another dual-tunnel VPN connection . Add a second VGW in a different Availability Zone, and a CGW in a different data center, and create another dual-tunnel. No changes are necessary: the network architecture is currently highly available. You are designing network connectivity for your fat client application. The application is designed for business travelers who must be able to connect to it from their hotel rooms, cafes, public Wi-Fi hotspots, and elsewhere on the Internet. You do not want to publish the application on the Internet. Which network design meets the above requirements while minimizing deployment and operational costs? [PROFESSIONAL] Implement AWS Direct Connect, and create a private interface to your VPC. Create a public subnet and place your application servers in it. (High Cost and does not minimize deployment) Implement Elastic Load Balancing with an SSL listener that terminates the back-end connection to the application. (Needs to be published to internet) Configure an IPsec VPN connection, and provide the users with the configuration details. Create a public subnet in your VPC, and place your application servers in it. (Instances still in public subnet are internet accessible) Configure an SSL VPN solution in a public subnet of your VPC, then install and configure SSL VPN client software on all user computers. Create a private subnet in your VPC and place your application servers in it. (Cost effective and can be in private subnet as well) You are designing a connectivity solution between on-premises infrastructure and Amazon VPC Your server\u2019s on-premises will De communicating with your VPC instances You will De establishing IPSec tunnels over the internet You will be using VPN gateways and terminating the IPsec tunnels on AWS-supported customer gateways. Which of the following objectives would you achieve by implementing an IPSec tunnel as outlined above? (Choose 4 answers) [PROFESSIONAL] End-to-end protection of data in transit End-to-end Identity authentication Data encryption across the Internet Protection of data in transit over the Internet Peer identity authentication between VPN gateway and customer gateway Data integrity protection across the Internet A development team that is currently doing a nightly six-hour build which is lengthening over time on-premises with a large and mostly under utilized server would like to transition to a continuous integration model of development on AWS with multiple builds triggered within the same day. However, they are concerned about cost, security and how to integrate with existing on-premises applications such as their LDAP and email servers, which cannot move off-premises. The development environment needs a source code repository; a project management system with a MySQL database resources for performing the builds and a storage location for QA to pick up builds from. What AWS services combination would you recommend to meet the development team\u2019s requirements? [PROFESSIONAL] A Bastion host Amazon EC2 instance running a VPN server for access from on-premises, Amazon EC2 for the source code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIP for the source code repository and project management system, Amazon SQL for a build queue, An Amazon Auto Scaling group of Amazon EC2 instances for performing builds and Amazon Simple Email Service for sending the build output. (Bastion is not for VPN connectivity also SES should not be used) An AWS Storage Gateway for connecting on-premises software applications with cloud-based storage securely, Amazon EC2 for the resource code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIPs for the source code repository and project management system, Amazon Simple Notification Service for a notification initiated build, An Auto Scaling group of Amazon EC2 instances for performing builds and Amazon S3 for the build output. (Storage Gateway does provide secure connectivity but still needs VPN. SNS alone cannot handle builds) An AWS Storage Gateway for connecting on-premises software applications with cloud-based storage securely, Amazon EC2 for the resource code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIPs for the source code repository and project management system, Amazon SQS for a build queue, An Amazon Elastic Map Reduce (EMR) cluster of Amazon EC2 instances for performing builds and Amazon CloudFront for the build output. (Storage Gateway does not provide secure connectivity, still needs VPN. EMR is not ideal for performing builds as it needs normal EC2 instances) A VPC with a VPN Gateway back to their on-premises servers, Amazon EC2 for the source-code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIPs for the source code repository and project management system, SQS for a build queue, An Auto Scaling group of EC2 instances for performing builds and S3 for the build output. (VPN gateway is required for secure connectivity. SQS for build queue and EC2 for builds) 16 VPC Exam You have a business-to-business web application running in a VPC consisting of an Elastic Load Balancer (ELB), web servers, application servers and a database. Your web application should only accept traffic from predefined customer IP addresses. Which two options meet this security requirement? Choose 2 answers Configure web server VPC security groups to allow traffic from your customers\u2019 IPs ( Web server is behind the ELB and customer IPs will never reach web servers ) Configure your web servers to filter traffic based on the ELB\u2019s \u201cX-forwarded-for\u201d header (get the customer IPs and create a custom filter to restrict access.) C onfigure ELB security groups to allow traffic from your customers\u2019 IPs and deny all outbound traffic (ELB will see the customer IPs so can restrict access, deny all is basically have no rules in outbound traffic, implicit, and its stateful so would work) Configure a VPC NACL to allow web traffic from your customers\u2019 IPs and deny all outbound traffic ( NACL is stateless, deny all will not work ) A user has created a VPC with public and private subnets using the VPC Wizard. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.0.0/24. Which of the below mentioned entries are required in the main route table to allow the instances in VPC to communicate with each other? Destination : 20.0.0.0/24 and Target : VPC Destination : 20.0.0.0/16 and Target : ALL Destination : 20.0.0.0/0 and Target : ALL Destination : 20.0.0.0/16 and Target : Local A user has created a VPC with two subnets: one public and one private. The user is planning to run the patch update for the instances in the private subnet. How can the instances in the private subnet connect to the internet? Use the internet gateway with a private IP Allow outbound traffic in the security group for port 80 to allow internet updates The private subnet can never connect to the internet Use NAT with an elastic IP A user has launched an EC2 instance and installed a website with the Apache webserver. The webserver is running but the user is not able to access the website from the Internet. What can be the possible reason for this failure? The security group of the instance is not configured properly . The instance is not configured with the proper key-pairs. The Apache website cannot be accessed from the Internet. Instance is not configured with an elastic IP. A user has created a VPC with public and private subnets using the VPC wizard. Which of the below mentioned statements is true in this scenario? AWS VPC will automatically create a NAT instance with the micro size VPC bounds the main route table with a private subnet and a custom route table with a public subnet User has to manually create a NAT instance VPC bounds the main route table with a public subnet and a custom route table with a private subnet A user has created a VPC with public and private subnets. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.1.0/24 and the public subnet uses CIDR 20.0.0.0/24. The user is planning to host a web server in the public subnet (port 80) and a DB server in the private subnet (port 3306). The user is configuring a security group of the NAT instance. Which of the below mentioned entries is not required for the NAT security group? For Inbound allow Source: 20.0.1.0/24 on port 80 For Outbound allow Destination: 0.0.0.0/0 on port 80 For Inbound allow Source: 20.0.0.0/24 on port 80 For Outbound allow Destination: 0.0.0.0/0 on port 443 A user has created a VPC with CIDR 20.0.0.0/24. The user has used all the IPs of CIDR and wants to increase the size of the VPC. The user has two subnets: public (20.0.0.0/25) and private (20.0.0.128/25). How can the user change the size of the VPC? The user can delete all the instances of the subnet. Change the size of the subnets to 20.0.0.0/32 and 20.0.1.0/32, respectively. Then the user can increase the size of the VPC using CLI It is not possible to change the size of the VPC once it has been created ( NOTE \u2013 You can now increase the VPC size ) User can add a subnet with a higher range so that it will automatically increase the size of the VPC User can delete the subnets first and then modify the size of the VPC A user has created a VPC with the public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The public subnet uses CIDR 20.0.1.0/24. The user is planning to host a web server in the public subnet (port 80) and a DB server in the private subnet (port 3306). The user is configuring a security group for the public subnet (WebSecGrp) and the private subnet (DBSecGrp). Which of the below mentioned entries is required in the web server security group (WebSecGrp)? Configure Destination as DB Security group ID (DbSecGrp) for port 3306 Outbound Configure port 80 for Destination 0.0.0.0/0 Outbound Configure port 3306 for source 20.0.0.0/24 InBound Configure port 80 InBound for source 20.0.0.0/16 A user has created a VPC with CIDR 20.0.0.0/16. The user has created one subnet with CIDR 20.0.0.0/16 by mistake. The user is trying to create another subnet of CIDR 20.0.0.1/24. How can the user create the second subnet? There is no need to update the subnet as VPC automatically adjusts the CIDR of the first subnet based on the second subnet\u2019s CIDR The user can modify the first subnet CIDR from the console It is not possible to create a second subnet as one subnet with the same CIDR as the VPC has been created The user can modify the first subnet CIDR with AWS CLI A user has setup a VPC with CIDR 20.0.0.0/16. The VPC has a private subnet (20.0.1.0/24) and a public subnet (20.0.0.0/24). The user\u2019s data centre has CIDR of 20.0.54.0/24 and 20.1.0.0/24. If the private subnet wants to communicate with the data centre, what will happen? It will allow traffic communication on both the CIDRs of the data centre It will not allow traffic with data centre on CIDR 20.1.0.0/24 but allows traffic communication on 20.0.54.0/24 It will not allow traffic communication on any of the data centre CIDRs It will allow traffic with data centre on CIDR 20.1.0.0/24 but does not allow on 20.0.54.0/24 (as the CIDR block would be overlapping) A user has created a VPC with public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.0.0/24 . The NAT instance ID is i-a12345. Which of the below mentioned entries are required in the main route table attached with the private subnet to allow instances to connect with the internet? Destination: 0.0.0.0/0 and Target: i-a12345 Destination: 20.0.0.0/0 and Target: 80 Destination: 20.0.0.0/0 and Target: i-a12345 Destination: 20.0.0.0/24 and Target: i-a12345 A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created a public subnet CIDR (20.0.0.0/24) and VPN only subnets CIDR (20.0.1.0/24) along with the VPN gateway (vgw-12345) to connect to the user\u2019s data centre. The user\u2019s data centre has CIDR 172.28.0.0/12. The user has also setup a NAT instance (i-123456) to allow traffic to the internet from the VPN subnet. Which of the below mentioned options is not a valid entry for the main route table in this scenario? Destination: 20.0.1.0/24 and Target: i-12345 Destination: 0.0.0.0/0 and Target: i-12345 Destination: 172.28.0.0/12 and Target: vgw-12345 Destination: 20.0.0.0/16 and Target: local A user has created a VPC with CIDR 20.0.0.0/16. The user has created one subnet with CIDR 20.0.0.0/16 in this VPC. The user is trying to create another subnet with the same VPC for CIDR 20.0.0.1/24. What will happen in this scenario? The VPC will modify the first subnet CIDR automatically to allow the second subnet IP range It is not possible to create a subnet with the same CIDR as VPC The second subnet will be created It will throw a CIDR overlaps error A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created both Public and VPN-Only subnets along with hardware VPN access to connect to the user\u2019s data centre. The user has not yet launched any instance as well as modified or deleted any setup. He wants to delete this VPC from the console. Will the console allow the user to delete the VPC? Yes, the console will delete all the setups and also delete the virtual private gateway No, the console will ask the user to manually detach the virtual private gateway first and then allow deleting the VPC Yes, the console will delete all the setups and detach the virtual private gateway No, since the NAT instance is running A user has created a VPC with the public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The public subnet uses CIDR 20.0.1.0/24. The user is planning to host a web server in the public subnet (port 80) and a DB server in the private subnet (port 3306). The user is configuring a security group for the public subnet (WebSecGrp) and the private subnet (DBSecGrp). Which of the below mentioned entries is required in the private subnet database security group (DBSecGrp)? Allow Inbound on port 3306 for Source Web Server Security Group (WebSecGrp) Allow Inbound on port 3306 from source 20.0.0.0/16 Allow Outbound on port 3306 for Destination Web Server Security Group (WebSecGrp. Allow Outbound on port 80 for Destination NAT Instance IP A user has created a VPC with a subnet and a security group. The user has launched an instance in that subnet and attached a public IP. The user is still unable to connect to the instance. The internet gateway has also been created. What can be the reason for the error? The internet gateway is not configured with the route table The private IP is not present The outbound traffic on the security group is disabled The internet gateway is not configured with the security group A user has created a subnet in VPC and launched an EC2 instance within it. The user has not selected the option to assign the IP address while launching the instance. Which of the below mentioned statements is true with respect to the Instance requiring access to the Internet? The instance will always have a public DNS attached to the instance by default The user can directly attach an elastic IP to the instance The instance will never launch if the public IP is not assigned The user would need to create an internet gateway and then attach an elastic IP to the instance to connect from internet A user has created a VPC with public and private subnets using the VPC wizard. Which of the below mentioned statements is not true in this scenario? VPC will create a routing instance and attach it with a public subnet VPC will create two subnets VPC will create one internet gateway and attach it to VPC VPC will launch one NAT instance with an elastic IP A user has created a VPC with the public subnet. The user has created a security group for that VPC. Which of the below mentioned statements is true when a security group is created? It can connect to the AWS services, such as S3 and RDS by default It will have all the inbound traffic by default It will have all the outbound traffic by default It will by default allow traffic to the internet gateway A user has created a VPC with CIDR 20.0.0.0/16 using VPC Wizard. The user has created a public CIDR (20.0.0.0/24) and a VPN only subnet CIDR (20.0.1.0/24) along with the hardware VPN access to connect to the user\u2019s data centre. Which of the below mentioned components is not present when the VPC is setup with the wizard? Main route table attached with a VPN only subnet A NAT instance configured to allow the VPN subnet instances to connect with the internet Custom route table attached with a public subnet An internet gateway for a public subnet A user has created a VPC with public and private subnets using the VPC wizard. The user has not launched any instance manually and is trying to delete the VPC. What will happen in this scenario? It will not allow to delete the VPC as it has subnets with route tables It will not allow to delete the VPC since it has a running route instance It will terminate the VPC along with all the instances launched by the wizard It will not allow to delete the VPC since it has a running NAT instance A user has created a public subnet with VPC and launched an EC2 instance within it. The user is trying to delete the subnet. What will happen in this scenario? It will delete the subnet and make the EC2 instance as a part of the default subnet It will not allow the user to delete the subnet until the instances are terminated It will delete the subnet as well as terminate the instances Subnet can never be deleted independently, but the user has to delete the VPC first A user has created a VPC with CIDR 20.0.0.0/24. The user has created a public subnet with CIDR 20.0.0.0/25 and a private subnet with CIDR 20.0.0.128/25. The user has launched one instance each in the private and public subnets. Which of the below mentioned options cannot be the correct IP address (private IP) assigned to an instance in the public or private subnet? 20.0.0.255 20.0.0.132 20.0.0.122 20.0.0.55 A user has created a VPC with CIDR 20.0.0.0/16. The user has created public and VPN only subnets along with hardware VPN access to connect to the user\u2019s datacenter. The user wants to make so that all traffic coming to the public subnet follows the organization\u2019s proxy policy. How can the user make this happen? Setting up a NAT with the proxy protocol and configure that the public subnet receives traffic from NAT Setting up a proxy policy in the internet gateway connected with the public subnet It is not possible to setup the proxy policy for a public subnet Setting the route table and security group of the public subnet which receives traffic from a virtual private gateway A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created a public subnet CIDR (20.0.0.0/24) and VPN only subnets CIDR (20.0.1.0/24) along with the VPN gateway (vgw-12345) to connect to the user\u2019s data centre. Which of the below mentioned options is a valid entry for the main route table in this scenario? Destination: 20.0.0.0/24 and Target: vgw-12345 Destination: 20.0.0.0/16 and Target: ALL Destination: 20.0.1.0/16 and Target: vgw-12345 Destination: 0.0.0.0/0 and Target: vgw-12345 Which two components provide connectivity with external networks? When attached to an Amazon VPC which two components provide connectivity with external networks? Choose 2 answers Elastic IPs (EIP) ( Does not provide connectivity, public IP address will do as well ) NAT Gateway (NAT) ( Not Attached to VPC and still needs IGW ) Internet Gateway (IGW) Virtual Private Gateway (VGW) You are attempting to connect to an instance in Amazon VPC without success You have already verified that the VPC has an Internet Gateway (IGW) the instance has an associated Elastic IP (EIP) and correct security group rules are in place. Which VPC component should you evaluate next? The configuration of a NAT instance The configuration of the Routing Table The configuration of the internet Gateway (IGW) The configuration of SRC/DST checking If you want to launch Amazon Elastic Compute Cloud (EC2) Instances and assign each Instance a predetermined private IP address you should: Assign a group or sequential Elastic IP address to the instances Launch the instances in a Placement Group Launch the instances in the Amazon virtual Private Cloud (VPC) Use standard EC2 instances since each instance gets a private Domain Name Service (DNS) already Launch the Instance from a private Amazon Machine image (AMI) A user has recently started using EC2. The user launched one EC2 instance in the default subnet in EC2-VPC Which of the below mentioned options is not attached or available with the EC2 instance when it is launched? Public IP address Internet gateway Elastic IP Private IP address A user has created a VPC with CIDR 20.0.0.0/24. The user has created a public subnet with CIDR 20.0.0.0/25. The user is trying to create the private subnet with CIDR 20.0.0.128/25. Which of the below mentioned statements is true in this scenario? It will not allow the user to create the private subnet due to a CIDR overlap It will allow the user to create a private subnet with CIDR as 20.0.0.128/25 This statement is wrong as AWS does not allow CIDR 20.0.0.0/25 It will not allow the user to create a private subnet due to a wrong CIDR range A user has created a VPC with CIDR 20.0.0.0/16 with only a private subnet and VPN connection using the VPC wizard. The user wants to connect to the instance in a private subnet over SSH. How should the user define the security rule for SSH? Allow Inbound traffic on port 22 from the user\u2019s network The user has to create an instance in EC2 Classic with an elastic IP and configure the security group of a private subnet to allow SSH from that elastic IP The user can connect to a instance in a private subnet using the NAT instance Allow Inbound traffic on port 80 and 22 to allow the user to connect to a private subnet over the Internet A company wants to implement their website in a virtual private cloud (VPC). The web tier will use an Auto Scaling group across multiple Availability Zones (AZs). The database will use Multi-AZ RDS MySQL and should not be publicly accessible. What is the minimum number of subnets that need to be configured in the VPC? 1 2 3 4 (2 public subnets for web instances in multiple AZs and 2 private subnets for RDS Multi-AZ) Which of the following are characteristics of Amazon VPC subnets? Choose 2 answers Each subnet maps to a single Availability Zone A CIDR block mask of /25 is the smallest range supported Instances in a private subnet can communicate with the Internet only if they have an Elastic IP. By default, all subnets can route between each other, whether they are private or public Each subnet spans at least 2 Availability zones to provide a high-availability environment You need to design a VPC for a web-application consisting of an Elastic Load Balancer (ELB). a fleet of web/application servers, and an RDS database The entire Infrastructure must be distributed over 2 availability zones. Which VPC configuration works while assuring the database is not available from the Internet? One public subnet for ELB one public subnet for the web-servers, and one private subnet for the database One public subnet for ELB two private subnets for the web-servers, two private subnets for RDS Two public subnets for ELB two private subnets for the web-servers and two private subnets for RDS Two public subnets for ELB two public subnets for the web-servers, and two public subnets for RDS You have deployed a three-tier web application in a VPC with a CIDR block of 10.0.0.0/28. You initially deploy two web servers, two application servers, two database servers and one NAT instance tor a total of seven EC2 instances. The web, application and database servers are deployed across two availability zones (AZs). You also deploy an ELB in front of the two web servers, and use Route53 for DNS Web traffic gradually increases in the first few days following the deployment, so you attempt to double the number of instances in each tier of the application to handle the new load unfortunately some of these new instances fail to launch. Which of the following could the root caused? (Choose 2 answers) [PROFESSIONAL] The Internet Gateway (IGW) of your VPC has scaled-up adding more instances to handle the traffic spike, reducing the number of available private IP addresses for new instance launches. AWS reserves one IP address in each subnet\u2019s CIDR block for Route53 so you do not have enough addresses left to launch all of the new EC2 instances. AWS reserves the first and the last private IP address in each subnet\u2019s CIDR block so you do not have enough addresses left to launch all of the new EC2 instances. The ELB has scaled-up. Adding more instances to handle the traffic reducing the number of available private IP addresses for new instance launches AWS reserves the first four and the last IP address in each subnet\u2019s CIDR block so you do not have enough addresses left to launch all of the new EC2 instances . A user wants to access RDS from an EC2 instance using IP addresses. Both RDS and EC2 are in the same region, but different AZs. Which of the below mentioned options help configure that the instance is accessed faster? Configure the Private IP of the Instance in RDS security group (Recommended as the data is transferred within the the Amazon network and not through internet \u2013 Refer link) Security group of EC2 allowed in the RDS security group Configuring the elastic IP of the instance in RDS security group Configure the Public IP of the instance in RDS security group In regards to VPC, select the correct statement: You can associate multiple subnets with the same Route Table. You can associate multiple subnets with the same Route Table, but you can\u2019t associate a subnet with only one Route Table. You can\u2019t associate multiple subnets with the same Route Table. None of these. You need to design a VPC for a web-application consisting of an ELB a fleet of web application servers, and an RDS DB. The entire infrastructure must be distributed over 2 AZ. Which VPC configuration works while assuring the DB is not available from the Internet? One Public Subnet for ELB, one Public Subnet for the web-servers, and one private subnet for the DB One Public Subnet for ELB, two Private Subnets for the web-servers, and two private subnets for the RDS Two Public Subnets for ELB, two private Subnet for the web-servers, and two private subnet for the RDS Two Public Subnets for ELB, two Public Subnet for the web-servers, and two public subnets for the RDS You have an Amazon VPC with one private subnet and one public subnet with a Network Address Translator (NAT) server. You are creating a group of Amazon Elastic Cloud Compute (EC2) instances that configure themselves at startup via downloading a bootstrapping script from Amazon Simple Storage Service (S3) that deploys an application via GIT. Which setup provides the highest level of security? Amazon EC2 instances in private subnet, no EIPs, route outgoing traffic via the NAT Amazon EC2 instances in public subnet, no EIPs, route outgoing traffic via the Internet Gateway (IGW) Amazon EC2 instances in private subnet, assign EIPs, route outgoing traffic via the Internet Gateway (IGW) Amazon EC2 instances in public subnet, assign EIPs, route outgoing traffic via the NAT You have launched an Amazon Elastic Compute Cloud (EC2) instance into a public subnet with a primary private IP address assigned, an internet gateway is attached to the VPC, and the public route table is configured to send all Internet-based traffic to the Internet gateway. The instance security group is set to allow all outbound traffic but cannot access the Internet. Why is the Internet unreachable from this instance? The instance does not have a public IP address The Internet gateway security group must allow all outbound traffic. The instance security group must allow all inbound traffic. The instance \u201cSource/Destination check\u201d property must be enabled. You have an environment that consists of a public subnet using Amazon VPC and 3 instances that are running in this subnet. These three instances can successfully communicate with other hosts on the Internet. You launch a fourth instance in the same subnet, using the same AMI and security group configuration you used for the others, but find that this instance cannot be accessed from the internet. What should you do to enable Internet access? Deploy a NAT instance into the public subnet. Assign an Elastic IP address to the fourth instance Configure a publically routable IP Address in the host OS of the fourth instance. Modify the routing table for the public subnet. You have a load balancer configured for VPC, and all back-end Amazon EC2 instances are in service. However, your web browser times out when connecting to the load balancer\u2019s DNS name. Which options are probable causes of this behavior? Choose 2 answers The load balancer was not configured to use a public subnet with an Internet gateway configured The Amazon EC2 instances do not have a dynamically allocated private IP address The security groups or network ACLs are not property configured for web traffic. The load balancer is not configured in a private subnet with a NAT instance. The VPC does not have a VGW configured. When will you incur costs with an Elastic IP address (EIP)? When an EIP is allocated. When it is allocated and associated with a running instance. When it is allocated and associated with a stopped instance. Costs are incurred regardless of whether the EIP is associated with a running instance. A company currently has a VPC with EC2 Instances. A new instance being launched, which will host an application that works on IPv6. You need to ensure that this instance can initiate outgoing traffic to the Internet. At the same time, you need to ensure that no incoming connection can be initiated from the Internet on to the instance. Which of the following would you add to the VPC for this requirement? A NAT Instance A NAT Gateway An Internet Gateway","title":"L1 AWS Virtual Private Cloud \u2013 VPC"},{"location":"chap3/1vpc/#l1-aws-virtual-private-cloud-vpc","text":"","title":"L1 AWS Virtual Private Cloud \u2013 VPC"},{"location":"chap3/1vpc/#1-vpc-overview-components","text":"A virtual private cloud (VPC) is a virtual network dedicated to the AWS account. It is logically isolated from other virtual networks in the AWS cloud. VPC allows the user to select IP address range, create subnets, and configure route tables, network gateways, and security settings.","title":"1 VPC Overview &amp; Components"},{"location":"chap3/1vpc/#1-1-vpc-sizing","text":"VPC needs a set of IP addresses in the form of a Classless Inter-Domain Routing (CIDR) block for e.g, 10.0.0.0/16 , which allows 2^16 (65536) IP address to be available Allowed CIDR block size is between /28 netmask (minimum with 2^4 \u2013 16 available IP address) and /16 netmask (maximum with 2^16 \u2013 65536 IP address) CIDR block from private (non-publicly routable) IP address can be assigned 10.0.0.0 \u2013 10.255.255.255 (10/8 prefix) 172.16.0.0 \u2013 172.31.255.255 (172.16/12 prefix) 192.168.0.0 \u2013 192.168.255.255 (192.168/16 prefix) It\u2019s possible to specify a range of publicly routable IP addresses; however, direct access to the Internet is not currently supported from publicly routable CIDR blocks in a VPC NOTE \u2013 You can now resize VPC Each VPC is separate from any other VPC created with the same CIDR block even if it resides within the same AWS account VPC allows VPC Peering connections with other VPC within the same or different AWS accounts Connection between your VPC and corporate or home network can be established , however the CIDR blocks should be not be overlapping for e.g. VPC with CIDR 10.0.0.0/16 can communicate with 10.1.0.0/16 corporate network but the connections would be dropped if it tries to connect to 10.0.37.0/16 corporate network cause of overlapping ip addresses.","title":"1-1 VPC Sizing"},{"location":"chap3/1vpc/#1-2-vpc-peering","text":"VPC allows VPC Peering connections with other VPC within the same or different AWS accounts VPC Peering Overview A VPC peering connection is a networking connection between two VPCs that enables routing of traffic between them using private IP addresses. Instances in either VPC can communicate with each other as if they are within the same network VPC peering connection can be established between your own VPCs, or with a VPC in another AWS account in a single different region. AWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is neither a gateway nor a VPN connection, and does not rely on a separate piece of physical hardware. There is no single point of failure for communication or a bandwidth bottleneck VPC Peering Rules & Limitations VPC peering connection cannot be created between VPCs that have matching or overlapping CIDR blocks. VPC Peering is now supported inter-region VPC peering connection are limited on the number active and pending VPC peering connections that you can have per VPC. VPC peering does not support transitive peering relationships . In a VPC peering connection, the VPC does not have access to any other VPCs that the peer VPC may be peered with even if established entirely within your own AWS account VPC peering does not support Edge to Edge Routing Through a Gateway or Private Connection In a VPC peering connection, the VPC does not have access to any other connection that the peer VPC may have and vice versa. Connections that the peer VPC can include A VPN connection or an AWS Direct Connect connection to a corporate network An Internet connection through an Internet gateway An Internet connection in a private subnet through a NAT device A ClassicLink connection to an EC2-Classic instance A VPC endpoint to an AWS service; for example, an endpoint to S3 . Only one VPC peering connection can be established between the same two VPCs at the same time Maximum Transmission Unit (MTU) across a VPC peering connection is 1500 bytes. A placement group can span peered VPCs that are in the same region ; however, you do not get full-bisection bandwidth between instances in peered VPCs Any tags created for the VPC peering connection are only applied in the account or region in which they were created Unicast reverse path forwarding in VPC peering connections is not supported Instance\u2019s Public DNS can now be resolved to its private IP address across peered VPCs VPC Peering Architecture VPC Peering can be applied to create shared services or perform authentication with an on-premises instance This would help creating a single point of contact, as well limiting the VPN connections to a single account or VPC Questions A company has an AWS account that contains three VPCs (Dev, Test, and Prod) in the same region. Test is peered to both Prod and Dev. All VPCs have non-overlapping CIDR blocks. The company wants to push minor code releases from Dev to Prod to speed up time to market. Which of the following options helps the company accomplish this? Create a new peering connection Between Prod and Dev along with appropriate routes. Create a new entry to Prod in the Dev route table using the peering connection as the target. Attach a second gateway to Dev. Add a new entry in the Prod route table identifying the gateway as the target. The VPCs have non-overlapping CIDR blocks in the same account. The route tables contain local routes for all VPCs. VPC allows you to set tenancy option for the Instances launched in it . By default, the tenancy option is shared. If dedicated option selected, all the instances within it are launched on a dedicated hardware overriding the individual instance tenancy setting Deletion of the VPC is possible only after terminating all instances within the VPC, and deleting all the components with the VPC for e.g. subnets, security groups, network ACLs, route tables, Internet gateways, VPC peering connections, and DHCP options","title":"1-2 VPC Peering"},{"location":"chap3/1vpc/#2-ip-addresses","text":"Instances launched in the VPC can have Private, Public and Elastic IP address assigned to it and are properties of ENI (Network Interfaces) Private IP Addresses Private IP addresses are not reachable over the Internet, and can be used for communication only between the instances within the VPC All instances are assigned a private IP address , within the IP address range of the subnet, to the default network interface Primary IP address is associated with the network interface for its lifetime, even when the instance is stopped and restarted and is released only when the instance is terminated Additional Private IP addresses, known as secondary private IP address, can be assigned to the instances and these can be reassigned from one network interface to another Public IP address Public IP addresses are reachable over the Internet , and can be used for communication between instances and the Internet, or with other AWS services that have public endpoints Public IP address assignment to the Instance depends if the Public IP Addressing is enabled for the Subnet . Public IP address can also be assigned to the Instance by enabling the Public IP addressing during the creation of the instance , which overrides the subnet\u2019s public IP addressing attribute Public IP address is assigned from AWS pool of IP addresses and it is not associated with the AWS account and hence is released when the instance is stopped and restarted or terminated. Elastic IP address Elastic IP addresses are static, persistent public IP addresses which can be associated and disassociated with the instance, as required Elastic IP address is allocated at an VPC and owned by the account unless released A Network Interface can be assigned either a Public IP or an Elastic IP . If you assign an instance, already having an Public IP, an Elastic IP, the public IP is released Elastic IP addresses can be moved from one instance to another , which can be within the same or different VPC within the same account Elastic IP are charged for non usage i.e. if it is not associated or associated with a stopped instance or an unattached Network Interface","title":"2 IP Addresses"},{"location":"chap3/1vpc/#3elastic-network-interface-eni","text":"Each Instance is attached with default elastic network interface ( Primary Network Interface eth0 ) and cannot be detached from the instance ENI can include the following attributes Primary private IP address One or more secondary private IP addresses One Elastic IP address per private IP address One public IP address, which can be auto-assigned to the network interface for eth0 when you launch an instance, but only when you create a network interface for eth0 instead of using an existing ENI One or more security groups A MAC address A source/destination check flag A description ENI\u2019s attributes follow the ENI as it is attached or detached from an instance and reattached to another instance . When an ENI is moved from one instance to another, network traffic is redirected to the new instance. Multiple ENIs can be attached to an instance and is useful for use cases: Create a management network. Use network and security appliances in your VPC. Create dual-homed instances with workloads/roles on distinct subnets. Create a low-budget, high-availability solution.","title":"3\u3001Elastic Network Interface (ENI)"},{"location":"chap3/1vpc/#4-route-tables","text":"Route table defines rules, termed as routes , which determine where network traffic from the subnet would be routed Each VPC has a implicit router to route network traffic Each VPC has a Main Route table, and can have multiple custom route tables created Each Subnet within a VPC must be associated with a single route table at a time, while a route table can have multiple subnets associated with it Subnet, if not explicitly associated to a route table, is implicitly associated with the main route table Every route table contains a local route that enables communication within a VPC which cannot be modified or deleted Route priority is decided by matching the most specific route in the route table that matches the traffic Route tables needs to be updated to defined routes for Internet gateways , Virtual Private gateways, VPC Peering, VPC Endpoints, NAT Device etc.","title":"4 Route Tables"},{"location":"chap3/1vpc/#5-internet-gateways-igw","text":"An Internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in the VPC and the Internet . IGW imposes no availability risks or bandwidth constraints on the network traffic. An Internet gateway serves two purposes: To provide a target in the VPC route tables for Internet-routable traffic, To perform network address translation (NAT) for instances that have been NOT been assigned public IP addresses .","title":"5 Internet Gateways \u2013 IGW"},{"location":"chap3/1vpc/#5-1-enabling-internet-access-to-an-instance-requires","text":"Attaching Internet gateway to the VPC Subnet should have route tables associated with the route pointing to the Internet gateway Instances should have a Public IP or Elastic IP address assigned Security groups and NACLs associated with the Instance should allow relevant traffic","title":"5-1 Enabling Internet access to an Instance requires"},{"location":"chap3/1vpc/#6-nat","text":"NAT device enables instances in a private subnet to connect to the Internet or other AWS services , but prevents the Internet from initiating connections with the instances. NAT devices do not support IPv6 traffic, use an egress-only Internet gateway instead .","title":"6 NAT"},{"location":"chap3/1vpc/#6-1-nat-overview","text":"Network Address Translation (NAT) devices, launched in the public subnet, enables instances in a private subnet to connect to the Internet, but prevents the Internet from initiating connections with the instances. Instances in private subnets would need internet connection for performing software updates or trying to access external services NAT device performs the function of both address translation and port address translation (PAT) NAT instance prevents instances to be directly exposed to the Internet and having to be launched in Public subnet and assignment of the Elastic IP address to all, which are limited. NAT device routes the traffic, from the private subnet to the Internet, by replacing the source IP address with its address and for the response traffic it translates the address back to the instances\u2019 private IP addresses. AWS allows NAT configuration in 2 ways NAT Instance NAT Gateway, managed service by AWS","title":"6-1 NAT Overview"},{"location":"chap3/1vpc/#6-2-nat-device-configuration-key-points","text":"needs to be launched in the Public Subnet needs to be associated with an Elastic IP address (or public IP address) should have the Source/Destination flag disabled to route traffic from the instances in the private subnet to the Internet and send the response back should have a Security group associated that allows Outbound Internet traffic from instances in the private subnet disallows Inbound Internet traffic from everywhere Instances in the private subnet should have the Route table configured to direct all Internet traffic to the NAT device","title":"6-2 NAT device Configuration Key Points"},{"location":"chap3/1vpc/#6-3-nat-gateway","text":"NAT gateway is a AWS managed NAT service that provides better availability, higher bandwidth, and requires less administrative effort. A NAT gateway supports bursts of up to 10 Gbps of bandwidth . For over 10 Gbps bursts requirement, the workload can be distributed by splitting the resources into multiple subnets, and creating a NAT gateway in each subnet NAT gateway is associated with One Elastic IP address which cannot be disassociated after it\u2019s creation. Each NAT gateway is created in a specific Availability Zone and implemented with redundancy in that zone . A NAT gateway supports the following protocols: TCP, UDP, and ICMP . NAT gateway cannot be associated a security group. Security can be configured for the instances in the private subnets to control the traffic Network ACL can be used to control the traffic to and from the subnet. NACL applies to the NAT gateway\u2019s traffic, which uses ports 1024-65535 NAT gateway when created receives an elastic network interface that\u2019s automatically assigned a private IP address from the IP address range of the subnet. Attributes of this network interface cannot be modified NAT gateway cannot send traffic over VPC endpoints, VPN connections, AWS Direct Connect, or VPC peering connections . Private subnet\u2019s route table should be modified to route the traffic directly to these devices.","title":"6-3 NAT Gateway"},{"location":"chap3/1vpc/#6-4-nat-instance","text":"NAT instance can be created by using Amazon Linux AMIs configured to route traffic to Internet. They do not provide the same availability and bandwidth and need to configured as per the application needs. NAT instances must have security groups associated with Inbound traffic enabled from private subnets and Outbound traffic enabled to the Internet NAT instances should have the Source Destination Check attribute disabled, as it is neither the source nor the destination for the traffic and merely acts as a gateway","title":"6-4 NAT Instance"},{"location":"chap3/1vpc/#6-5-high-availability-nat-instance","text":"Create One NAT instance per Availability Zone Configure all Private subnet route tables to the same zone NAT instance Use Auto Scaling for NAT availability Use Auto Scaling group per NAT instance with min and max size set of 1. So if NAT instances fail, Auto Scaling will automatically launch an replacement instance NAT instance is highly available with limited downtime Let Auto Scaling monitor health and availability of the NAT instance Bootstrap scripts with the NAT instance to update the Route tables programmatically Keep a close watch on the Network Metrics and scale vertically the NAT instance type to the one with high network performance","title":"6-5 High Availability NAT Instance"},{"location":"chap3/1vpc/#6-6-disabling-sourcedestination-checks","text":"Each EC2 instance performs source/destination checks, by default, and the instance must be the source or destination of any traffic it sends or receives However, as the NAT instance acts as a router between the Internet and the instances in the private subnet it must be able to send and receive traffic when the source or destination is not itself . Therefore, the source/destination checks on the NAT instance should be disabled","title":"6-6 Disabling Source/Destination checks"},{"location":"chap3/1vpc/#6-7-nat-gateway-instance-comparison","text":"","title":"6-7 NAT Gateway &amp; Instance Comparison"},{"location":"chap3/1vpc/#6-8-nat-exam","text":"After launching an instance that you intend to serve as a NAT (Network Address Translation) device in a public subnet you modify your route tables to have the NAT device be the target of internet bound traffic of your private subnet. When you try and make an outbound connection to the Internet from an instance in the private subnet , you are not successful. Which of the following steps could resolve the issue? Attaching a second Elastic Network interface (ENI) to the NAT instance, and placing it in the private subnet Attaching an Elastic IP address to the instance in the private subnet Attaching a second Elastic Network Interface (ENI) to the instance in the private subnet, and placing it in the public subnet Disabling the Source/Destination Check attribute on the NAT instance (V) You manually launch a NAT AMI in a public subnet. The network is properly configured. Security groups and network access control lists are property configured. Instances in a private subnet can access the NAT. The NAT can access the Internet. However, private instances cannot access the Internet . What additional step is required to allow access from the private instances? Enable Source/Destination Check on the private Instances. Enable Source/Destination Check on the NAT instance. Disable Source/Destination Check on the private instances Disable Source/Destination Check on the NAT instance A user has created a VPC with public and private subnets. The VPC has CIDR 20.0.0.0/16 . The private subnet uses CIDR 20.0.1.0/24 and the public subnet uses CIDR 20.0.0.0/24 . The user is planning to host a web server in the public subnet (port 80. and a DB server in the private subnet (port 3306.. The user is configuring a security group of the NAT instance. Which of the below mentioned entries is not required for the NAT security group ? For Inbound allow Source: 20.0.1.0/24 on port 80 For Outbound allow Destination: 0.0.0.0/0 on port 80 For Inbound allow Source: 20.0.0.0/24 on port 80 For Outbound allow Destination: 0.0.0.0/0 on port 443 A web company is looking to implement an external payment service into their highly available application deployed in a VPC. Their application EC2 instances are behind a public facing ELB. Auto scaling is used to add additional instances as traffic increases. Under normal load the application runs 2 instances in the Auto Scaling group but at peak it can scale 3x in size. The application instances need to communicate with the payment service over the Internet, which requires whitelisting of all public IP addresses used to communicate with it . A maximum of 4 whitelisting IP addresses are allowed at a time and can be added through an API . How should they architect their solution? Route payment requests through two NAT instances setup for High Availability and whitelist the Elastic IP addresses attached to the NAT instances (check) Whitelist the VPC Internet Gateway Public IP and route payment requests through the Internet Gateway. ( Internet gateway is only to route traffic ) Whitelist the ELB IP addresses and route payment requests from the Application servers through the ELB. ( ELB does not have a fixed IP address ) Automatically assign public IP addresses to the application instances in the Auto Scaling group and run a script on boot that adds each instances public IP address to the payment validation whitelist API. ( would exceed the allowed 4 IP addresses )","title":"6-8 NAT Exam"},{"location":"chap3/1vpc/#7egress-only-internet-gateway","text":"Egress-only Internet gateway works as a NAT gateway, but for IPv6 traffic Egress-only Internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in the VPC to the Internet, and prevents the Internet from initiating an IPv6 connection with the instances. An egress-only Internet gateway is for use with IPv6 traffic only . To enable outbound-only Internet communication over IPv4, use a NAT gateway instead .","title":"7\u3001Egress-only Internet gateway"},{"location":"chap3/1vpc/#8vpc-subnet-sizing","text":"VPC supports IPv4 and IPv6 addressing, and has different CIDR block size limits for each IPv6 CIDR block can be optionally associated with the VPC VPC IPv4 CIDR block cannot be modified once created i.e. cannot increase or decrease the size of an existing CIDR block. However, secondary CIDR blocks can be associated with the VPC to extend the VPC Limitations allowed block size is between a /28 netmask and /16 netmask. CIDR block must not overlap with any existing CIDR block that\u2019s associated with the VPC. CIDR block must not be the same or larger than the CIDR range of a route in any of the VPC route tables for e.g. for a CIDR block 10.0.0.0/24 , can only associate smaller CIDR blocks like 10.0.0.0/25","title":"8\u3001VPC &amp; Subnet Sizing"},{"location":"chap3/1vpc/#9vpc-security","text":"Security within a VPC is provided through Security groups \u2013 Act as a firewall for associated EC2 instances , controlling both inbound and outbound traffic at the instance level Network access control lists (ACLs) \u2013 Act as a firewall for associated subnets, controlling both inbound and outbound traffic at the subnet level Flow logs \u2013 Capture information about the IP traffic going to and from network interfaces in your VPC","title":"9\u3001VPC Security"},{"location":"chap3/1vpc/#10security-groups-vs-nacls","text":"","title":"10\u3001Security Groups vs NACLs"},{"location":"chap3/1vpc/#10-1-aws-vpc-security-overview","text":"In a VPC, both Security Groups and Network ACLs (NACLS) together help to build a layered network defence. Security groups \u2013 Act as a virtual firewall for associated instances , controlling both inbound and outbound traffic at the instance level Network access control lists (NACLs) \u2013 Act as a firewall for associated subnets, controlling both inbound and outbound traffic at the subnet level","title":"10-1 AWS VPC Security Overview"},{"location":"chap3/1vpc/#10-2-security-groups","text":"Acts at an Instance level and not at the subnet level. Each instance within a subnet can be assigned a different set of Security groups An instance can be assigned 5 security groups with each security group having60 rules allows separate rules for inbound and outbound traffic allows adding or removing rules (authorizing or revoking access) for both Inbound (ingress) and Outbound (egress) traffic to the instance Default Security group allows no external inbound traffic but allows inbound traffic from instances with the same security group Default Security group allows all outbound traffic New Security groups start with only an outbound rule that allows all traffic to leave the instances can specify only Allow rules, but not deny rules can grant access to a specific IP, CIDR range, or to another security group in the VPC or in a peer VPC (requires a VPC peering connection) are evaluated as a Whole or Cumulative bunch of rules with the most permissive rule taking precedence for e.g. if you have a rule that allows access to TCP port 22 (SSH) from IP address 203.0.113.1 and another rule that allows access to TCP port 22 from everyone, everyone has access to TCP port 22. are Stateful \u2013 responses to allowed inbound traffic are allowed to flow outbound regardless of outbound rules, and vice versa. Hence an Outbound rule for the response is not needed Instances associated with a security group can\u2019t talk to each other unless rules allowing the traffic are added . are associated with ENI (network interfaces) . are associated with the instance and can be changed, which changes the security groups associated with the primary network interface (eth0) and the changes would be applicable immediately to all the instances associated with the Security group","title":"10-2 Security Groups"},{"location":"chap3/1vpc/#10-3-connection-tracking","text":"Security groups are Stateful as they use Connection tracking to track information about traffic to and from the instance. Responses to inbound traffic are allowed to flow out of the instance regardless of outbound security group rules , and vice versa. Connection Tracking is maintained only if there is no explicit Outbound rule for an Inbound request (and vice versa) However, if there is an explicit Outbound rule for an Inbound request, the response traffic is allowed on the basis of the Outbound rule and not on the Tracking information Tracking flow e.g. If an instance (host A) initiates traffic to host B and uses a protocol other than TCP, UDP, or ICMP, the instance\u2019s firewall only tracks the IP address & protocol number for the purpose of allowing response traffic from host B. If host B initiates traffic to the instance in a separate request within 600 seconds of the original request or response, the instance accepts it regardless of inbound security group rules, because it\u2019s regarded as response traffic. This can be controlled by modifying the security group\u2019s outbound rules to permit only certain types of outbound traffic. Alternatively, Network ACLs (NACLs) can be used for the subnet, network ACLs are stateless and therefore do not automatically allow response traffic.","title":"10-3 Connection Tracking"},{"location":"chap3/1vpc/#10-4-network-access-control-lists-nacls","text":"A Network ACLs (NACLs) is an optional layer of security for the VPC that acts as a firewall for controlling traffic in and out of one or more subnets. are not for granular control and are assigned at a Subnet level and is applicable to all the instances in that Subnet has separate inbound and outbound rules, and each rule can either allow or deny traffic Default ACL allows all inbound and outbound traffic . Newly created ACL denies all inbound and outbound traffic A Subnet can be assigned only 1 NACLs and if not associated explicitly would be associated implicitly with the default NACL can associate a network ACL with multiple subnets is a numbered list of rules that are evaluated in order starting with the lowest numbered rule, to determine whether traffic is allowed in or out of any subnet associated with the network ACL for e.g. if you have a Rule No. 100 with Allow All and 110 with Deny All, the Allow All would take precedence and all the traffic will be allowed are Stateless ; responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa) for e.g. if you enable Inbound SSH on port 22 from the specific IP address, you would need to add an Outbound rule for the response as well.","title":"10-4 Network Access Control Lists \u2013 NACLs"},{"location":"chap3/1vpc/#10-4-security-group-vs-nacls-questions","text":"Instance A and instance B are running in two different subnets A and B of a VPC. Instance A is not able to ping instance B . What are two possible reasons for this? (Pick 2 correct answers) The routing table of subnet A has no target route to subnet B The security group attached to instance B does not allow inbound ICMP traffic The policy linked to the IAM role on instance A is not configured correctly The NACL on subnet B does not allow outbound ICMP traffic An instance is launched into a VPC subnet with the network ACL configured to allow all inbound traffic and deny all outbound traffic . The instance\u2019s security group is configured to allow SSH from any IP address and deny all outbound traffic . What changes need to be made to allow SSH access to the instance? The outbound security group needs to be modified to allow outbound traffic. The outbound network ACL needs to be modified to allow outbound traffic . Nothing, it can be accessed from any IP address using SSH. Both the outbound security group and outbound network ACL need to be modified to allow outbound traffic. From what services I can block incoming/outgoing IPs? Security Groups DNS ELB VPC subnet IGW NACL What is the difference between a security group in VPC and a network ACL in VPC (chose 3 correct answers) Security group restricts access to a Subnet while ACL restricts traffic to EC2 Security group restricts access to EC2 while ACL restricts traffic to a subnet Security group can work outside the VPC also while ACL only works within a VPC Network ACL performs stateless filtering and Security group provides stateful filtering Security group can only set Allow rule, while ACL can set Deny rule also You are currently hosting multiple applications in a VPC and have logged numerous port scans coming in from a specific IP address block. Your security team has requested that all access from the offending IP address block be denied for the next 24 hours. Which of the following is the best method to quickly and temporarily deny access from the specified IP address block? Create an AD policy to modify Windows Firewall settings on all hosts in the VPC to deny access from the IP address block Modify the Network ACLs associated with all public subnets in the VPC to deny access from the IP address block Add a rule to all of the VPC 5 Security Groups to deny access from the IP address block Modify the Windows Firewall settings on all Amazon Machine Images (AMIs) that your organization uses in that VPC to deny access from the IP address block You have two Elastic Compute Cloud (EC2) instances inside a Virtual Private Cloud (VPC) in the same Availability Zone (AZ) but in different subnets. One instance is running a database and the other instance an application that will interface with the database. You want to confirm that they can talk to each other for your application to work properly. Which two things do we need to confirm in the VPC settings so that these EC2 instances can communicate inside the VPC ? Choose 2 answers A network ACL that allows communication between the two subnets . Both instances are the same instance class and using the same Key-pair. That the default route is set to a NAT instance or Internet Gateway (IGW) for them to communicate. Security groups are set to allow the application host to talk to the database on the right port/protocol A benefits enrollment company is hosting a 3-tier web application running in a VPC on AWS, which includes a NAT (Network Address Translation) instance in the public Web tier. There is enough provisioned capacity for the expected workload tor the new fiscal year benefit enrollment period plus some extra overhead Enrollment proceeds nicely for two days and then the web tier becomes unresponsive, upon investigation using CloudWatch and other monitoring tools it is discovered that there is an extremely large and unanticipated amount of inbound traffic coming from a set of 15 specific IP addresses over port 80 from a country where the benefits company has no customers. The web tier instances are so overloaded that benefit enrollment administrators cannot even SSH into them . Which activity would be useful in defending against this attack? Create a custom route table associated with the web tier and block the attacking IP addresses from the IGW (internet Gateway) Change the EIP (Elastic IP Address) of the NAT instance in the web tier subnet and update the Main Route Table with the new EIP Create 15 Security Group rules to block the attacking IP addresses over port 80 Create an inbound NACL (Network Access control list) associated with the web tier subnet with deny rules to block the attacking IP addresses Which of the following statements describes network ACLs? (Choose 2 answers) Responses to allowed inbound traffic are allowed to flow outbound regardless of outbound rules, and vice versa ( are stateless ) Using network ACLs, you can deny access from a specific IP range Keep network ACL rules simple and use a security group to restrict application level access NACLs are associated with a single Availability Zone ( associated with Subnet ) You are designing security inside your VPC. You are considering the options for establishing separate security zones and enforcing network traffic rules across different zone to limit Instances can communications. How would you accomplish these requirements? Choose 2 answers Configure a security group for every zone. Configure a default allow all rule. Configure explicit deny rules for the zones that shouldn\u2019t be able to communicate with one another ( Security group does not allow deny rules ) Configure you instances to use pre-set IP addresses with an IP address range every security zone. Configure NACL to explicitly allow or deny communication between the different IP address ranges, as required for interzone communication Configure a security group for every zone. Configure allow rules only between zone that need to be able to communicate with one another. Use implicit deny all rule to block any other traffic Configure multiple subnets in your VPC, one for each zone. Configure routing within your VPC in such a way that each subnet only has routes to other subnets with which it needs to communicate, and doesn\u2019t have routes to subnets with which it shouldn\u2019t be able to communicate. ( default routes are unmodifiable ) Your entire AWS infrastructure lives inside of one Amazon VPC. You have an Infrastructure monitoring application running on an Amazon instance in Availability Zone (AZ) A of the region, and another application instance running in AZ B. The monitoring application needs to make use of ICMP ping to confirm network reachability of the instance hosting the application. Can you configure the security groups for these instances to only allow the ICMP ping to pass from the monitoring instance to the application instance and nothing else\u201d If so how? No Two instances in two different AZ\u2019s can\u2019t talk directly to each other via ICMP ping as that protocol is not allowed across subnet (i.e. broadcast) boundaries ( Can communicate ) Yes Both the monitoring instance and the application instance have to be a part of the same security group, and that security group needs to allow inbound ICMP ( Need not have to be part of same security group ) Yes, The security group for the monitoring instance needs to allow outbound ICMP and the application instance\u2019s security group needs to allow Inbound ICMP (is stateful, so just allow outbound ICMP from monitoring and inbound ICMP on monitored instance) Yes, Both the monitoring instance\u2019s security group and the application instance\u2019s security group need to allow both inbound and outbound ICMP ping packets since ICMP is not a connection-oriented protocol (Security groups are stateful) A user has configured a VPC with a new subnet. The user has created a security group. The user wants to configure that instances of the same subnet communicate with each other. How can the user configure this with the security group? There is no need for a security group modification as all the instances can communicate with each other inside the same subnet Configure the subnet as the source in the security group and allow traffic on all the protocols and ports Configure the security group itself as the source and allow traffic on all the protocols and ports The user has to use VPC peering to configure this You are designing a data leak prevention solution for your VPC environment. You want your VPC Instances to be able to access software depots and distributions on the Internet for product updates. The depots and distributions are accessible via third party CDNs by their URLs. You want to explicitly deny any other outbound connections from your VPC instances to hosts on the Internet. Which of the following options would you consider? Configure a web proxy server in your VPC and enforce URL-based rules for outbound access Remove default routes. (Security group and NACL cannot have URLs in the rules nor does the route) Implement security groups and configure outbound rules to only permit traffic to software depots. Move all your instances into private VPC subnets remove default routes from all routing tables and add specific routes to the software depots and distributions only. Implement network access control lists to all specific destinations, with an Implicit deny as a rule. You have an EC2 Security Group with several running EC2 instances. You change the Security Group rules to allow inbound traffic on a new port and protocol, and launch several new instances in the same Security Group. The new rules apply: Immediately to all instances in the security group . Immediately to the new instances only. Immediately to the new instances, but old instances must be stopped and restarted before the new rules apply. To all instances, but it may take several minutes for old instances to see the changes.","title":"10-4  Security Group vs NACLs Questions"},{"location":"chap3/1vpc/#11vpc-flow-logs","text":"VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in the VPC and can help in monitoring the traffic or troubleshooting any connectivity issues Flow log data is stored using Amazon CloudWatch Logs Flow log can be created for the entire VPC , subnets or each network interface. If enabled, for entire VPC or subnet all the network interfaces are monitored Flow logs do not capture real-time log streams for network interfaces . Flow logs can be created for network interfaces that are created by other AWS services; for example, Elastic Load Balancing, RDS, ElastiCache, Redshift, and WorkSpaces","title":"11\u3001VPC Flow logs"},{"location":"chap3/1vpc/#12-subnets","text":"Subnet spans a single Availability Zone, distinct locations engineered to be isolated from failures in other AZs, and cannot span across AZs Subnet can be configured with an Internet gateway to enable communication over the Internet , or virtual private gateway (VPN) connection to enable communication with your corporate network Subnet can be Public or Private and it depends on whether it has Internet connectivity i.e. is able to route traffic to the Internet through the IGW Instances within the Public Subnet should be assigned a Public IP or Elastic IP address to be able to communicate with the Internet For Subnets not connected to the Internet, but has traffic routed through Virtual Private Gateway only is termed as VPN-only subnet Subnets can be configured to Enable assignment of the Public IP address to all the Instances launched within the Subnet by default , which can be overridden during the creation of the Instance","title":"12 Subnets"},{"location":"chap3/1vpc/#12-1-subnet-sizing","text":"CIDR block assigned to the Subnet can be the same as the VPC CIDR, in this case you can launch only one subnet within your VPC CIDR block assigned to the Subnet can be a subset of the VPC CIDR, which allows you to launch multiple subnets within the VPC CIDR block assigned to the subnet should not be overlapping CIDR block size allowed is between /28 netmask (minimum with 2^4 \u2013 16 available IP address) and /16 netmask (maximum with 2^16 \u2013 65536 IP address) AWS reserves 5 IPs address (first 4 and last 1 IP address) in each Subnet which are not available for use and cannot be assigned to an instance. for e.g. for a Subnet with a CIDR block 10.0.0.0/24 the following five IPs are reserved 10.0.0.0 : Network address 10.0.0.1 : Reserved by AWS for the VPC router 10.0.0.2 : Reserved by AWS for mapping to Amazon-provided DNS 10.0.0.3 : Reserved by AWS for future use 10.0.0.255 : Network broadcast address. AWS does not support broadcast in a VPC, therefore the address is reserved.","title":"12-1 Subnet Sizing"},{"location":"chap3/1vpc/#12-2-subnet-routing","text":"Each Subnet is associated with a route table which controls the traffic.","title":"12-2 Subnet Routing"},{"location":"chap3/1vpc/#12-3-subnet-security","text":"Subnet security can be configured using Security groups and NACLs Security groups works at instance level, NACLs work at the subnet level","title":"12-3 Subnet Security"},{"location":"chap3/1vpc/#13-shared-vpcs","text":"VPC sharing allows multiple AWS accounts to create their application resources , such as EC2 instances, RDS databases, Redshift clusters, and AWS Lambda functions, into shared, centrally-managed VPCs. In this model, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations . After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them . Participants cannot view, modify, or delete resources that belong to other participants or the VPC owner.","title":"13 Shared VPCs"},{"location":"chap3/1vpc/#14-aws-vpc-endpoints","text":"","title":"14 AWS VPC Endpoints"},{"location":"chap3/1vpc/#14-1-vpc-endpoints-overview","text":"VPC endpoint enables creation of a private connection between VPC to supported AWS services and VPC endpoint services powered by PrivateLink using its private IP address VPC Endpoint does not require a public IP address , access over the Internet, NAT device, a VPN connection or AWS Direct Connect Traffic between VPC and AWS service does not leave the Amazon network E ndpoints are virtual devices , that are horizontally scaled, redundant, and highly available VPC components that allow communication between instances in the VPC and AWS services without imposing availability risks or bandwidth constraints on your network traffic. Endpoints currently do not support cross-region requests, ensure that the endpoint is created in the same region as your bucket AWS currently supports two types of Endpoints VPC Interface Endpoints VPC Gateway Endpoints VPC Endpoint policy is an IAM resource policy attached to an endpoint for controlling access from the endpoint to the specified service.. Endpoint policy, by default, allows full access to the service. Endpoint policy does not override or replace IAM user policies or service-specific policies (such as S3 bucket policies).","title":"14-1 VPC Endpoints Overview"},{"location":"chap3/1vpc/#14-2-vpc-gateway-endpoints","text":"A VPC Gateway Endpoint is a gateway that is a target for a specified route in the route table , used for traffic destined to a supported AWS service. VPC Gateway Endpoint currently supports S3 and DynamoDB services","title":"14-2 VPC Gateway Endpoints"},{"location":"chap3/1vpc/#14-3-configuration","text":"Endpoint requires the VPC and the service to be accessed via the endpoint Endpoint needs to be associated with the Route table and the route table cannot be modified to remove the route entry. It can only be deleted by removing the Endpoint association with the Route table A route is automatically added to the Route table with a destination that specifies the prefix list of service and the target with the endpoint id . for e.g. A rule with destination pl-68a54001 (com.amazonaws.us-west-2.s3) and a target with this endpoints\u2019 ID (e.g. vpce-12345678) will be added to the route tables Access to the resources in other services can be controlled by endpoint policies Security groups needs to be modified to allow Outbound traffic from the VPC to the service thats specified in the endpoint . Use the service prefix list ID for e.g. com.amazonaws.us-east-1.s3 as the destination in the outbound rule Multiple endpoints can be created in a single VPC, for e.g., to multiple services. Multiple endpoints can be created for a single service, and different route tables used to enforce different access policies from different subnets to the same service. Multiple endpoints to the same service CANNOT be specified in a single route table","title":"14-3 Configuration"},{"location":"chap3/1vpc/#14-2-limitations","text":"Endpoint cannot be created between a VPC and an AWS service in a different region . Endpoints support IPv4 traffic only . Endpoint cannot be transferred from one VPC to another, or from one service to another Endpoint connections cannot be extended out of a VPC i.e. resources across the VPN connection, VPC peering connection, AWS Direct Connect connection cannot use the endpoint","title":"14-2 Limitations"},{"location":"chap3/1vpc/#14-3-vpc-interface-endpoints","text":"VPC Interface endpoint enables connectivity to services powered by AWS PrivateLink. Services include some AWS services for e.g. CloudTrail, CloudWatch etc., services hosted by other AWS customers and partners in their own VPCs (referred to as endpoint services) , and supported AWS Marketplace partner services.","title":"14-3 VPC Interface Endpoints"},{"location":"chap3/1vpc/#14-4-limitations","text":"For each interface endpoint, only one subnet per Availability Zone can be selected. Each interface endpoint can support a bandwidth of up to 10 Gbps per Availability Zone by default. Additional capacity may be added automatically based on your usage. Network ACL for the subnet can restrict traffic, and needs to be configured properly Interface Endpoint supports TCP traffic only . Endpoints are supported within the same region only . Endpoints support IPv4 traffic only. Endpoints cannot be transferred from one VPC to another, or from one service to another .","title":"14-4 Limitations"},{"location":"chap3/1vpc/#14-5-aws-vpc-endpoints-exam","text":"You have an application running on an Amazon EC2 instance that uploads 10 GB video objects to amazon S3. Video uploads are taking longer than expected inspite of using multipart upload cause of internet bandwidth, resulting in poor application performance. Which action can help improve the upload performance? Apply an Amazon S3 bucket policy Use Amazon EBS provisioned IOPS Use VPC endpoints for S3 Request a service limit increase What are the services supported by VPC endpoints, using Gateway endpoint type? Choose 2 answers Amazon S3 Amazon EFS Amazon DynamoDB Amazon Glacier Amazon SQS What are the different types of endpoint types supported by VPC endpoints? Choose 2 Answers Gateway Classic Interface Virtual Network An application running on EC2 instances processes sensitive information stored on Amazon S3. The information is accessed over the Internet. The security team is concerned that the Internet connectivity to Amazon S3 is a security risk. Which solution will resolve the security concern? Access the data through an Internet Gateway. Access the data through a VPN connection. Access the data through a NAT Gateway. Access the data through a VPC endpoint for Amazon S3 . You need to design a VPC for a three-tier architecture, web-application consisting of an Elastic Load Balancer (ELB), a fleet of web/application servers, and backend consisting of an RDS database. The entire Infrastructure must be distributed over 2 availability zones. Which VPC configuration works while assuring the least components are exposed to Internet? Two public subnets for ELB, two private subnets for the web-servers, two private subnets for RDS and DynamoDB Two public subnets for ELB and web-servers, two private subnets for RDS and DynamoDB Two public subnets for ELB, two private subnets for the web-servers, two private subnets for RDS and VPC Endpoints for DynamoDB Two public subnets for ELB and web-servers, two private subnets for RDS and VPC Endpoints for DynamoDB","title":"14-5 AWS VPC Endpoints Exam"},{"location":"chap3/1vpc/#15-aws-vpc-vpn-cloudhub-connections","text":"VPC VPN connections are used to extend on-premise data centers to AWS VPC VPN connections provide secure IPSec connections from on-premise computers/services to AWS","title":"15 AWS VPC VPN \u2013 CloudHub Connections"},{"location":"chap3/1vpc/#15-1-vpc-vpn-connections","text":"AWS hardware VPN Connectivity can be established by creating an IPSec, hardware VPN connection between the VPC and the remote network . On the AWS side of the VPN connection, a Virtual Private Gateway (VGW) provides two VPN endpoints for automatic failover . On customer side a customer gateway (CGW) needs to be configured, which is the physical device or software application on the remote side of the VPN connection AWS Direct Connect AWS Direct Connect provides a dedicated p rivate connection from a remote network to your VPC . Direct Connect can be combined with an AWS hardware VPN connection to create an IPsec-encrypted connection AWS VPN CloudHub For more than one remote network for e.g. multiple branch offices , multiple AWS hardware VPN connections can be created via the VPC to enable communication between these networks Software VPN VPN connection can be setup by running a software VPN like OpenVPN appliance on an EC2 instance in the VPC AWS does not provide or maintain software VPN appliances ; however, there are range of products provided by partners and open source communities","title":"15-1 VPC VPN Connections"},{"location":"chap3/1vpc/#15-2-hardware-vpn-connection","text":"","title":"15-2 Hardware VPN Connection"},{"location":"chap3/1vpc/#15-3-vpn-components","text":"Virtual Private Gateway \u2013 VGW A virtual private gateway is the VPN concentrator on the AWS side of the VPN connection Customer Gateway \u2013 CGW A customer gateway is a physical device or software application on customer side of the VPN connection. When a VPN connection is created, the VPN tunnel comes up when traffic is generated from the remote side of the VPN connection . VGW is not the initiator; CGW must initiate the tunnels If the VPN connection experiences a period of idle time, usually 10 seconds, depending on the configuration, the tunnel may go dow n. To prevent this, a network monitoring tool to generate keepalive pings; for e.g. by using IP SLA.","title":"15-3 VPN Components"},{"location":"chap3/1vpc/#15-4-vpn-configuration","text":"VPC has an attached virtual private gateway, and the remote network includes a customer gateway, which must be configured to enable the VPN connection. Routing must be setup so that any traffic from the VPC bound for the remote network is routed to the virtual private gateway. Each VPN has two tunnels associated with it that can be configured on the customer router, as is not single point of failure M ultiple VPN connections to a single VPC can be created, and a second CGW can be configured to create a redundant connection to the same external location or to create VPN connections to multiple geographic locations.","title":"15-4 VPN Configuration"},{"location":"chap3/1vpc/#15-5-vpn-routing-options","text":"For a VPN connection, the route table for the subnets s hould be updated with the type of routing (static of dynamic) that you plan to use . Route tables determine where network traffic is directed. Traffic destined for the VPN connections must be routed to the virtual private gateway. Type of routing can depend on the make and model of your VPN devices . Static Routing If your device does not support BGP, specify static routing . Using static routing, the routes (IP prefixes) can be specified that should be communicated to the virtual private gateway. Devices that don\u2019t support BGP may also perform health checks to assist failover to the second tunnel when needed. BGP dynamic routing If the VPN device supports Border Gateway Protocol (BGP) , specify dynamic routing with the VPN connection. When using a BGP device, static routes need not be specified to the VPN connection because the device uses BGP for auto discovery and to advertise its routes to the virtual private gateway. BGP-capable devices are recommended as the BGP protocol offers robust liveness detection checks that can assist failover to the second VPN tunnel if the first tunnel goes down. Only IP prefixes known to the virtual private gateway, either through BGP advertisement or static route entry, can receive traffic from your VPC. Virtual private gateway does not route any other traffic destined outside of the advertised BGP, static route entries, or its attached VPC CIDR. Only IP prefixes known to the virtual private gateway, either through BGP advertisement or static route entry, can receive traffic from your VPC . Virtual private gateway does not route any other traffic destined outside of the advertised BGP, static route entries, or its attached VPC CIDR. A VPN connection is used to connect the customer network to a VPC. Each VPN connection has two tunnels to help ensure connectivity in case one of the VPN connections becomes unavailable, with each tunnel using a unique virtual private gateway public IP address . Both tunnels should be configured for redundancy . When one tunnel becomes unavailable, for e.g. down for maintenance, network traffic is automatically routed to the available tunnel for that specific VPN connection. To protect against a loss of connectivity in case the customer gateway becomes unavailable, a second VPN connection can be setup to the VPC and virtual private gateway by using a second customer gateway. Customer gateway IP address for the second VPN connection must be publicly accessibl e. By using redundant VPN connections and CGWs, maintenance on one of the customer gateways can be performed while traffic continues to flow over the second customer gateway\u2019s VPN connection. Dynamically routed VPN connections using the Border Gateway Protocol (BGP) are recommended, if available, to exchange routing information between the customer gateways and the virtual private gateways . Statically routed VPN connections require static routes for the network to be entered on the customer gateway side . BGP-advertised and statically entered route information allow gateways on both sides to determine which tunnels are available and reroute traffic if a failure occurs.","title":"15-5 VPN Routing Options"},{"location":"chap3/1vpc/#15-6-vpn-cloudhub","text":"VPN CloudHub can be used to provide secure communication between sites, if you have multiple VPN connections VPN CloudHub operates on a simple hub-and-spoke model that can be used with or without a VPC . Design is suitable for customers with multiple branch offices and existing Internet connections who\u2019d like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices VPN CloudHub architecture with blue dashed lines indicates network traffic between remote sites being routed over their VPN connections. A WS VPN CloudHub requires a virtual private gateway with multiple customer gateways . Each customer gateway must use a unique Border Gateway Protocol (BGP) Autonomous System Number (ASN) Customer gateways advertise the appropriate routes (BGP prefixes) over their VPN connections. Routing advertisements are received and re-advertised to each BGP peer, enabling each site to send data to and receive data from the other sites. Routes for each spoke must have unique ASNs and the sites must not have overlapping IP ranges. Each site can also send and receive data from the VPC as if they were using a standard VPN connection . Sites that use AWS Direct Connect connections to the virtual private gateway can also be part of the AWS VPN CloudHub. To configure the AWS VPN CloudHub, multiple customer gateways can be created, each with the unique public IP address of the gateway and the ASN. a VPN connection can be created from each customer gateway to a common virtual private gateway. each VPN connection must advertise its specific BGP routes. This is done using the network statements in the VPN configuration files for the VPN connection.","title":"15-6 VPN CloudHub"},{"location":"chap3/1vpc/#15-7-vpc-vpn-cloudhub-connections","text":"You have in total 5 offices, and the entire employee related information is stored under AWS VPC instances. Now all the offices want to connect the instances in VPC using VPN. Which of the below help you to implement this? you can have redundant customer gateways between your data center and your VPC you can have multiple locations connected to the AWS VPN CloudHub You have to define 5 different static IP addresses in route table. 1 and 2 1,2 and 3 You have in total 15 offices, and the entire employee related information is stored under AWS VPC instances. Now all the offices want to connect the instances in VPC using VPN. What problem do you see in this scenario? You can not create more than 1 VPN connections with single VPC ( Can be created ) You can not create more than 10 VPN connections with single VPC ( soft limit can be extended ) When you create multiple VPN connections, the virtual private gateway can not sends network traffic to the appropriate VPN connection using statically assigned routes. ( Can route the traffic to correct connection ) Statically assigned routes cannot be configured in case of more than 1 VPN with virtual private gateway. ( can be configured ) None of above You have been asked to virtually extend two existing data centers into AWS to support a highly available application that depends on existing, on-premises resources located in multiple data centers and static content that is served from an Amazon Simple Storage Service (S3) bucket. Your design currently includes a dual-tunnel VPN connection between your CGW and VGW. Which component of your architecture represents a potential single point of failure that you should consider changing to make the solution more highly available? Add another VGW in a different Availability Zone and create another dual-tunnel VPN connection. Add another CGW in a different data center and create another dual-tunnel VPN connection . Add a second VGW in a different Availability Zone, and a CGW in a different data center, and create another dual-tunnel. No changes are necessary: the network architecture is currently highly available. You are designing network connectivity for your fat client application. The application is designed for business travelers who must be able to connect to it from their hotel rooms, cafes, public Wi-Fi hotspots, and elsewhere on the Internet. You do not want to publish the application on the Internet. Which network design meets the above requirements while minimizing deployment and operational costs? [PROFESSIONAL] Implement AWS Direct Connect, and create a private interface to your VPC. Create a public subnet and place your application servers in it. (High Cost and does not minimize deployment) Implement Elastic Load Balancing with an SSL listener that terminates the back-end connection to the application. (Needs to be published to internet) Configure an IPsec VPN connection, and provide the users with the configuration details. Create a public subnet in your VPC, and place your application servers in it. (Instances still in public subnet are internet accessible) Configure an SSL VPN solution in a public subnet of your VPC, then install and configure SSL VPN client software on all user computers. Create a private subnet in your VPC and place your application servers in it. (Cost effective and can be in private subnet as well) You are designing a connectivity solution between on-premises infrastructure and Amazon VPC Your server\u2019s on-premises will De communicating with your VPC instances You will De establishing IPSec tunnels over the internet You will be using VPN gateways and terminating the IPsec tunnels on AWS-supported customer gateways. Which of the following objectives would you achieve by implementing an IPSec tunnel as outlined above? (Choose 4 answers) [PROFESSIONAL] End-to-end protection of data in transit End-to-end Identity authentication Data encryption across the Internet Protection of data in transit over the Internet Peer identity authentication between VPN gateway and customer gateway Data integrity protection across the Internet A development team that is currently doing a nightly six-hour build which is lengthening over time on-premises with a large and mostly under utilized server would like to transition to a continuous integration model of development on AWS with multiple builds triggered within the same day. However, they are concerned about cost, security and how to integrate with existing on-premises applications such as their LDAP and email servers, which cannot move off-premises. The development environment needs a source code repository; a project management system with a MySQL database resources for performing the builds and a storage location for QA to pick up builds from. What AWS services combination would you recommend to meet the development team\u2019s requirements? [PROFESSIONAL] A Bastion host Amazon EC2 instance running a VPN server for access from on-premises, Amazon EC2 for the source code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIP for the source code repository and project management system, Amazon SQL for a build queue, An Amazon Auto Scaling group of Amazon EC2 instances for performing builds and Amazon Simple Email Service for sending the build output. (Bastion is not for VPN connectivity also SES should not be used) An AWS Storage Gateway for connecting on-premises software applications with cloud-based storage securely, Amazon EC2 for the resource code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIPs for the source code repository and project management system, Amazon Simple Notification Service for a notification initiated build, An Auto Scaling group of Amazon EC2 instances for performing builds and Amazon S3 for the build output. (Storage Gateway does provide secure connectivity but still needs VPN. SNS alone cannot handle builds) An AWS Storage Gateway for connecting on-premises software applications with cloud-based storage securely, Amazon EC2 for the resource code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIPs for the source code repository and project management system, Amazon SQS for a build queue, An Amazon Elastic Map Reduce (EMR) cluster of Amazon EC2 instances for performing builds and Amazon CloudFront for the build output. (Storage Gateway does not provide secure connectivity, still needs VPN. EMR is not ideal for performing builds as it needs normal EC2 instances) A VPC with a VPN Gateway back to their on-premises servers, Amazon EC2 for the source-code repository with attached Amazon EBS volumes, Amazon EC2 and Amazon RDS MySQL for the project management system, EIPs for the source code repository and project management system, SQS for a build queue, An Auto Scaling group of EC2 instances for performing builds and S3 for the build output. (VPN gateway is required for secure connectivity. SQS for build queue and EC2 for builds)","title":"15-7 VPC VPN \u2013 CloudHub Connections"},{"location":"chap3/1vpc/#16-vpc-exam","text":"You have a business-to-business web application running in a VPC consisting of an Elastic Load Balancer (ELB), web servers, application servers and a database. Your web application should only accept traffic from predefined customer IP addresses. Which two options meet this security requirement? Choose 2 answers Configure web server VPC security groups to allow traffic from your customers\u2019 IPs ( Web server is behind the ELB and customer IPs will never reach web servers ) Configure your web servers to filter traffic based on the ELB\u2019s \u201cX-forwarded-for\u201d header (get the customer IPs and create a custom filter to restrict access.) C onfigure ELB security groups to allow traffic from your customers\u2019 IPs and deny all outbound traffic (ELB will see the customer IPs so can restrict access, deny all is basically have no rules in outbound traffic, implicit, and its stateful so would work) Configure a VPC NACL to allow web traffic from your customers\u2019 IPs and deny all outbound traffic ( NACL is stateless, deny all will not work ) A user has created a VPC with public and private subnets using the VPC Wizard. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.0.0/24. Which of the below mentioned entries are required in the main route table to allow the instances in VPC to communicate with each other? Destination : 20.0.0.0/24 and Target : VPC Destination : 20.0.0.0/16 and Target : ALL Destination : 20.0.0.0/0 and Target : ALL Destination : 20.0.0.0/16 and Target : Local A user has created a VPC with two subnets: one public and one private. The user is planning to run the patch update for the instances in the private subnet. How can the instances in the private subnet connect to the internet? Use the internet gateway with a private IP Allow outbound traffic in the security group for port 80 to allow internet updates The private subnet can never connect to the internet Use NAT with an elastic IP A user has launched an EC2 instance and installed a website with the Apache webserver. The webserver is running but the user is not able to access the website from the Internet. What can be the possible reason for this failure? The security group of the instance is not configured properly . The instance is not configured with the proper key-pairs. The Apache website cannot be accessed from the Internet. Instance is not configured with an elastic IP. A user has created a VPC with public and private subnets using the VPC wizard. Which of the below mentioned statements is true in this scenario? AWS VPC will automatically create a NAT instance with the micro size VPC bounds the main route table with a private subnet and a custom route table with a public subnet User has to manually create a NAT instance VPC bounds the main route table with a public subnet and a custom route table with a private subnet A user has created a VPC with public and private subnets. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.1.0/24 and the public subnet uses CIDR 20.0.0.0/24. The user is planning to host a web server in the public subnet (port 80) and a DB server in the private subnet (port 3306). The user is configuring a security group of the NAT instance. Which of the below mentioned entries is not required for the NAT security group? For Inbound allow Source: 20.0.1.0/24 on port 80 For Outbound allow Destination: 0.0.0.0/0 on port 80 For Inbound allow Source: 20.0.0.0/24 on port 80 For Outbound allow Destination: 0.0.0.0/0 on port 443 A user has created a VPC with CIDR 20.0.0.0/24. The user has used all the IPs of CIDR and wants to increase the size of the VPC. The user has two subnets: public (20.0.0.0/25) and private (20.0.0.128/25). How can the user change the size of the VPC? The user can delete all the instances of the subnet. Change the size of the subnets to 20.0.0.0/32 and 20.0.1.0/32, respectively. Then the user can increase the size of the VPC using CLI It is not possible to change the size of the VPC once it has been created ( NOTE \u2013 You can now increase the VPC size ) User can add a subnet with a higher range so that it will automatically increase the size of the VPC User can delete the subnets first and then modify the size of the VPC A user has created a VPC with the public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The public subnet uses CIDR 20.0.1.0/24. The user is planning to host a web server in the public subnet (port 80) and a DB server in the private subnet (port 3306). The user is configuring a security group for the public subnet (WebSecGrp) and the private subnet (DBSecGrp). Which of the below mentioned entries is required in the web server security group (WebSecGrp)? Configure Destination as DB Security group ID (DbSecGrp) for port 3306 Outbound Configure port 80 for Destination 0.0.0.0/0 Outbound Configure port 3306 for source 20.0.0.0/24 InBound Configure port 80 InBound for source 20.0.0.0/16 A user has created a VPC with CIDR 20.0.0.0/16. The user has created one subnet with CIDR 20.0.0.0/16 by mistake. The user is trying to create another subnet of CIDR 20.0.0.1/24. How can the user create the second subnet? There is no need to update the subnet as VPC automatically adjusts the CIDR of the first subnet based on the second subnet\u2019s CIDR The user can modify the first subnet CIDR from the console It is not possible to create a second subnet as one subnet with the same CIDR as the VPC has been created The user can modify the first subnet CIDR with AWS CLI A user has setup a VPC with CIDR 20.0.0.0/16. The VPC has a private subnet (20.0.1.0/24) and a public subnet (20.0.0.0/24). The user\u2019s data centre has CIDR of 20.0.54.0/24 and 20.1.0.0/24. If the private subnet wants to communicate with the data centre, what will happen? It will allow traffic communication on both the CIDRs of the data centre It will not allow traffic with data centre on CIDR 20.1.0.0/24 but allows traffic communication on 20.0.54.0/24 It will not allow traffic communication on any of the data centre CIDRs It will allow traffic with data centre on CIDR 20.1.0.0/24 but does not allow on 20.0.54.0/24 (as the CIDR block would be overlapping) A user has created a VPC with public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The private subnet uses CIDR 20.0.0.0/24 . The NAT instance ID is i-a12345. Which of the below mentioned entries are required in the main route table attached with the private subnet to allow instances to connect with the internet? Destination: 0.0.0.0/0 and Target: i-a12345 Destination: 20.0.0.0/0 and Target: 80 Destination: 20.0.0.0/0 and Target: i-a12345 Destination: 20.0.0.0/24 and Target: i-a12345 A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created a public subnet CIDR (20.0.0.0/24) and VPN only subnets CIDR (20.0.1.0/24) along with the VPN gateway (vgw-12345) to connect to the user\u2019s data centre. The user\u2019s data centre has CIDR 172.28.0.0/12. The user has also setup a NAT instance (i-123456) to allow traffic to the internet from the VPN subnet. Which of the below mentioned options is not a valid entry for the main route table in this scenario? Destination: 20.0.1.0/24 and Target: i-12345 Destination: 0.0.0.0/0 and Target: i-12345 Destination: 172.28.0.0/12 and Target: vgw-12345 Destination: 20.0.0.0/16 and Target: local A user has created a VPC with CIDR 20.0.0.0/16. The user has created one subnet with CIDR 20.0.0.0/16 in this VPC. The user is trying to create another subnet with the same VPC for CIDR 20.0.0.1/24. What will happen in this scenario? The VPC will modify the first subnet CIDR automatically to allow the second subnet IP range It is not possible to create a subnet with the same CIDR as VPC The second subnet will be created It will throw a CIDR overlaps error A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created both Public and VPN-Only subnets along with hardware VPN access to connect to the user\u2019s data centre. The user has not yet launched any instance as well as modified or deleted any setup. He wants to delete this VPC from the console. Will the console allow the user to delete the VPC? Yes, the console will delete all the setups and also delete the virtual private gateway No, the console will ask the user to manually detach the virtual private gateway first and then allow deleting the VPC Yes, the console will delete all the setups and detach the virtual private gateway No, since the NAT instance is running A user has created a VPC with the public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The public subnet uses CIDR 20.0.1.0/24. The user is planning to host a web server in the public subnet (port 80) and a DB server in the private subnet (port 3306). The user is configuring a security group for the public subnet (WebSecGrp) and the private subnet (DBSecGrp). Which of the below mentioned entries is required in the private subnet database security group (DBSecGrp)? Allow Inbound on port 3306 for Source Web Server Security Group (WebSecGrp) Allow Inbound on port 3306 from source 20.0.0.0/16 Allow Outbound on port 3306 for Destination Web Server Security Group (WebSecGrp. Allow Outbound on port 80 for Destination NAT Instance IP A user has created a VPC with a subnet and a security group. The user has launched an instance in that subnet and attached a public IP. The user is still unable to connect to the instance. The internet gateway has also been created. What can be the reason for the error? The internet gateway is not configured with the route table The private IP is not present The outbound traffic on the security group is disabled The internet gateway is not configured with the security group A user has created a subnet in VPC and launched an EC2 instance within it. The user has not selected the option to assign the IP address while launching the instance. Which of the below mentioned statements is true with respect to the Instance requiring access to the Internet? The instance will always have a public DNS attached to the instance by default The user can directly attach an elastic IP to the instance The instance will never launch if the public IP is not assigned The user would need to create an internet gateway and then attach an elastic IP to the instance to connect from internet A user has created a VPC with public and private subnets using the VPC wizard. Which of the below mentioned statements is not true in this scenario? VPC will create a routing instance and attach it with a public subnet VPC will create two subnets VPC will create one internet gateway and attach it to VPC VPC will launch one NAT instance with an elastic IP A user has created a VPC with the public subnet. The user has created a security group for that VPC. Which of the below mentioned statements is true when a security group is created? It can connect to the AWS services, such as S3 and RDS by default It will have all the inbound traffic by default It will have all the outbound traffic by default It will by default allow traffic to the internet gateway A user has created a VPC with CIDR 20.0.0.0/16 using VPC Wizard. The user has created a public CIDR (20.0.0.0/24) and a VPN only subnet CIDR (20.0.1.0/24) along with the hardware VPN access to connect to the user\u2019s data centre. Which of the below mentioned components is not present when the VPC is setup with the wizard? Main route table attached with a VPN only subnet A NAT instance configured to allow the VPN subnet instances to connect with the internet Custom route table attached with a public subnet An internet gateway for a public subnet A user has created a VPC with public and private subnets using the VPC wizard. The user has not launched any instance manually and is trying to delete the VPC. What will happen in this scenario? It will not allow to delete the VPC as it has subnets with route tables It will not allow to delete the VPC since it has a running route instance It will terminate the VPC along with all the instances launched by the wizard It will not allow to delete the VPC since it has a running NAT instance A user has created a public subnet with VPC and launched an EC2 instance within it. The user is trying to delete the subnet. What will happen in this scenario? It will delete the subnet and make the EC2 instance as a part of the default subnet It will not allow the user to delete the subnet until the instances are terminated It will delete the subnet as well as terminate the instances Subnet can never be deleted independently, but the user has to delete the VPC first A user has created a VPC with CIDR 20.0.0.0/24. The user has created a public subnet with CIDR 20.0.0.0/25 and a private subnet with CIDR 20.0.0.128/25. The user has launched one instance each in the private and public subnets. Which of the below mentioned options cannot be the correct IP address (private IP) assigned to an instance in the public or private subnet? 20.0.0.255 20.0.0.132 20.0.0.122 20.0.0.55 A user has created a VPC with CIDR 20.0.0.0/16. The user has created public and VPN only subnets along with hardware VPN access to connect to the user\u2019s datacenter. The user wants to make so that all traffic coming to the public subnet follows the organization\u2019s proxy policy. How can the user make this happen? Setting up a NAT with the proxy protocol and configure that the public subnet receives traffic from NAT Setting up a proxy policy in the internet gateway connected with the public subnet It is not possible to setup the proxy policy for a public subnet Setting the route table and security group of the public subnet which receives traffic from a virtual private gateway A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created a public subnet CIDR (20.0.0.0/24) and VPN only subnets CIDR (20.0.1.0/24) along with the VPN gateway (vgw-12345) to connect to the user\u2019s data centre. Which of the below mentioned options is a valid entry for the main route table in this scenario? Destination: 20.0.0.0/24 and Target: vgw-12345 Destination: 20.0.0.0/16 and Target: ALL Destination: 20.0.1.0/16 and Target: vgw-12345 Destination: 0.0.0.0/0 and Target: vgw-12345 Which two components provide connectivity with external networks? When attached to an Amazon VPC which two components provide connectivity with external networks? Choose 2 answers Elastic IPs (EIP) ( Does not provide connectivity, public IP address will do as well ) NAT Gateway (NAT) ( Not Attached to VPC and still needs IGW ) Internet Gateway (IGW) Virtual Private Gateway (VGW) You are attempting to connect to an instance in Amazon VPC without success You have already verified that the VPC has an Internet Gateway (IGW) the instance has an associated Elastic IP (EIP) and correct security group rules are in place. Which VPC component should you evaluate next? The configuration of a NAT instance The configuration of the Routing Table The configuration of the internet Gateway (IGW) The configuration of SRC/DST checking If you want to launch Amazon Elastic Compute Cloud (EC2) Instances and assign each Instance a predetermined private IP address you should: Assign a group or sequential Elastic IP address to the instances Launch the instances in a Placement Group Launch the instances in the Amazon virtual Private Cloud (VPC) Use standard EC2 instances since each instance gets a private Domain Name Service (DNS) already Launch the Instance from a private Amazon Machine image (AMI) A user has recently started using EC2. The user launched one EC2 instance in the default subnet in EC2-VPC Which of the below mentioned options is not attached or available with the EC2 instance when it is launched? Public IP address Internet gateway Elastic IP Private IP address A user has created a VPC with CIDR 20.0.0.0/24. The user has created a public subnet with CIDR 20.0.0.0/25. The user is trying to create the private subnet with CIDR 20.0.0.128/25. Which of the below mentioned statements is true in this scenario? It will not allow the user to create the private subnet due to a CIDR overlap It will allow the user to create a private subnet with CIDR as 20.0.0.128/25 This statement is wrong as AWS does not allow CIDR 20.0.0.0/25 It will not allow the user to create a private subnet due to a wrong CIDR range A user has created a VPC with CIDR 20.0.0.0/16 with only a private subnet and VPN connection using the VPC wizard. The user wants to connect to the instance in a private subnet over SSH. How should the user define the security rule for SSH? Allow Inbound traffic on port 22 from the user\u2019s network The user has to create an instance in EC2 Classic with an elastic IP and configure the security group of a private subnet to allow SSH from that elastic IP The user can connect to a instance in a private subnet using the NAT instance Allow Inbound traffic on port 80 and 22 to allow the user to connect to a private subnet over the Internet A company wants to implement their website in a virtual private cloud (VPC). The web tier will use an Auto Scaling group across multiple Availability Zones (AZs). The database will use Multi-AZ RDS MySQL and should not be publicly accessible. What is the minimum number of subnets that need to be configured in the VPC? 1 2 3 4 (2 public subnets for web instances in multiple AZs and 2 private subnets for RDS Multi-AZ) Which of the following are characteristics of Amazon VPC subnets? Choose 2 answers Each subnet maps to a single Availability Zone A CIDR block mask of /25 is the smallest range supported Instances in a private subnet can communicate with the Internet only if they have an Elastic IP. By default, all subnets can route between each other, whether they are private or public Each subnet spans at least 2 Availability zones to provide a high-availability environment You need to design a VPC for a web-application consisting of an Elastic Load Balancer (ELB). a fleet of web/application servers, and an RDS database The entire Infrastructure must be distributed over 2 availability zones. Which VPC configuration works while assuring the database is not available from the Internet? One public subnet for ELB one public subnet for the web-servers, and one private subnet for the database One public subnet for ELB two private subnets for the web-servers, two private subnets for RDS Two public subnets for ELB two private subnets for the web-servers and two private subnets for RDS Two public subnets for ELB two public subnets for the web-servers, and two public subnets for RDS You have deployed a three-tier web application in a VPC with a CIDR block of 10.0.0.0/28. You initially deploy two web servers, two application servers, two database servers and one NAT instance tor a total of seven EC2 instances. The web, application and database servers are deployed across two availability zones (AZs). You also deploy an ELB in front of the two web servers, and use Route53 for DNS Web traffic gradually increases in the first few days following the deployment, so you attempt to double the number of instances in each tier of the application to handle the new load unfortunately some of these new instances fail to launch. Which of the following could the root caused? (Choose 2 answers) [PROFESSIONAL] The Internet Gateway (IGW) of your VPC has scaled-up adding more instances to handle the traffic spike, reducing the number of available private IP addresses for new instance launches. AWS reserves one IP address in each subnet\u2019s CIDR block for Route53 so you do not have enough addresses left to launch all of the new EC2 instances. AWS reserves the first and the last private IP address in each subnet\u2019s CIDR block so you do not have enough addresses left to launch all of the new EC2 instances. The ELB has scaled-up. Adding more instances to handle the traffic reducing the number of available private IP addresses for new instance launches AWS reserves the first four and the last IP address in each subnet\u2019s CIDR block so you do not have enough addresses left to launch all of the new EC2 instances . A user wants to access RDS from an EC2 instance using IP addresses. Both RDS and EC2 are in the same region, but different AZs. Which of the below mentioned options help configure that the instance is accessed faster? Configure the Private IP of the Instance in RDS security group (Recommended as the data is transferred within the the Amazon network and not through internet \u2013 Refer link) Security group of EC2 allowed in the RDS security group Configuring the elastic IP of the instance in RDS security group Configure the Public IP of the instance in RDS security group In regards to VPC, select the correct statement: You can associate multiple subnets with the same Route Table. You can associate multiple subnets with the same Route Table, but you can\u2019t associate a subnet with only one Route Table. You can\u2019t associate multiple subnets with the same Route Table. None of these. You need to design a VPC for a web-application consisting of an ELB a fleet of web application servers, and an RDS DB. The entire infrastructure must be distributed over 2 AZ. Which VPC configuration works while assuring the DB is not available from the Internet? One Public Subnet for ELB, one Public Subnet for the web-servers, and one private subnet for the DB One Public Subnet for ELB, two Private Subnets for the web-servers, and two private subnets for the RDS Two Public Subnets for ELB, two private Subnet for the web-servers, and two private subnet for the RDS Two Public Subnets for ELB, two Public Subnet for the web-servers, and two public subnets for the RDS You have an Amazon VPC with one private subnet and one public subnet with a Network Address Translator (NAT) server. You are creating a group of Amazon Elastic Cloud Compute (EC2) instances that configure themselves at startup via downloading a bootstrapping script from Amazon Simple Storage Service (S3) that deploys an application via GIT. Which setup provides the highest level of security? Amazon EC2 instances in private subnet, no EIPs, route outgoing traffic via the NAT Amazon EC2 instances in public subnet, no EIPs, route outgoing traffic via the Internet Gateway (IGW) Amazon EC2 instances in private subnet, assign EIPs, route outgoing traffic via the Internet Gateway (IGW) Amazon EC2 instances in public subnet, assign EIPs, route outgoing traffic via the NAT You have launched an Amazon Elastic Compute Cloud (EC2) instance into a public subnet with a primary private IP address assigned, an internet gateway is attached to the VPC, and the public route table is configured to send all Internet-based traffic to the Internet gateway. The instance security group is set to allow all outbound traffic but cannot access the Internet. Why is the Internet unreachable from this instance? The instance does not have a public IP address The Internet gateway security group must allow all outbound traffic. The instance security group must allow all inbound traffic. The instance \u201cSource/Destination check\u201d property must be enabled. You have an environment that consists of a public subnet using Amazon VPC and 3 instances that are running in this subnet. These three instances can successfully communicate with other hosts on the Internet. You launch a fourth instance in the same subnet, using the same AMI and security group configuration you used for the others, but find that this instance cannot be accessed from the internet. What should you do to enable Internet access? Deploy a NAT instance into the public subnet. Assign an Elastic IP address to the fourth instance Configure a publically routable IP Address in the host OS of the fourth instance. Modify the routing table for the public subnet. You have a load balancer configured for VPC, and all back-end Amazon EC2 instances are in service. However, your web browser times out when connecting to the load balancer\u2019s DNS name. Which options are probable causes of this behavior? Choose 2 answers The load balancer was not configured to use a public subnet with an Internet gateway configured The Amazon EC2 instances do not have a dynamically allocated private IP address The security groups or network ACLs are not property configured for web traffic. The load balancer is not configured in a private subnet with a NAT instance. The VPC does not have a VGW configured. When will you incur costs with an Elastic IP address (EIP)? When an EIP is allocated. When it is allocated and associated with a running instance. When it is allocated and associated with a stopped instance. Costs are incurred regardless of whether the EIP is associated with a running instance. A company currently has a VPC with EC2 Instances. A new instance being launched, which will host an application that works on IPv6. You need to ensure that this instance can initiate outgoing traffic to the Internet. At the same time, you need to ensure that no incoming connection can be initiated from the Internet on to the instance. Which of the following would you add to the VPC for this requirement? A NAT Instance A NAT Gateway An Internet Gateway","title":"16 VPC Exam"},{"location":"chap3/2bastion/","text":"L2 AWS Bastion Host 1 Bastion Host Overview Bastion means a structure for Fortification to protect things behind it In AWS, a Bastion host (also referred to as a Jump server) can be used to securely access instances in the private subnets . Bastion host launched in the Public subnet s would act as a primary access point from the Internet and acts as a proxy to other instances . 2 Key points Bastion host is deployed in the Public subnet and acts as a proxy or a gateway between you and your instances Bastion host is a security measure that helps to reduce attack on your infrastructure and you have to concentrate to hardening a single layer Bastion host allows you to login to instances in the Private subnet securely without having to store the private keys on the Bastion host (using ssh-agent forwarding or RDP gateways) Bastion host security can be further tightened to allow SSH/RDP access from specific trusted IPs or corporate IP ranges Bastion host for your AWS infrastructure shouldn\u2019t be used for any other purpose, as that could open unnecessary security holes Security for all the Instances in the private subnet should be hardened to accept SSH/RDP connections only from the Bastion host Deploy a Bastion host within each Availability Zone for HA, cause if the Bastion instance or the AZ hosting the Bastion server goes down the ability to connect to your private instances is lost completely 3 Exam A customer is running a multi-tier web application farm in a virtual private cloud (VPC) that is not connected to their corporate network. They are connecting to the VPC over the Internet to manage all of their Amazon EC2 instances running in both the public and private subnets. They have only authorized the bastion-security-group with Microsoft Remote Desktop Protocol (RDP) access to the application instance security groups, but the company wants to further limit administrative access to all of the instances in the VPC. Which of the following Bastion deployment scenarios will meet this requirement? Deploy a Windows Bastion host on the corporate network that has RDP access to all instances in the VPC. Deploy a Windows Bastion host with an Elastic IP address in the public subnet and allow SSH access to the bastion from anywhere. Deploy a Windows Bastion host with an Elastic IP address in the private subnet, and restrict RDP access to the bastion from only the corporate public IP addresses. Deploy a Windows Bastion host with an auto-assigned Public IP address in the public subnet, and allow RDP access to the bastion from only the corporate public IP addresses . You are designing a system that has a Bastion host. This component needs to be highly available without human intervention. Which of the following approaches would you select? Run the bastion on two instances one in each AZ Run the bastion on an active Instance in one AZ and have an AMI ready to boot up in the event of failure Configure the bastion instance in an Auto Scaling group Specify the Auto Scaling group to include multiple AZs but have a min-size of 1 and max-size of 1 Configure an ELB in front of the bastion instance You\u2019ve been brought in as solutions architect to assist an enterprise customer with their migration of an ecommerce platform to Amazon Virtual Private Cloud (VPC) The previous architect has already deployed a 3- tier VPC. The configuration is as follows: VPC vpc-2f8t>C447 IGW ig-2d8bc445 NACL acl-2080c448 Subnets and Route Tables: Web server\u2019s subnet-258bc44d Application server\u2019s subnet-248DC44c Database server\u2019s subnet-9189c6f9 Route Tables: rtb-2i8bc449 rtb-238bc44b Associations: Subnet-258bc44d: rtb-2i8bc449 Subnet-248DC44c: rtb-238bc44b Subnet-9189c6f9: rtb-238bc44b You are now ready to begin deploying EC2 instances into the VPC. Web servers must have direct access to the internet Application and database servers cannot have direct access to the internet. Which configuration below will allow you the ability to remotely administer your application and database servers, as well as allow these servers to retrieve updates from the Internet? Create a bastion and NAT Instance in subnet-258bc44d and add a route from rtb-238bc44b to subnet-258bc44d. (Route should point to the NAT) Add a route from rtb-238bc44b to igw-2d8bc445 and add a bastion and NAT instance within Subnet-248DC44c. (Adding IGW to routertb-238bc44b would expose the Application and Database server to internet. Bastion and NAT should be in public subnet) Create a Bastion and NAT Instance in subnet-258bc44d. Add a route from rtb-238bc44b to igw-2d8bc445. And a new NACL that allows access between subnet-258bc44d and subnet-248bc44c. (Route should point to NAT and not Internet Gateway else it would be internet accessible.) Create a Bastion and NAT instance in subnet-258bc44d and add a route from rtb-238bc44b to the NAT instance. (Bastion and NAT should be in the public subnet. As Web Server has direct access to Internet, the subnet subnet-258bc44d should be public and Route rtb-2i8bc449 pointing to IGW. Route rtb-238bc44b for private subnets should point to NAT for outgoing internet access) You are tasked with setting up a Linux bastion host for access to Amazon EC2 instances running in your VPC. Only clients connecting from the corporate external public IP address 72.34.51.100 should have SSH access to the host. Which option will meet the customer requirement? Security Group Inbound Rule: Protocol \u2013 TCP. Port Range \u2013 22, Source 72.34.51.100/32 Security Group Inbound Rule: Protocol \u2013 UDP, Port Range \u2013 22, Source 72.34.51.100/32 Network ACL Inbound Rule: Protocol \u2013 UDP, Port Range \u2013 22, Source 72.34.51.100/32 Network ACL Inbound Rule: Protocol \u2013 TCP, Port Range-22, Source 72.34.51.100/0","title":"L2 AWS Bastion Host"},{"location":"chap3/2bastion/#l2-aws-bastion-host","text":"","title":"L2 AWS Bastion Host"},{"location":"chap3/2bastion/#1-bastion-host-overview","text":"Bastion means a structure for Fortification to protect things behind it In AWS, a Bastion host (also referred to as a Jump server) can be used to securely access instances in the private subnets . Bastion host launched in the Public subnet s would act as a primary access point from the Internet and acts as a proxy to other instances .","title":"1 Bastion Host Overview"},{"location":"chap3/2bastion/#2-key-points","text":"Bastion host is deployed in the Public subnet and acts as a proxy or a gateway between you and your instances Bastion host is a security measure that helps to reduce attack on your infrastructure and you have to concentrate to hardening a single layer Bastion host allows you to login to instances in the Private subnet securely without having to store the private keys on the Bastion host (using ssh-agent forwarding or RDP gateways) Bastion host security can be further tightened to allow SSH/RDP access from specific trusted IPs or corporate IP ranges Bastion host for your AWS infrastructure shouldn\u2019t be used for any other purpose, as that could open unnecessary security holes Security for all the Instances in the private subnet should be hardened to accept SSH/RDP connections only from the Bastion host Deploy a Bastion host within each Availability Zone for HA, cause if the Bastion instance or the AZ hosting the Bastion server goes down the ability to connect to your private instances is lost completely","title":"2 Key points"},{"location":"chap3/2bastion/#3-exam","text":"A customer is running a multi-tier web application farm in a virtual private cloud (VPC) that is not connected to their corporate network. They are connecting to the VPC over the Internet to manage all of their Amazon EC2 instances running in both the public and private subnets. They have only authorized the bastion-security-group with Microsoft Remote Desktop Protocol (RDP) access to the application instance security groups, but the company wants to further limit administrative access to all of the instances in the VPC. Which of the following Bastion deployment scenarios will meet this requirement? Deploy a Windows Bastion host on the corporate network that has RDP access to all instances in the VPC. Deploy a Windows Bastion host with an Elastic IP address in the public subnet and allow SSH access to the bastion from anywhere. Deploy a Windows Bastion host with an Elastic IP address in the private subnet, and restrict RDP access to the bastion from only the corporate public IP addresses. Deploy a Windows Bastion host with an auto-assigned Public IP address in the public subnet, and allow RDP access to the bastion from only the corporate public IP addresses . You are designing a system that has a Bastion host. This component needs to be highly available without human intervention. Which of the following approaches would you select? Run the bastion on two instances one in each AZ Run the bastion on an active Instance in one AZ and have an AMI ready to boot up in the event of failure Configure the bastion instance in an Auto Scaling group Specify the Auto Scaling group to include multiple AZs but have a min-size of 1 and max-size of 1 Configure an ELB in front of the bastion instance You\u2019ve been brought in as solutions architect to assist an enterprise customer with their migration of an ecommerce platform to Amazon Virtual Private Cloud (VPC) The previous architect has already deployed a 3- tier VPC. The configuration is as follows: VPC vpc-2f8t>C447 IGW ig-2d8bc445 NACL acl-2080c448 Subnets and Route Tables: Web server\u2019s subnet-258bc44d Application server\u2019s subnet-248DC44c Database server\u2019s subnet-9189c6f9 Route Tables: rtb-2i8bc449 rtb-238bc44b Associations: Subnet-258bc44d: rtb-2i8bc449 Subnet-248DC44c: rtb-238bc44b Subnet-9189c6f9: rtb-238bc44b You are now ready to begin deploying EC2 instances into the VPC. Web servers must have direct access to the internet Application and database servers cannot have direct access to the internet. Which configuration below will allow you the ability to remotely administer your application and database servers, as well as allow these servers to retrieve updates from the Internet? Create a bastion and NAT Instance in subnet-258bc44d and add a route from rtb-238bc44b to subnet-258bc44d. (Route should point to the NAT) Add a route from rtb-238bc44b to igw-2d8bc445 and add a bastion and NAT instance within Subnet-248DC44c. (Adding IGW to routertb-238bc44b would expose the Application and Database server to internet. Bastion and NAT should be in public subnet) Create a Bastion and NAT Instance in subnet-258bc44d. Add a route from rtb-238bc44b to igw-2d8bc445. And a new NACL that allows access between subnet-258bc44d and subnet-248bc44c. (Route should point to NAT and not Internet Gateway else it would be internet accessible.) Create a Bastion and NAT instance in subnet-258bc44d and add a route from rtb-238bc44b to the NAT instance. (Bastion and NAT should be in the public subnet. As Web Server has direct access to Internet, the subnet subnet-258bc44d should be public and Route rtb-2i8bc449 pointing to IGW. Route rtb-238bc44b for private subnets should point to NAT for outgoing internet access) You are tasked with setting up a Linux bastion host for access to Amazon EC2 instances running in your VPC. Only clients connecting from the corporate external public IP address 72.34.51.100 should have SSH access to the host. Which option will meet the customer requirement? Security Group Inbound Rule: Protocol \u2013 TCP. Port Range \u2013 22, Source 72.34.51.100/32 Security Group Inbound Rule: Protocol \u2013 UDP, Port Range \u2013 22, Source 72.34.51.100/32 Network ACL Inbound Rule: Protocol \u2013 UDP, Port Range \u2013 22, Source 72.34.51.100/32 Network ACL Inbound Rule: Protocol \u2013 TCP, Port Range-22, Source 72.34.51.100/0","title":"3 Exam"},{"location":"chap3/3direct_connect/","text":"L3 AWS Direct Connect \u2013 DX 1\u3001Direct Connect Overview AWS Direct Connect is a network service that provides an alternative to using the Internet to utilize AWS cloud services AWS Direct Connect links your internal network to an AWS Direct Connect location over a standard 1 gigabit or 10 gigabit Ethernet fiber-optic cable with one end of the cable connected to your router, the other to an AWS Direct Connect router. \u3001 Direct Connect connection can be established with 1Gbps and 10Gbps ports. Speeds of 50Mbps, 100Mbps, 200Mbps, 300Mbps, 400Mbps, and 500Mbps can be ordered from any APN partners supporting AWS Direct Connect. AWS Direct Connect helps to create virtual interfaces directly to the AWS cloud for e.g, to EC2 & S3 and to Virtual Private Cloud (VPC) , bypassing Internet service providers in the network path. AWS Direct Connect location provides access to Amazon Web Services in the region it is associated with, as well as access to other US regions (in case of a Direct Connect in a US region). for e.g. , you can provision a single connection to any AWS Direct Connect location in the US and use it to access public AWS services in all US Regions and AWS GovCloud (US) . Each AWS Direct Connect location enables connectivity to all Availability Zones within the geographically nearest AWS region. 2\u3001Direct Connect Advantages 2-1 Reduced Bandwidth Costs All data transferred over the dedicated connection is charged at the reduced AWS Direct Connect data transfer rate rather than Internet data transfer rates. Transferring data to and from AWS directly reduces your bandwidth commitment to your Internet service provider 2-2 Consistent Network Performance Direct Connect provides a dedicated connection and a more consistent network performance experience as compared to the Internet which can widely vary 2-3 AWS Services Compatibility Direct Connect is a network service and works with all of the AWS services like S3, EC2 and VPC 2-4 Private Connectivity to AWS VPC Using Direct Connect Private Virtual Interface a private, dedicated, high bandwidth network connection can be established between your network and VPC 2-5 Elastic Direct Connect can be easily scaled to meet the needs by either using a higher bandwidth connection or by establishing multiple connections. 3\u3001Direct Connect vs IPSec VPN Connections A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution for immediate needs, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC VPN connections are very cheap ($37.20/month as of now) as compared to Direct Connect connection as it requires actual hardware and infrastructure and might go in thousands. 4\u3001Direct Connect Anatomy Amazon maintains AWS Direct Connect PoP across different locations (referred to as Colocation Facilities) which are different from AWS regions Connection from the AWS Direct Connect PoP to the AWS regions is maintained by AWS itself As a consumer, you can either purchase a rack space or use any of the AWS APN Partner which already have the infrastructure within the Colocation Facility and configure a Customer Gateway Connection between the AWS Direct Connect PoP and the Customer gateway within the Colocation Facility is called Cross Connect . Connection from the Customer Gateway to the Customer Data Center can be establish using any Service Provider Network Once a Direct Connect connection is created with AWS, a LOA-CFA (Letter Of Authority \u2013 Connecting Facility Assignment) would be received. LOA-CFA can be handover to the Colocation Facility or the APN Partner to establish the Cross Connect Once the Cross Connect and the connectivity between the CGW and Customer DataCenter is established, Virtual Interfaces can be created AWS Direct Connect requires a VGW to access the AWS VPC 4-1 Virtual Interfaces Each AWS Direct Connect connection requires a Virtual Interface Each AWS Direct Connect connection can be configured with one or more virtual interfaces. Public Virtual Interface can be created to connect to public resources for e.g. SQS, S3, EC2, Glacier etc which are reachable publicly only. Private virtual interface can be created to connect to the VPC for e.g. instances with private ip address Each virtual interface needs a VLAN ID, interface IP address, ASN, and BGP key. To use your AWS Direct Connect connection with another AWS account, you can create a hosted virtual interface for that account. These hosted virtual interfaces work the same as standard virtual interfaces and can connect to public resources or a VPC. 5\u3001Direct Connect Redundancy Direct Connect connections do not provide redundancy and have multiple single point of failures wrt to the hardware devices as each connection consists of a single dedicated connection between ports on your router and an Amazon router Redundancy can be provided by Establishing a second Direct Connect connection, preferably in a different Colocation Facility using different router and AWS Direct Connect PoP IPsec VPN connection between the Customer DC to the VGW For Multiple ports requested in the same AWS Direct Connect location, Amazon itself makes sure they are provisioned on redundant Amazon routers to prevent impact from an hardware failure 6\u3001Direct Connect LAG A link aggregation group (LAG) is a logical interface that uses the Link Aggregation Control Protocol (LACP) to aggregate multiple connections at a single AWS Direct Connect endpoint, treating them as a single, managed connection. LAG can be created from existing connections, or you can provision new connections . Existing connections (whether standalone or part of another LAG) with the LAG can be associated after LAG creation LAG needs following rules All connections in the LAG must use the same bandwidth. Maximum of four connections in a LAG. Each connection in the LAG counts towards the overall connection limit for the Region. All connections in the LAG must terminate at the same AWS Direct Connect endpoint. 7\u3001Direct Connect - Exam You are building a solution for a customer to extend their on-premises data center to AWS. The customer requires a 50-Mbps dedicated and private connection to their VPC . Which AWS product or feature satisfies this requirement? Amazon VPC peering Elastic IP Addresses AWS Direct Connect Amazon VPC virtual private gateway Is there any way to own a direct connection to Amazon Web Service s? You can create an encrypted tunnel to VPC, but you don\u2019t own the connection. Yes, it\u2019s called Amazon Dedicated Connection. No, AWS only allows access from the public Internet. Yes, it\u2019s called Direct Connect An organization has established an Internet-based VPN connection between their on-premises data center and AWS. They are considering migrating from VPN to AWS Direct Connect. Which operational concern should drive an organization to consider switching from an Internet-based VPN connection to AWS Direct Connect? AWS Direct Connect provides greater redundancy than an Internet-based VPN connection. AWS Direct Connect provides greater resiliency than an Internet-based VPN connection. AWS Direct Connect provides greater bandwidth than an Internet-based VPN connection . AWS Direct Connect provides greater control of network provider selection than an Internet-based VPN connection. Does AWS Direct Connect allow you access to all Availabilities Zones within a Region? Depends on the type of connection No Yes Only when there\u2019s just one availability zone in a region. If there are more than one, only one availability zone can be accessed directly. A customer has established an AWS Direct Connect connection to AWS. The link is up and routes are being advertised from the customer\u2019s end, however the customer is unable to connect from EC2 instances inside its VPC to servers residing in its datacenter. Which of the following options provide a viable solution to remedy this situation? (Choose 2 answers) Add a route to the route table with an IPSec VPN connection as the target (deals with VPN) Enable route propagation to the Virtual Private Gateway (VGW) Enable route propagation to the customer gateway (CGW) (route propagation is enabled on VGW) Modify the route table of all Instances using the \u2018route\u2019 command. (no route command available) Modify the Instances VPC subnet route table by adding a route back to the customer\u2019s on-premises environment. A company has configured and peered two VPCs: VPC-1 and VPC-2. VPC-1 contains only private subnets, and VPC-2 contains only public subnets. The company uses a single AWS Direct Connect connection and private virtual interface to connect their on-premises network with VPC-1. Which two methods increase the fault tolerance of the connection to VPC-1? Choose 2 answers Establish a hardware VPN over the internet between VPC-2 and the on-premises network. (Peered VPC does not support Edge to Edge Routing) Establish a hardware VPN over the internet between VPC-1 and the on-premises network Establish a new AWS Direct Connect connection and private virtual interface in the same region as VPC-2 (Peered VPC does not support Edge to Edge Routing) Establish a new AWS Direct Connect connection and private virtual interface in a different AWS region than VPC-1 (need to be in the same region as VPC-1) Establish a new AWS Direct Connect connection and private virtual interface in the same AWS region as VPC-1 Your company previously configured a heavily used, dynamically routed VPN connection between your on premises data center and AWS. You recently provisioned a Direct Connect connection and would like to start using the new connection. After configuring Direct Connect settings in the AWS Console, which of the following options will provide the most seamless transition for your users? Delete your existing VPN connection to avoid routing loops configure your Direct Connect router with the appropriate settings and verity network traffic is leveraging Direct Connect. Configure your Direct Connect router with a higher BGP priority than your VPN router, verify network traffic is leveraging Direct Connect and then delete your existing VPN connection. Update your VPC route tables to point to the Direct Connect connection configure your Direct Connect router with the appropriate settings verify network traffic is leveraging Direct Connect and then delete the VPN connection . Configure your Direct Connect router, update your VPC route tables to point to the Direct Connect connection, configure your VPN connection with a higher BGP priority. And verify network traffic is leveraging the Direct Connect connection You are designing the network infrastructure for an application server in Amazon VPC. Users will access all the application instances from the Internet as well as from an on-premises network The on-premises network is connected to your VPC over an AWS Direct Connect link. How would you design routing to meet the above requirements? Configure a single routing Table with a default route via the Internet gateway. Propagate a default route via BGP on the AWS Direct Connect customer router. Associate the routing table with all VPC subnets (propagating default route would cause conflict) Configure a single routing table with a default route via the internet gateway. Propagate specific routes for the on-premises networks via BGP on the AWS Direct Connect customer router. Associate the routing table with all VPC subnets. Configure a single routing table with two default routes: one to the internet via an Internet gateway the other to the on-premises network via the VPN gateway use this routing table across all subnets in your VPC. (there cannot be 2 default routes) Configure two routing tables one that has a default route via the Internet gateway and another that has a default route via the VPN gateway Associate both routing tables with each VPC subnet. (as the instances has to be in public subnet and should have a single routing table associated with them) You are implementing AWS Direct Connect. You intend to use AWS public service end points such as Amazon S3, across the AWS Direct Connect link. You want other Internet traffic to use your existing link to an Internet Service Provider. What is the correct way to configure AWS Direct Connect for access to services such as Amazon S3? Configure a public Interface on your AWS Direct Connect link. Configure a static route via your AWS Direct Connect link that points to Amazon S3. Advertise a default route to AWS using BGP. Create a private interface on your AWS Direct Connect link. Configure a static route via your AWS Direct connect link that points to Amazon S3 Configure specific routes to your network in your VPC. Create a public interface on your AWS Direct Connect link. Redistribute BGP routes into your existing routing infrastructure advertise specific routes for your network to AWS Create a private interface on your AWS Direct connect link. Redistribute BGP routes into your existing routing infrastructure and advertise a default route to AWS. You have been asked to design network connectivity between your existing data centers and AWS. Your application\u2019s EC2 instances must be able to connect to existing backend resources located in your data center. Network traffic between AWS and your data centers will start small, but ramp up to 10s of GB per second over the course of several months. The success of your application is dependent upon getting to market quickly. Which of the following design options will allow you to meet your objectives? Quickly create an internal ELB for your backend applications, submit a DirectConnect request to provision a 1 Gbps cross connect between your data center and VPC, then increase the number or size of your DirectConnect connections as needed. Allocate EIPs and an Internet Gateway for your VPC instances to use for quick, temporary access to your backend applications, then provision a VPN connection between a VPC and existing on -premises equipment. Provision a VPN connection between a VPC and existing on -premises equipment, submit a DirectConnect partner request to provision cross connects between your data center and the DirectConnect location, then cut over from the VPN connection to one or more DirectConnect connections as needed. Quickly submit a DirectConnect request to provision a 1 Gbps cross connect between your data center and VPC, then increase the number or size of your DirectConnect connections as needed. You are tasked with moving a legacy application from a virtual machine running inside your datacenter to an Amazon VPC. Unfortunately this app requires access to a number of on-premises services and no one who configured the app still works for your company. Even worse there\u2019s no documentation for it. What will allow the application running inside the VPC to reach back and access its internal dependencies without being reconfigured? (Choose 3 answers) An AWS Direct Connect link between the VPC and the network housing the internal services (VPN or a DX for communication) An Internet Gateway to allow a VPN connection. (Virtual and Customer gateway is needed) An Elastic IP address on the VPC instance (Don\u2019t need a EIP as private subnets can also interact with on-premises network) An IP address space that does not conflict with the one on-premises (IP address cannot conflict) Entries in Amazon Route 53 that allow the Instance to resolve its dependencies\u2019 IP addresses (Route 53 is not required) A VM Import of the current virtual machine (VM Import to copy the VM to AWS as there is no documentation it can\u2019t be configured from scratch)","title":"L3 AWS Direct Connect \u2013 DX"},{"location":"chap3/3direct_connect/#l3-aws-direct-connect-dx","text":"","title":"L3 AWS Direct Connect \u2013 DX"},{"location":"chap3/3direct_connect/#1direct-connect-overview","text":"AWS Direct Connect is a network service that provides an alternative to using the Internet to utilize AWS cloud services AWS Direct Connect links your internal network to an AWS Direct Connect location over a standard 1 gigabit or 10 gigabit Ethernet fiber-optic cable with one end of the cable connected to your router, the other to an AWS Direct Connect router. \u3001 Direct Connect connection can be established with 1Gbps and 10Gbps ports. Speeds of 50Mbps, 100Mbps, 200Mbps, 300Mbps, 400Mbps, and 500Mbps can be ordered from any APN partners supporting AWS Direct Connect. AWS Direct Connect helps to create virtual interfaces directly to the AWS cloud for e.g, to EC2 & S3 and to Virtual Private Cloud (VPC) , bypassing Internet service providers in the network path. AWS Direct Connect location provides access to Amazon Web Services in the region it is associated with, as well as access to other US regions (in case of a Direct Connect in a US region). for e.g. , you can provision a single connection to any AWS Direct Connect location in the US and use it to access public AWS services in all US Regions and AWS GovCloud (US) . Each AWS Direct Connect location enables connectivity to all Availability Zones within the geographically nearest AWS region.","title":"1\u3001Direct Connect Overview"},{"location":"chap3/3direct_connect/#2direct-connect-advantages","text":"","title":"2\u3001Direct Connect Advantages"},{"location":"chap3/3direct_connect/#2-1-reduced-bandwidth-costs","text":"All data transferred over the dedicated connection is charged at the reduced AWS Direct Connect data transfer rate rather than Internet data transfer rates. Transferring data to and from AWS directly reduces your bandwidth commitment to your Internet service provider","title":"2-1 Reduced Bandwidth Costs"},{"location":"chap3/3direct_connect/#2-2-consistent-network-performance","text":"Direct Connect provides a dedicated connection and a more consistent network performance experience as compared to the Internet which can widely vary","title":"2-2 Consistent Network Performance"},{"location":"chap3/3direct_connect/#2-3-aws-services-compatibility","text":"Direct Connect is a network service and works with all of the AWS services like S3, EC2 and VPC","title":"2-3 AWS Services Compatibility"},{"location":"chap3/3direct_connect/#2-4-private-connectivity-to-aws-vpc","text":"Using Direct Connect Private Virtual Interface a private, dedicated, high bandwidth network connection can be established between your network and VPC","title":"2-4 Private Connectivity to AWS VPC"},{"location":"chap3/3direct_connect/#2-5-elastic","text":"Direct Connect can be easily scaled to meet the needs by either using a higher bandwidth connection or by establishing multiple connections.","title":"2-5 Elastic"},{"location":"chap3/3direct_connect/#3direct-connect-vs-ipsec-vpn-connections","text":"A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution for immediate needs, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your intranet and Amazon VPC VPN connections are very cheap ($37.20/month as of now) as compared to Direct Connect connection as it requires actual hardware and infrastructure and might go in thousands.","title":"3\u3001Direct Connect vs IPSec VPN Connections"},{"location":"chap3/3direct_connect/#4direct-connect-anatomy","text":"Amazon maintains AWS Direct Connect PoP across different locations (referred to as Colocation Facilities) which are different from AWS regions Connection from the AWS Direct Connect PoP to the AWS regions is maintained by AWS itself As a consumer, you can either purchase a rack space or use any of the AWS APN Partner which already have the infrastructure within the Colocation Facility and configure a Customer Gateway Connection between the AWS Direct Connect PoP and the Customer gateway within the Colocation Facility is called Cross Connect . Connection from the Customer Gateway to the Customer Data Center can be establish using any Service Provider Network Once a Direct Connect connection is created with AWS, a LOA-CFA (Letter Of Authority \u2013 Connecting Facility Assignment) would be received. LOA-CFA can be handover to the Colocation Facility or the APN Partner to establish the Cross Connect Once the Cross Connect and the connectivity between the CGW and Customer DataCenter is established, Virtual Interfaces can be created AWS Direct Connect requires a VGW to access the AWS VPC","title":"4\u3001Direct Connect Anatomy"},{"location":"chap3/3direct_connect/#4-1-virtual-interfaces","text":"Each AWS Direct Connect connection requires a Virtual Interface Each AWS Direct Connect connection can be configured with one or more virtual interfaces. Public Virtual Interface can be created to connect to public resources for e.g. SQS, S3, EC2, Glacier etc which are reachable publicly only. Private virtual interface can be created to connect to the VPC for e.g. instances with private ip address Each virtual interface needs a VLAN ID, interface IP address, ASN, and BGP key. To use your AWS Direct Connect connection with another AWS account, you can create a hosted virtual interface for that account. These hosted virtual interfaces work the same as standard virtual interfaces and can connect to public resources or a VPC.","title":"4-1 Virtual Interfaces"},{"location":"chap3/3direct_connect/#5direct-connect-redundancy","text":"Direct Connect connections do not provide redundancy and have multiple single point of failures wrt to the hardware devices as each connection consists of a single dedicated connection between ports on your router and an Amazon router Redundancy can be provided by Establishing a second Direct Connect connection, preferably in a different Colocation Facility using different router and AWS Direct Connect PoP IPsec VPN connection between the Customer DC to the VGW For Multiple ports requested in the same AWS Direct Connect location, Amazon itself makes sure they are provisioned on redundant Amazon routers to prevent impact from an hardware failure","title":"5\u3001Direct Connect Redundancy"},{"location":"chap3/3direct_connect/#6direct-connect-lag","text":"A link aggregation group (LAG) is a logical interface that uses the Link Aggregation Control Protocol (LACP) to aggregate multiple connections at a single AWS Direct Connect endpoint, treating them as a single, managed connection. LAG can be created from existing connections, or you can provision new connections . Existing connections (whether standalone or part of another LAG) with the LAG can be associated after LAG creation LAG needs following rules All connections in the LAG must use the same bandwidth. Maximum of four connections in a LAG. Each connection in the LAG counts towards the overall connection limit for the Region. All connections in the LAG must terminate at the same AWS Direct Connect endpoint.","title":"6\u3001Direct Connect LAG"},{"location":"chap3/3direct_connect/#7direct-connect-exam","text":"You are building a solution for a customer to extend their on-premises data center to AWS. The customer requires a 50-Mbps dedicated and private connection to their VPC . Which AWS product or feature satisfies this requirement? Amazon VPC peering Elastic IP Addresses AWS Direct Connect Amazon VPC virtual private gateway Is there any way to own a direct connection to Amazon Web Service s? You can create an encrypted tunnel to VPC, but you don\u2019t own the connection. Yes, it\u2019s called Amazon Dedicated Connection. No, AWS only allows access from the public Internet. Yes, it\u2019s called Direct Connect An organization has established an Internet-based VPN connection between their on-premises data center and AWS. They are considering migrating from VPN to AWS Direct Connect. Which operational concern should drive an organization to consider switching from an Internet-based VPN connection to AWS Direct Connect? AWS Direct Connect provides greater redundancy than an Internet-based VPN connection. AWS Direct Connect provides greater resiliency than an Internet-based VPN connection. AWS Direct Connect provides greater bandwidth than an Internet-based VPN connection . AWS Direct Connect provides greater control of network provider selection than an Internet-based VPN connection. Does AWS Direct Connect allow you access to all Availabilities Zones within a Region? Depends on the type of connection No Yes Only when there\u2019s just one availability zone in a region. If there are more than one, only one availability zone can be accessed directly. A customer has established an AWS Direct Connect connection to AWS. The link is up and routes are being advertised from the customer\u2019s end, however the customer is unable to connect from EC2 instances inside its VPC to servers residing in its datacenter. Which of the following options provide a viable solution to remedy this situation? (Choose 2 answers) Add a route to the route table with an IPSec VPN connection as the target (deals with VPN) Enable route propagation to the Virtual Private Gateway (VGW) Enable route propagation to the customer gateway (CGW) (route propagation is enabled on VGW) Modify the route table of all Instances using the \u2018route\u2019 command. (no route command available) Modify the Instances VPC subnet route table by adding a route back to the customer\u2019s on-premises environment. A company has configured and peered two VPCs: VPC-1 and VPC-2. VPC-1 contains only private subnets, and VPC-2 contains only public subnets. The company uses a single AWS Direct Connect connection and private virtual interface to connect their on-premises network with VPC-1. Which two methods increase the fault tolerance of the connection to VPC-1? Choose 2 answers Establish a hardware VPN over the internet between VPC-2 and the on-premises network. (Peered VPC does not support Edge to Edge Routing) Establish a hardware VPN over the internet between VPC-1 and the on-premises network Establish a new AWS Direct Connect connection and private virtual interface in the same region as VPC-2 (Peered VPC does not support Edge to Edge Routing) Establish a new AWS Direct Connect connection and private virtual interface in a different AWS region than VPC-1 (need to be in the same region as VPC-1) Establish a new AWS Direct Connect connection and private virtual interface in the same AWS region as VPC-1 Your company previously configured a heavily used, dynamically routed VPN connection between your on premises data center and AWS. You recently provisioned a Direct Connect connection and would like to start using the new connection. After configuring Direct Connect settings in the AWS Console, which of the following options will provide the most seamless transition for your users? Delete your existing VPN connection to avoid routing loops configure your Direct Connect router with the appropriate settings and verity network traffic is leveraging Direct Connect. Configure your Direct Connect router with a higher BGP priority than your VPN router, verify network traffic is leveraging Direct Connect and then delete your existing VPN connection. Update your VPC route tables to point to the Direct Connect connection configure your Direct Connect router with the appropriate settings verify network traffic is leveraging Direct Connect and then delete the VPN connection . Configure your Direct Connect router, update your VPC route tables to point to the Direct Connect connection, configure your VPN connection with a higher BGP priority. And verify network traffic is leveraging the Direct Connect connection You are designing the network infrastructure for an application server in Amazon VPC. Users will access all the application instances from the Internet as well as from an on-premises network The on-premises network is connected to your VPC over an AWS Direct Connect link. How would you design routing to meet the above requirements? Configure a single routing Table with a default route via the Internet gateway. Propagate a default route via BGP on the AWS Direct Connect customer router. Associate the routing table with all VPC subnets (propagating default route would cause conflict) Configure a single routing table with a default route via the internet gateway. Propagate specific routes for the on-premises networks via BGP on the AWS Direct Connect customer router. Associate the routing table with all VPC subnets. Configure a single routing table with two default routes: one to the internet via an Internet gateway the other to the on-premises network via the VPN gateway use this routing table across all subnets in your VPC. (there cannot be 2 default routes) Configure two routing tables one that has a default route via the Internet gateway and another that has a default route via the VPN gateway Associate both routing tables with each VPC subnet. (as the instances has to be in public subnet and should have a single routing table associated with them) You are implementing AWS Direct Connect. You intend to use AWS public service end points such as Amazon S3, across the AWS Direct Connect link. You want other Internet traffic to use your existing link to an Internet Service Provider. What is the correct way to configure AWS Direct Connect for access to services such as Amazon S3? Configure a public Interface on your AWS Direct Connect link. Configure a static route via your AWS Direct Connect link that points to Amazon S3. Advertise a default route to AWS using BGP. Create a private interface on your AWS Direct Connect link. Configure a static route via your AWS Direct connect link that points to Amazon S3 Configure specific routes to your network in your VPC. Create a public interface on your AWS Direct Connect link. Redistribute BGP routes into your existing routing infrastructure advertise specific routes for your network to AWS Create a private interface on your AWS Direct connect link. Redistribute BGP routes into your existing routing infrastructure and advertise a default route to AWS. You have been asked to design network connectivity between your existing data centers and AWS. Your application\u2019s EC2 instances must be able to connect to existing backend resources located in your data center. Network traffic between AWS and your data centers will start small, but ramp up to 10s of GB per second over the course of several months. The success of your application is dependent upon getting to market quickly. Which of the following design options will allow you to meet your objectives? Quickly create an internal ELB for your backend applications, submit a DirectConnect request to provision a 1 Gbps cross connect between your data center and VPC, then increase the number or size of your DirectConnect connections as needed. Allocate EIPs and an Internet Gateway for your VPC instances to use for quick, temporary access to your backend applications, then provision a VPN connection between a VPC and existing on -premises equipment. Provision a VPN connection between a VPC and existing on -premises equipment, submit a DirectConnect partner request to provision cross connects between your data center and the DirectConnect location, then cut over from the VPN connection to one or more DirectConnect connections as needed. Quickly submit a DirectConnect request to provision a 1 Gbps cross connect between your data center and VPC, then increase the number or size of your DirectConnect connections as needed. You are tasked with moving a legacy application from a virtual machine running inside your datacenter to an Amazon VPC. Unfortunately this app requires access to a number of on-premises services and no one who configured the app still works for your company. Even worse there\u2019s no documentation for it. What will allow the application running inside the VPC to reach back and access its internal dependencies without being reconfigured? (Choose 3 answers) An AWS Direct Connect link between the VPC and the network housing the internal services (VPN or a DX for communication) An Internet Gateway to allow a VPN connection. (Virtual and Customer gateway is needed) An Elastic IP address on the VPC instance (Don\u2019t need a EIP as private subnets can also interact with on-premises network) An IP address space that does not conflict with the one on-premises (IP address cannot conflict) Entries in Amazon Route 53 that allow the Instance to resolve its dependencies\u2019 IP addresses (Route 53 is not required) A VM Import of the current virtual machine (VM Import to copy the VM to AWS as there is no documentation it can\u2019t be configured from scratch)","title":"7\u3001Direct Connect - Exam"},{"location":"chap3/4ELB/","text":"L4 AWS Elastic Load Balancer \u2013 ELB 1\u3001AWS Elastic Load Balancer \u2013 ELB Elastic Load Balancer allows the incoming traffic to be distributed automatically across multiple healthy EC2 instances . ELB serves as a single point of contact to the client ELB helps to being transparent and increases the application availability by allowing the addition or removal of multiple EC2 instances across one or more AZs , without disrupting the overall flow of information. 1-1 ELB benefits is a distributed system that is fault-tolerant and actively monitored abstracts out the complexity of managing, maintaining, and scaling load balancers can also serve as the first line of defense against attacks on network\\ can offload the work of encryption and decryption (SSL termination) so that the EC2 instances can focus on their main work offers integration with Auto Scaling, which ensures enough back-end capacity available to meet varying traffic levels are engineered to not be a single point of failure 1-2 Load Balancers only work across AZs within a region 2\u3001AWS Elastic Load Balancer Types Elastic Load Balancing supports three types of load balancers: Application Load Balancer Network Load Balancer Elastic Load Balancer 3\u3001Elastic Load Balancer Features Following ELB key concepts apply to all the Elastic Load Balancer types 3-1 Scaling ELB Each ELB is allocated and configured with a default capacity ELB Controller is the service that stores all the configurations and also monitors the load balancer and manages the capacity that is used to handle the client requests As the traffic profile changes, the controller service scales the load balancers to handle more requests, scaling equally in all AZs . ELB increases its capacity by utilizing either larger resources (scale up \u2013 resources with higher performance characteristics) or more individual resources (scale-out) . AWS itself handles the scaling of the ELB capacity and this scaling is different to scaling of the EC2 instances to which the ELB routes its request, which is handled by Auto Scaling Time required for Elastic Load Balancing to scale can range from 1 to 7 minutes, depending on the changes in the traffic profile When an Availability Zone is enabled for the load balancer, Elastic Load Balancing creates a load balancer node in the Availability Zone. By default, each load balancer node distributes traffic across the registered targets in its Availability Zone only. 3-2 Pre-Warming ELB NOTE \u2013 AWS documentation does not include Pre-warming now ELB works best with a gradual increase in traffic AWS is able to scale automatically and handle a vast majority of use cases However, in certain scenarios, if there is a flash traffic spike expected or a load test cannot be configured to gradually increase traffic, recommended to contact AWS support to have the load balancer \u201cpre-warmed\u201d A WS will help Pre-warming the ELB, by configuring the load balancer to have the appropriate level of capacity based on the expected traffic AWS would need the information for the start, end dates, and the expected request rate per second with the total size of request/response. 3-3 DNS Resolution ELB is scaled automatically depending on the traffic profile When scaled, Elastic Load Balancing service will update the Domain Name System (DNS) record of the load balancer so that the new resources have their respective IP addresses registered in DNS . DNS record created includes a Time-to-Live (TTL) setting of 60 seconds By default, ELB will return multiple IP addresses when clients perform a DNS resolution, with the records being randomly ordered on each DNS resolution request. It is recommended that clients will re-lookup the DNS at least every 60 seconds to take advantage of the increased capacity 4\u3001Load Balancer Types Internet Load Balancer An Internet-facing load balancer takes requests from clients over the Internet and distributes them across the EC2 instances that are registered with the load balancer Internal Load Balancer \u2013 Internal load balancer routes traffic to EC2 instances in private subnets 5\u3001Availability Zones/Subnets Elastic Load Balancer should have at least one subnet attached Elastic Load Balancing allows subnets to be added and creates a load balancer node in each of the Availability Zone where the subnet resides. Only one subnet per AZ can be attached to the ELB. Attaching a subnet with an AZ already attached replaces the existing subnet Each Subnet must have a CIDR block with at least a /27 bitmask and h as at least 8 free IP addresses , which ELB uses to establish connections with the back-end instances. For High Availability, it is recommended to attach one subnet per AZ for at least two AZs, even if the instances are in a single subnet. Subnets can be attached or detached from the ELB and it would start or stop sending requests to the instances in the subnet accordingly 6\u3001Security Groups & NACL Security groups & NACLs should allow Inbound traffic, on the load balancer listener port, from the Client for an Internet ELB or VPC CIDR for an Internal ELB Security groups & NACLs should allow Outbound traffic to the back-end instances on both the instance listener port and the health check port NACLs, in addition, should allow responses on the ephemeral ports All EC2 instances should allow incoming traffic from ELB 7\u3001SSL Negotiation Configuration For HTTPS load balancer, Elastic Load Balancing uses a Secure Socket Layer (SSL) negotiation configuration, known as a security policy, to negotiate SSL connections between a client and the load balancer. A security policy is a combination of SSL protocols, SSL ciphers, and the Server Order Preference option Elastic Load Balancing supports the following versions of the SSL protocol TLS 1.2, TLS 1.1, TLS 1.0, SSL 3.0 SSL protocols use several SSL ciphers to encrypt data over the Internet . Elastic Load Balancing supports the Server Order Preference option for negotiating connections between a client and a load balancer. During the SSL connection negotiation process, this allows the load balancer to control and select the first cipher in its list that is in the client\u2019s list of ciphers instead of the default behavior of checking matching first cipher in client\u2019s list with server\u2019s list. Elastic Load Balancer allows using a Predefined Security Policies or creating a Custom Security Policy for specific needs . If none is specified, ELB selects the latest Predefined Security Policy. Elastic Load Balancer support multiple certificates using Server Name Indication (SNI) If the hostname provided by a client matches a single certificate in the certificate list, the load balancer selects this certificate . If a hostname provided by a client matches multiple certificates in the certificate list, the load balancer selects the best certificate that the client can support. Classic Load Balancer does not support multiple certificates ALB and NLB support multiple certificates 8\u3001Health Checks Load balancer performs health checks on all registered instances, whether the instance is in a healthy state or an unhealthy state. Load balancer performs health checks to discover the availability of the EC2 instances, the load balancer periodically sends pings, attempts connections, or sends request to health check the EC2 instances. Health check is InService for status of healthy instances and OutOfService for unhealthy ones Load balancer sends a request to each registered instance at the * Ping Protocol, Ping Port and Ping Path every HealthCheck Interval seconds . It waits for the instance to respond within the Response Timeout period. If the health checks exceed the Unhealthy Threshold for consecutive failed responses, the load balancer takes the instance out of service . When the health checks exceed the Healthy Threshold for consecutive successful responses, the load balancer puts the instance back in service. Load balancer only sends requests to the healthy EC2 instances and stops routing requests to the unhealthy instances All ELB types support health checks 9\u3001Listeners Listeners is the process that checks for connection requests from client Listeners are configured with a protocol and a port for front-end (client to load balancer) connections, and a protocol and a port for back-end (load balancer to back-end instance) connections. Listeners support HTTP, HTTPS, SSL, TCP protocols An X.509 certificate is required for HTTPS or SSL connections and load balancer uses the certificate to terminate the connection and then decrypt requests from clients before sending them to the back-end instances. If you want to use SSL, but don\u2019t want to terminate the connection on the load balancer, use TCP for connections from the client to the load balancer, use the SSL protocol for connections from the load balancer to the back-end application , and deploy certificates on the back-end instances handling requests. If you use an HTTPS/SSL connection for your back end, you can enable authentication on the back-end instance. This authentication can be used to ensure that back-end instances accept only encrypted communication, and to ensure that the back-end instance has the correct certificates. ELB HTTPS listener does not support Client-Side SSL certificates 10\u3001Idle Connection Timeout For each request that a client makes through a load balancer, it maintains two connections, for each client request, one connection is with the client, and the other connection is to the back-end instance. For each connection, the load balancer manages an idle timeout that is triggered when no data is sent over the connection for a specified time period . If no data has been sent or received, it closes the connection after the idle timeout period (defaults to 60 seconds) has elapsed For lengthy operations, such as file uploads, the idle timeout setting for the connections should be adjusted to ensure that lengthy operations have time to complete. 11\u3001X-Forwarded Headers & Proxy Protocol Support As the Elastic Load Balancer intercepts the traffic between the client and the back-end servers, the back-end server does not know the IP address, Protocol, and the Port used between the Client and the Load balancer. ELB provides X-Forwarded headers support to help back end servers track the same when using the HTTP protocol X-Forwarded-For request header to help back-end servers identify the IP address of a client when you use an HTTP or HTTPS load balancer. X-Forwarded-Proto request header to help back end servers identify the protocol (HTTP/S) that a client used to connect to the server X-Forwarded-Port request header to help back-end servers identify the port that an HTTP or HTTPS load balancer uses to connect to the client. ELB provides Proxy Protocol support to help back-end servers track the same when using non-HTTP protocol or when using HTTPS and not terminating the SSL connection on the load balancer. Proxy Protocol is an Internet protocol used to carry connection information from the source requesting the connection to the destination for which the connection was requested. Elastic Load Balancing uses Proxy Protocol version 1, which uses a human-readable header format with connection information such as the source IP address, destination IP address, and port numbers If the ELB is already behind a Proxy with the Proxy protocol enabled, enabling the Proxy Protocol on ELB would add the header twice 12\u3001Cross-Zone By default, the load balancer distributes incoming requests evenly across its enabled Availability Zones for e.g. If AZ-a has 5 instances and AZ-b has 2 instances, the load will still be distributed 50% across each of the AZs Enabling Cross-Zone load balancing allows the ELB to distribute incoming requests evenly across all the back-end instances , regardless of the AZ Elastic Load Balancing creates a load balancer node in the AZ. By default, each load balancer node distributes traffic across the registered targets in its AZ only. If you enable cross-zone load balancing, each load balancer node distributes traffic across the registered targets in all enabled AZs. Cross-zone load balancer reduces the need to maintain equivalent numbers of back-end instances in each Availability Zone, and improves application\u2019s ability to handle the loss of one or more back-end instances. It is still recommended to maintain approximately equivalent numbers of instances in each Availability Zone for higher fault tolerance. With cross-zone load balancing, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones . ALB -> Cross Zone load balancing is enabled by default and free CLB -> Cross Zone load balancing is disabled, by default, and can be enabled and free NLB -> Cross Zone load balancing is disabled, by default, and can be enabled and but charged for inter-az data transfer. 13\u3001Connection Draining (Deregistration Delay) By default, if a registered EC2 instance with the ELB is deregistered or becomes unhealthy, the load balancer immediately closes the connection Connection draining can help the load balancer to complete the in-flight requests made while keeping the existing connections open, and preventing any new requests from being sent to the instances that are de-registering or unhealthy . Connection draining helps perform maintenance such as deploying software upgrades or replacing back-end instances without affecting customers\u2019 experience Connection draining allows you to specify a maximum time (between 1 and 3,600 seconds and default 300 seconds) to keep the connections alive before reporting the instance as de-registered. The maximum timeout limit does not apply to connections to unhealthy instances. If the instances are part of an Auto Scaling group and connection draining is enabled for your load balancer, Auto Scaling waits for the in-flight requests to complete, or for the maximum timeout to expire, before terminating instances due to a scaling event or health check replacement. 14\u3001sticky Sessions (Session Affinity) ELB can be configured to use Sticky Session feature (also called session affinity) which enables it to bind a user\u2019s session to an instance and ensures all requests are sent to the same instance. Stickiness remains for a period of time which can be controlled by the application\u2019s session cookie if one exists, or through a cookie, named AWSELB , created through Elastic Load balancer . Sticky sessions for CLB and ALB are disabled, by default . NLB does not support sticky sessions 14-1 Requirements An HTTP/HTTPS load balancer. SSL traffic should be terminated on the ELB. ELB does session stickiness on an HTTP/HTTPS listener by utilizing an HTTP cookie. ELB has no visibility into the HTTP headers if the SSL traffic is not terminated on the ELB and is terminated on the back-end instance.\u201c At least one healthy instance in each Availability Zone. 14-2 Duration-Based Session Stickiness Duration-Based Session Stickiness is maintained by ELB using a special cookie created to track the instance for each request to each listener. When the load balancer receives a request, it first checks to see if this cookie is present in the request. If so, the request is sent to the instance specified in the cookie. If there is no cookie, the ELB chooses an instance based on the existing load balancing algorithm and a cookie is inserted into the response for binding subsequent requests from the same user to that instance. Stickiness policy configuration defines a cookie expiration, which establishes the duration of validity for each cookie. Cookie is automatically updated after its duration expires. 14-3 Application-Controlled Session Stickiness Load balancer uses a special cookie only to associate the session with the instance that handled the initial request, but follows the lifetime of the application cookie specified in the policy configuration. Load balancer only inserts a new stickiness cookie if the application response includes a new application cookie. The load balancer stickiness cookie does not update with each request. If the application cookie is explicitly removed or expires, the session stops being sticky until a new application cookie is issued. If an instance fails or becomes unhealthy, the load balancer stops routing request to that instance, instead chooses a new healthy instance based on the existing load balancing algorithm. The load balancer treats the session as now \u201cstuck\u201d to the new healthy instance, and continues routing requests to that instance even if the failed instance comes back. 14-4 Load Balancer Deletion Deleting a load balancer does not affect the instances registered with the load balancer and they would continue to run 15 ELB with Autoscaling 15-1 Auto Scaling & ELB Auto Scaling dynamically adds and removes EC2 instances, while Elastic Load Balancing manages incoming requests by optimally routing traffic so that no one instance is overwhelmed Auto Scaling helps to automatically increase the number of EC2 instances when the user demand goes up, and decrease the number of EC2 instances when demand goes down ELB service helps to distribute the incoming web traffic (called the load) automatically among all the running EC2 instances ELB uses load balancers to monitor traffic and handle requests that come through the Internet. makes it easy to route traffic across a dynamically changing fleet of EC2 instances load balancer acts as a single point of contact for all incoming traffic to the instances in an Auto Scaling group. 15-2 Attaching/Detaching ELB with Auto Scaling Group Auto Scaling integrates with Elastic Load Balancing and enables to attach one or more load balancers to an existing Auto Scaling group. ELB registers the EC2 instance using its IP address and routes requests to the primary IP address of the primary interface (eth0) of the instance . After the ELB is attached, it automatically registers the instances in the group and distributes incoming traffic across the instances When ELB is detached, it enters the Removing state while deregistering the instances in the group . If connection draining is enabled, ELB waits for in-flight requests to complete before deregistering the instances . Instances remain running after they are deregistered from the ELB Auto Scaling adds instances to the ELB as they are launched, but this can be suspended. Instances launched during the suspension period are not added to the load balancer, after the resumption, and must be registered manually. 15-3 High Availability & Redundancy Auto Scaling can span across multiple AZs, within the same region When one AZ becomes unhealthy or unavailable, Auto Scaling launches new instances in an unaffected AZ When the unhealthy AZs recovers, Auto Scaling redistributes the traffic across all the healthy AZs Elastic Load balancer can be setup to distribute incoming requests across EC2 instances in a single AZ or multiple AZs within a region Using Auto Scaling & ELB by spanning Auto Scaling groups across multiple AZs within a region and then setting up ELB to distribute incoming traffic across those AZs helps take advantage of the safety and reliability of geographic redundancy Incoming traffic is load balanced equally across all the AZs enabled for ELB 15-4 Health Checks Auto Scaling group determines the health state of each instance by periodically checking the results of EC2 instance status checks Auto Scaling marks the instance as unhealthy and replaces the instance, if the instance fails the EC2 instance status check ELB also performs health checks on the EC2 instances that are registered with the it for e.g. application is available by pinging an health check page Auto Scaling, by default, does not replace the instance, if the ELB health check fails ELB health check with the instances should be used to ensure that traffic is routed only to the healthy instances After a load balancer is registered with an Auto Scaling group, it can be configured to use the results of the ELB health check in addition to the EC2 instance status checks to determine the health of the EC2 instances in the Auto Scaling group. 15-5 Monitoring Elastic Load Balancing sends data about the load balancers and EC2 instances to Amazon CloudWatch. CloudWatch collects data about the performance of your resources and presents it as metrics . After registering one or more load balancers with the Auto Scaling group, Auto Scaling group can be configured to use ELB metrics (such as request latency or request count) to scale the application automatically 15-6 ELB with Autoscaling Exam A company is building a two-tier web application to serve dynamic transaction-based content. The data tier is leveraging an Online Transactional Processing (OLTP) database. What services should you leverage to enable an elastic and scalable web tier? Elastic Load Balancing, Amazon EC2, and Auto Scaling Elastic Load Balancing, Amazon RDS with Multi-AZ, and Amazon S3 Amazon RDS with Multi-AZ and Auto Scaling Amazon EC2, Amazon DynamoDB, and Amazon S3 You have been given a scope to deploy some AWS infrastructure for a large organization. The requirements are that you will have a lot of EC2 instances but may need to add more when the average utilization of your Amazon EC2 fleet is high and conversely remove them when CPU utilization is low. Which AWS services would be best to use to accomplish this? Amazon CloudFront, Amazon CloudWatch and Elastic Load Balancing Auto Scaling, Amazon CloudWatch and AWS CloudTrail Auto Scaling, Amazon CloudWatch and Elastic Load Balancing Auto Scaling, Amazon CloudWatch and AWS Elastic Beanstalk A user has configured ELB with Auto Scaling. The user suspended the Auto Scaling AddToLoadBalancer, which adds instances to the load balancer. process for a while. What will happen to the instances launched during the suspension period ? The instances will not be registered with ELB and the user has to manually register when the process is resumed The instances will be registered with ELB only once the process has resumed Auto Scaling will not launch the instance during this period due to process suspension It is not possible to suspend only the AddToLoadBalancer process You have an Auto Scaling group associated with an Elastic Load Balancer (ELB). You have noticed that instances launched via the Auto Scaling group are being marked unhealthy due to an ELB health check, but these unhealthy instances are not being terminated . What do you need to do to ensure trial instances marked unhealthy by the ELB will be terminated and replaced? Change the thresholds set on the Auto Scaling group health check Add an Elastic Load Balancing health check to your Auto Scaling group Increase the value for the Health check interval set on the Elastic Load Balancer Change the health check set on the Elastic Load Balancer to use TCP rather than HTTP checks You are responsible for a web application that consists of an Elastic Load Balancing (ELB) load balancer in front of an Auto Scaling group of Amazon Elastic Compute Cloud (EC2) instances. For a recent deployment of a new version of the application, a new Amazon Machine Image (AMI) was created, and the Auto Scaling group was updated with a new launch configuration that refers to this new AMI. During the deployment, you received complaints from users that the website was responding with errors. All instances passed the ELB health checks. What should you do in order to avoid errors for future deployments? (Choose 2 answer) [ PROFESSIONAL ] Add an Elastic Load Balancing health check to the Auto Scaling group. Set a short period for the health checks to operate as soon as possible in order to prevent premature registration of the instance to the load balancer. Enable EC2 instance CloudWatch alerts to change the launch configuration\u2019s AMI to the previous one. Gradually terminate instances that are using the new AMI. Set the Elastic Load Balancing health check configuration to target a part of the application that fully tests application health and returns an error if the tests fail. Create a new launch configuration that refers to the new AMI, and associate it with the group. Double the size of the group, wait for the new instances to become healthy, and reduce back to the original size. If new instances do not become healthy, associate the previous launch configuration . Increase the Elastic Load Balancing Unhealthy Threshold to a higher value to prevent an unhealthy instance from going into service behind the load balancer . What is the order of most-to-least rapidly-scaling (fastest to scale first)? A) EC2 + ELB + Auto Scaling B) Lambda C) RDS B, A, C (Lambda is designed to scale instantly. EC2 + ELB + Auto Scaling require single-digit minutes to scale out. RDS will take at least 15 minutes, and will apply OS patches or any other updates when applied.) C, B, A C, A, B A, C, B A user has hosted an application on EC2 instances. The EC2 instances are configured with ELB and Auto Scaling. The application server session time out is 2 hours. The user wants to configure connection draining to ensure that all in-flight requests are supported by ELB even though the instance is being deregistered. What time out period should the user specify for connection draining ? 5 minutes 1 hour (max allowed is 3600 secs that is close to 2 hours to keep the in flight requests alive) 30 minutes 2 hours 16\u3001ELB Exams A user has configured an HTTPS listener on an ELB. The user has not configured any security policy which can help to negotiate SSL between the client and ELB. What will ELB do in this scenario? By default ELB will select the first version of the security policy By default ELB will select the latest version of the policy ELB creation will fail without a security policy It is not required to have a security policy since SSL is already installed A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. The ELB security policy supports various ciphers. Which of the below mentioned options helps identify the matching cipher at the client side to the ELB cipher list when client is requesting ELB DNS over SSL Cipher Protocol Client Configuration Preference Server Order Preference Load Balancer Preference A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. Which of the below mentioned security policies is supported by ELB? Dynamic Security Policy All the other options Predefined Security Policy Default Security Policy A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. Which of the below mentioned SSL protocols is not supported by the security policy ? TLS 1.3 TLS 1.2 SSL 2.0 SSL 3.0 A user has configured ELB with a TCP listener at ELB as well as on the back-end instances. The user wants to enable a proxy protocol to capture the source and destination IP information in the header. Which of the below mentioned statements helps the user understand a proxy protocol with TCP configuration ? If the end user is requesting behind a proxy server then the user should not enable a proxy protocol on ELB ELB does not support a proxy protocol when it is listening on both the load balancer and the back-end instances Whether the end user is requesting from a proxy server or directly, it does not make a difference for the proxy protocol If the end user is requesting behind the proxy then the user should add the \u201cisproxy\u201d flag to the ELB Configuration A user has enabled session stickiness with ELB. The user does not want ELB to manage the cookie; instead he wants the application to manage the cookie. What will happen when the server instance, which is bound to a cookie, crashes? The response will have a cookie but stickiness will be deleted The session will not be sticky until a new cookie is inserted ELB will throw an error due to cookie unavailability The session will be sticky and ELB will route requests to another server as ELB keeps replicating the Cookie A user has created an ELB with Auto Scaling. Which of the below mentioned offerings from ELB helps the user to stop sending new requests traffic from the load balancer to the EC2 instance when the instance is being deregistered while continuing in-flight requests? ELB sticky session ELB deregistration check ELB connection draining ELB auto registration Off When using an Elastic Load Balancer to serve traffic to web servers, which one of the following is true? Web servers must be publicly accessible The same security group must be applied to both the ELB and EC2 instances ELB and EC2 instance must be in the same subnet ELB and EC2 instances must be in the same VPC A user has configured Elastic Load Balancing by enabling a Secure Socket Layer (SSL) negotiation configuration known as a Security Policy. Which of the below mentioned options is not part of this secure policy while negotiating the SSL connection between the user and the client? SSL Protocols Client Order Preference SSL Ciphers Server Order Preference A user has created an ELB with the availability zone us-east-1. The user wants to add more zones to ELB to achieve High Availability. How can the user add more zones to the existing ELB? It is not possible to add more zones to the existing ELB Only option is to launch instances in different zones and add to ELB The user should stop the ELB and add zones and instances as required The user can add zones on the fly from the AWS console A user has launched an ELB which has 5 instances registered with it. The user deletes the ELB by mistake. What will happen to the instances? ELB will ask the user whether to delete the instances or not Instances will be terminated ELB cannot be deleted if it has running instances registered with it Instances will keep running A Sys-admin has created a shopping cart application and hosted it on EC2. The EC2 instances are running behind ELB. The admin wants to ensure that the end user request will always go to the EC2 instance where the user session has been created. How can the admin configure this? Enable ELB cross zone load balancing Enable ELB cookie setup Enable ELB sticky session Enable ELB connection draining A user has setup connection draining with ELB to allow in-flight requests to continue while the instance is being deregistered through Auto Scaling. If the user has not specified the draining time, how long will ELB allow inflight requests traffic to continue? 600 seconds 3600 seconds 300 seconds 0 seconds A customer has a web application that uses cookie Based sessions to track logged in users. It is deployed on AWS using ELB and Auto Scaling. The customer observes that when load increases Auto Scaling launches new Instances but the load on the existing Instances does not decrease, causing all existing users to have a sluggish experience. Which two answer choices independently describe a behavior that could be the cause of the sluggish user experience? ELB\u2019s normal behavior sends requests from the same user to the same backend instance (its not by default) ELB\u2019s behavior when sticky sessions are enabled causes ELB to send requests in the same session to the same backend A faulty browser is not honoring the TTL of the ELB DNS name (DNS TTL would only impact the ELB instances if scaled and not the EC2 instances to which the traffic is routed) The web application uses long polling such as comet or websockets. Thereby keeping a connection open to a web server tor a long time A customer has an online store that uses the cookie-based sessions to track logged-in customers. It is deployed on AWS using ELB and autoscaling. When the load increases, Auto scaling automatically launches new web servers, but the load on the web servers do not decrease. This causes the customers a poor experience. What could be causing the issue ? ELB DNS records Time to Live is set too high (DNS TTL would only impact the ELB instances if scaled and not the EC2 instances to which the traffic is routed) ELB is configured to send requests with previously established sessions Website uses CloudFront which is keeping sessions alive New Instances are not being added to the ELB during the Auto Scaling cool down period You are designing a multi-platform web application for AWS. The application will run on EC2 instances and will be accessed from PCs, tablets and smart phones. Supported accessing platforms are Windows, MACOS, IOS and Android. Separate sticky session and SSL certificate setups are required for different platform types. Which of the following describes the most cost effective and performance efficient architecture setup? Setup a hybrid architecture to handle session state and SSL certificates on-prem and separate EC2 Instance groups running web applications for different platform types running in a VPC. Set up one ELB for all platforms to distribute load among multiple instance under it. Each EC2 instance implements all functionality for a particular platform. Set up two ELBs. The first ELB handles SSL certificates for all platforms and the second ELB handles session stickiness for all platforms for each ELB run separate EC2 instance groups to handle the web application for each platform. Assign multiple ELBs to an EC2 instance or group of EC2 instances running the common components of the web application, one ELB for each platform type. Session stickiness and SSL termination are done at the ELBs. (Session stickiness requires HTTPS listener with SSL termination on the ELB and ELB does not support multiple SSL certs so one is required for each cert) You are migrating a legacy client-server application to AWS. The application responds to a specific DNS domain (e.g. www.example.com) and has a 2-tier architecture, with multiple application servers and a database server. Remote clients use TCP to connect to the application servers. The application servers need to know the IP address of the clients in order to function properly and are currently taking that information from the TCP socket. A Multi-AZ RDS MySQL instance will be used for the database. During the migration you can change the application code but you have to file a change request. How would you implement the architecture on AWS in order to maximize scalability and high availability? File a change request to implement Proxy Protocol support In the application. Use an ELB with a TCP Listener and Proxy Protocol enabled to distribute load on two application servers in different AZs. (ELB with TCP listener and proxy protocol will allow IP to be passed ) File a change request to Implement Cross-Zone support in the application. Use an ELB with a TCP Listener and Cross-Zone Load Balancing enabled, two application servers in different AZs. File a change request to implement Latency Based Routing support in the application. Use Route 53 with Latency Based Routing enabled to distribute load on two application servers in different AZs. File a change request to implement Alias Resource support in the application Use Route 53 Alias Resource Record to distribute load on two application servers in different AZs. A user has created an ELB with three instances. How many security groups will ELB create by default? 3 5 2 (One for ELB to allow inbound and Outbound to listener and health check port of instances and One for the Instances to allow inbound from ELB) 1 You have a web-style application with a stateless but CPU and memory-intensive web tier running on a cc2 8xlarge EC2 instance inside of a VPC The instance when under load is having problems returning requests within the SLA as defined by your business The application maintains its state in a DynamoDB table, but the data tier is properly provisioned and responses are consistently fast. How can you best resolve the issue of the application responses not meeting your SLA? Add another cc2 8xlarge application instance, and put both behind an Elastic Load Balancer Move the cc2 8xlarge to the same Availability Zone as the DynamoDB table (Does not improve the response time and performance) Cache the database responses in ElastiCache for more rapid access (Data tier is responding fast) Move the database from DynamoDB to RDS MySQL in scale-out read-replica configuration (Data tier is responding fast) An organization has configured a VPC with an Internet Gateway (IGW). pairs of public and private subnets (each with one subnet per Availability Zone), and an Elastic Load Balancer (ELB) configured to use the public subnets. The applications web tier leverages the ELB, Auto Scaling and a Multi-AZ RDS database instance. The organization would like to eliminate any potential single points of failure in this design. What step should you take to achieve this organization\u2019s objective? Nothing, there are no single points of failure in this architecture . Create and attach a second IGW to provide redundant internet connectivity. (VPC can be attached only 1 IGW) Create and configure a second Elastic Load Balancer to provide a redundant load balancer. (ELB scales by itself with multiple availability zones configured with it) Create a second multi-AZ RDS instance in another Availability Zone and configure replication to provide a redundant database. (Multi AZ requires 2 different AZ for setup and already has a standby) Your application currently leverages AWS Auto Scaling to grow and shrink as load Increases/ decreases and has been performing well. Your marketing team expects a steady ramp up in traffic to follow an upcoming campaign that will result in a 20x growth in traffic over 4 weeks. Your forecast for the approximate number of Amazon EC2 instances necessary to meet the peak demand is 175. What should you do to avoid potential service disruptions during the ramp up in traffic? Ensure that you have pre-allocated 175 Elastic IP addresses so that each server will be able to obtain one as it launches (max limit 5 EIP and a service request needs to be submitted) Check the service limits in Trusted Advisor and adjust as necessary so the forecasted count remains within limits . Change your Auto Scaling configuration to set a desired capacity of 175 prior to the launch of the marketing campaign (Will cause 175 instances to be launched and running but not gradually scale) Pre-warm your Elastic Load Balancer to match the requests per second anticipated during peak demand (Does not need pre warming as the load is increasing steadily) Which of the following features ensures even distribution of traffic to Amazon EC2 instances in multiple Availability Zones registered with a load balancer? Elastic Load Balancing request routing An Amazon Route 53 weighted routing policy (does not control traffic to EC2 instance) Elastic Load Balancing cross-zone load balancing An Amazon Route 53 latency routing policy (does not control traffic to EC2 instance) Your web application front end consists of multiple EC2 instances behind an Elastic Load Balancer. You configured ELB to perform health checks on these EC2 instances, if an instance fails to pass health checks, which statement will be true? The instance gets terminated automatically by the ELB (it is done by Autoscaling) The instance gets quarantined by the ELB for root cause analysis. The instance is replaced automatically by the ELB. (it is done by Autoscaling) The ELB stops sending traffic to the instance that failed its health check You have a web application running on six Amazon EC2 instances, consuming about 45% of resources on each instance. You are using auto-scaling to make sure that six instances are running at all times. The number of requests this application processes is consistent and does not experience spikes. The application is critical to your business and you want high availability at all times. You want the load to be distributed evenly between all instances. You also want to use the same Amazon Machine Image (AMI) for all instances. Which of the following architectural choices should you make? Deploy 6 EC2 instances in one availability zone and use Amazon Elastic Load Balancer. (Single AZ will not provide High Availability) Deploy 3 EC2 instances in one region and 3 in another region and use Amazon Elastic Load Balancer. (Different region, AMI would not be available unless copied) Deploy 3 EC2 instances in one availability zone and 3 in another availability zone and use Amazon Elastic Load Balancer. Deploy 2 EC2 instances in three regions and use Amazon Elastic Load Balancer. (Different region, AMI would not be available unless copied) You are designing an SSL/TLS solution that requires HTTPS clients to be authenticated by the Web server using client certificate authentication. The solution must be resilient. Which of the following options would you consider for configuring the web server infrastructure? (Choose 2 answers) Configure ELB with TCP listeners on TCP/443. And place the Web servers behind it. (terminate SSL on the instance using client-side certificate) Configure your Web servers with EIPs. Place the Web servers in a Route53 Record Set and configure health checks against all Web servers. (Remove ELB and use Web Servers directly with Route 53 ) Configure ELB with HTTPS listeners, and place the Web servers behind it. (ELB with HTTPs does not support Client-Side certificates) Configure your web servers as the origins for a CloudFront distribution. Use custom SSL certificates on your CloudFront distribution (CloudFront does not Client-Side ssl certificates) You are designing an application that contains protected health information. Security and compliance requirements for your application mandate that all protected health information in the application use encryption at rest and in transit. The application uses a three-tier architecture where data flows through the load balancer and is stored on Amazon EBS volumes for processing, and the results are stored in Amazon S3 using the AWS SDK. Which of the following two options satisfy the security requirements? Choose 2 answers Use SSL termination on the load balancer, Amazon EBS encryption on Amazon EC2 instances, and Amazon S3 with server-side encryption. (connection between ELB and EC2 not encrypted) Use SSL termination with a SAN SSL certificate on the load balancer, Amazon EC2 with all Amazon EBS volumes using Amazon EBS encryption, and Amazon S3 with server-side encryption with customer-managed keys. Use TCP load balancing on the load balancer, SSL termination on the Amazon EC2 instances, OS-level disk encryption on the Amazon EBS volumes, and Amazon S3 with server-side encryption. Use TCP load balancing on the load balancer, SSL termination on the Amazon EC2 instances, and Amazon S3 with server-side encryption. (Does not mention EBS encryption) Use SSL termination on the load balancer, an SSL listener on the Amazon EC2 instances, Amazon EBS encryption on EBS volumes containing PHI, and Amazon S3 with server-side encryption. A startup deploys its photo-sharing site in a VPC. An elastic load balancer distributes web traffic across two subnets. The load balancer session stickiness is configured to use the AWS-generated session cookie, with a session TTL of 5 minutes. The web server Auto Scaling group is configured as min-size=4, max-size=4. The startup is preparing for a public launch, by running load-testing software installed on a single Amazon Elastic Compute Cloud (EC2) instance running in us-west-2a . After 60 minutes of load-testing, the web server logs show the following:WEBSERVER LOGS | # of HTTP requests from load-tester | # of HTTP requests from private beta users || webserver #1 (subnet in us-west-2a): | 19,210 | 434 || webserver #2 (subnet in us-west-2a): | 21,790 | 490 || webserver #3 (subnet in us-west-2b): | 0 | 410 || webserver #4 (subnet in us-west-2b): | 0 | 428 |Which recommendations can help ensure that load-testing HTTP requests are evenly distributed across the four web servers? Choose 2 answers Launch and run the load-tester Amazon EC2 instance from us-east-1 instead. Configure Elastic Load Balancing session stickiness to use the app-specific session cookie. Re-configure the load-testing software to re-resolve DNS for each web request . Configure Elastic Load Balancing and Auto Scaling to distribute across us-west-2a and us-west-2b. Use a third-party load-testing service which offers globally distributed test clients. To serve Web traffic for a popular product your chief financial officer and IT director have purchased 10 m1.large heavy utilization Reserved Instances (RIs) evenly spread across two availability zones: Route 53 is used to deliver the traffic to an Elastic Load Balancer (ELB). After several months, the product grows even more popular and you need additional capacity As a result, your company purchases two c3.2xlarge medium utilization RIs You register the two c3.2xlarge instances with your ELB and quickly find that the ml large instances are at 100% of capacity and the c3.2xlarge instances have significant capacity that\u2019s unused Which option is the most cost effective and uses EC2 capacity most effectively? Use a separate ELB for each instance type and distribute load to ELBs with Route 53 weighted round robin Configure Autoscaling group and Launch Configuration with ELB to add up to 10 more on-demand mi large instances when triggered by CloudWatch shut off c3.2xlarge instances (increase cost as you still pay for the RI) Route traffic to EC2 m1.large and c3.2xlarge instances directly using Route 53 latency based routing and health checks shut off ELB (will not still use the capacity effectively) Configure ELB with two c3.2xlarge Instances and use on-demand Autoscailng group for up to two additional c3.2xlarge instances Shut on m1.large instances(Increases cost, as you still pay for the 10 m1.large RI) Which header received at the EC2 instance identifies the port used by the client while requesting ELB? X-Forwarded-Proto X-Requested-Proto X-Forwarded-Port X-Requested-Port A user has configured ELB with two instances running in separate AZs of the same region? Which of the below mentioned statements is true? Multi AZ instances will provide HA with ELB (ELB provides HA to route traffic to healthy instances only it does not provide scalability) Multi AZ instances are not possible with a single ELB Multi AZ instances will provide scalability with ELB The user can achieve both HA and scalability with ELB A user is configuring the HTTPS protocol on a front end ELB and the SSL protocol for the back-end listener in ELB. What will ELB do? It will allow you to create the configuration, but the instance will not pass the health check Receives requests on HTTPS and sends it to the back end instance on SSL It will not allow you to create this configuration (Will give error \u201cLoad Balancer protocol is an application layer protocol, but instance protocol is not. Both the Load Balancer protocol and the instance protocol should be at the same layer. Please fix.\u201d) It will allow you to create the configuration, but ELB will not work as expected An ELB is diverting traffic across 5 instances. One of the instances was unhealthy only for 20 minutes. What will happen after 20 minutes when the instance becomes healthy? ELB will never divert traffic back to the same instance ELB will not automatically send traffic to the same instance. However, the user can configure to start sending traffic to the same instance ELB starts sending traffic to the instance once it is healthy ELB terminates the instance once it is unhealthy. Thus, the instance cannot be healthy after 10 minutes A user has hosted a website on AWS and uses ELB to load balance the multiple instances. The user application does not have any cookie management. How can the user bind the session of the requestor with a particular instance? Bind the IP address with a sticky cookie Create a cookie at the application level to set at ELB Use session synchronization with ELB Let ELB generate a cookie for a specified duration A user has configured a website and launched it using the Apache web server on port 80. The user is using ELB with the EC2 instances for Load Balancing. What should the user do to ensure that the EC2 instances accept requests only from ELB? Open the port for an ELB static IP in the EC2 security group Configure the security group of EC2, which allows access to the ELB source security group Configure the EC2 instance so that it only listens on the ELB port Configure the security group of EC2, which allows access only to the ELB listener AWS Elastic Load Balancer supports SSL termination. For specific availability zones only False For specific regions only For all regions User has launched five instances with ELB. How can the user add the sixth EC2 instance to ELB? The user can add the sixth instance on the fly. The user must stop the ELB and add the sixth instance. The user can add the instance and change the ELB config file. The ELB can only have a maximum of five instances.","title":"L4 AWS Elastic Load Balancer \u2013 ELB"},{"location":"chap3/4ELB/#l4-aws-elastic-load-balancer-elb","text":"","title":"L4 AWS Elastic Load Balancer \u2013 ELB"},{"location":"chap3/4ELB/#1aws-elastic-load-balancer-elb","text":"Elastic Load Balancer allows the incoming traffic to be distributed automatically across multiple healthy EC2 instances . ELB serves as a single point of contact to the client ELB helps to being transparent and increases the application availability by allowing the addition or removal of multiple EC2 instances across one or more AZs , without disrupting the overall flow of information.","title":"1\u3001AWS Elastic Load Balancer \u2013 ELB"},{"location":"chap3/4ELB/#1-1-elb-benefits","text":"is a distributed system that is fault-tolerant and actively monitored abstracts out the complexity of managing, maintaining, and scaling load balancers can also serve as the first line of defense against attacks on network\\ can offload the work of encryption and decryption (SSL termination) so that the EC2 instances can focus on their main work offers integration with Auto Scaling, which ensures enough back-end capacity available to meet varying traffic levels are engineered to not be a single point of failure","title":"1-1 ELB benefits"},{"location":"chap3/4ELB/#1-2-load-balancers-only-work-across-azs-within-a-region","text":"","title":"1-2 Load Balancers only work across AZs within a region"},{"location":"chap3/4ELB/#2aws-elastic-load-balancer-types","text":"Elastic Load Balancing supports three types of load balancers: Application Load Balancer Network Load Balancer Elastic Load Balancer","title":"2\u3001AWS Elastic Load Balancer Types"},{"location":"chap3/4ELB/#3elastic-load-balancer-features","text":"Following ELB key concepts apply to all the Elastic Load Balancer types","title":"3\u3001Elastic Load Balancer Features"},{"location":"chap3/4ELB/#3-1-scaling-elb","text":"Each ELB is allocated and configured with a default capacity ELB Controller is the service that stores all the configurations and also monitors the load balancer and manages the capacity that is used to handle the client requests As the traffic profile changes, the controller service scales the load balancers to handle more requests, scaling equally in all AZs . ELB increases its capacity by utilizing either larger resources (scale up \u2013 resources with higher performance characteristics) or more individual resources (scale-out) . AWS itself handles the scaling of the ELB capacity and this scaling is different to scaling of the EC2 instances to which the ELB routes its request, which is handled by Auto Scaling Time required for Elastic Load Balancing to scale can range from 1 to 7 minutes, depending on the changes in the traffic profile When an Availability Zone is enabled for the load balancer, Elastic Load Balancing creates a load balancer node in the Availability Zone. By default, each load balancer node distributes traffic across the registered targets in its Availability Zone only.","title":"3-1 Scaling ELB"},{"location":"chap3/4ELB/#3-2-pre-warming-elb","text":"NOTE \u2013 AWS documentation does not include Pre-warming now ELB works best with a gradual increase in traffic AWS is able to scale automatically and handle a vast majority of use cases However, in certain scenarios, if there is a flash traffic spike expected or a load test cannot be configured to gradually increase traffic, recommended to contact AWS support to have the load balancer \u201cpre-warmed\u201d A WS will help Pre-warming the ELB, by configuring the load balancer to have the appropriate level of capacity based on the expected traffic AWS would need the information for the start, end dates, and the expected request rate per second with the total size of request/response.","title":"3-2 Pre-Warming ELB"},{"location":"chap3/4ELB/#3-3-dns-resolution","text":"ELB is scaled automatically depending on the traffic profile When scaled, Elastic Load Balancing service will update the Domain Name System (DNS) record of the load balancer so that the new resources have their respective IP addresses registered in DNS . DNS record created includes a Time-to-Live (TTL) setting of 60 seconds By default, ELB will return multiple IP addresses when clients perform a DNS resolution, with the records being randomly ordered on each DNS resolution request. It is recommended that clients will re-lookup the DNS at least every 60 seconds to take advantage of the increased capacity","title":"3-3 DNS Resolution"},{"location":"chap3/4ELB/#4load-balancer-types","text":"Internet Load Balancer An Internet-facing load balancer takes requests from clients over the Internet and distributes them across the EC2 instances that are registered with the load balancer Internal Load Balancer \u2013 Internal load balancer routes traffic to EC2 instances in private subnets","title":"4\u3001Load Balancer Types"},{"location":"chap3/4ELB/#5availability-zonessubnets","text":"Elastic Load Balancer should have at least one subnet attached Elastic Load Balancing allows subnets to be added and creates a load balancer node in each of the Availability Zone where the subnet resides. Only one subnet per AZ can be attached to the ELB. Attaching a subnet with an AZ already attached replaces the existing subnet Each Subnet must have a CIDR block with at least a /27 bitmask and h as at least 8 free IP addresses , which ELB uses to establish connections with the back-end instances. For High Availability, it is recommended to attach one subnet per AZ for at least two AZs, even if the instances are in a single subnet. Subnets can be attached or detached from the ELB and it would start or stop sending requests to the instances in the subnet accordingly","title":"5\u3001Availability Zones/Subnets"},{"location":"chap3/4ELB/#6security-groups-nacl","text":"Security groups & NACLs should allow Inbound traffic, on the load balancer listener port, from the Client for an Internet ELB or VPC CIDR for an Internal ELB Security groups & NACLs should allow Outbound traffic to the back-end instances on both the instance listener port and the health check port NACLs, in addition, should allow responses on the ephemeral ports All EC2 instances should allow incoming traffic from ELB","title":"6\u3001Security Groups &amp; NACL"},{"location":"chap3/4ELB/#7ssl-negotiation-configuration","text":"For HTTPS load balancer, Elastic Load Balancing uses a Secure Socket Layer (SSL) negotiation configuration, known as a security policy, to negotiate SSL connections between a client and the load balancer. A security policy is a combination of SSL protocols, SSL ciphers, and the Server Order Preference option Elastic Load Balancing supports the following versions of the SSL protocol TLS 1.2, TLS 1.1, TLS 1.0, SSL 3.0 SSL protocols use several SSL ciphers to encrypt data over the Internet . Elastic Load Balancing supports the Server Order Preference option for negotiating connections between a client and a load balancer. During the SSL connection negotiation process, this allows the load balancer to control and select the first cipher in its list that is in the client\u2019s list of ciphers instead of the default behavior of checking matching first cipher in client\u2019s list with server\u2019s list. Elastic Load Balancer allows using a Predefined Security Policies or creating a Custom Security Policy for specific needs . If none is specified, ELB selects the latest Predefined Security Policy. Elastic Load Balancer support multiple certificates using Server Name Indication (SNI) If the hostname provided by a client matches a single certificate in the certificate list, the load balancer selects this certificate . If a hostname provided by a client matches multiple certificates in the certificate list, the load balancer selects the best certificate that the client can support. Classic Load Balancer does not support multiple certificates ALB and NLB support multiple certificates","title":"7\u3001SSL Negotiation Configuration"},{"location":"chap3/4ELB/#8health-checks","text":"Load balancer performs health checks on all registered instances, whether the instance is in a healthy state or an unhealthy state. Load balancer performs health checks to discover the availability of the EC2 instances, the load balancer periodically sends pings, attempts connections, or sends request to health check the EC2 instances. Health check is InService for status of healthy instances and OutOfService for unhealthy ones Load balancer sends a request to each registered instance at the * Ping Protocol, Ping Port and Ping Path every HealthCheck Interval seconds . It waits for the instance to respond within the Response Timeout period. If the health checks exceed the Unhealthy Threshold for consecutive failed responses, the load balancer takes the instance out of service . When the health checks exceed the Healthy Threshold for consecutive successful responses, the load balancer puts the instance back in service. Load balancer only sends requests to the healthy EC2 instances and stops routing requests to the unhealthy instances All ELB types support health checks","title":"8\u3001Health Checks"},{"location":"chap3/4ELB/#9listeners","text":"Listeners is the process that checks for connection requests from client Listeners are configured with a protocol and a port for front-end (client to load balancer) connections, and a protocol and a port for back-end (load balancer to back-end instance) connections. Listeners support HTTP, HTTPS, SSL, TCP protocols An X.509 certificate is required for HTTPS or SSL connections and load balancer uses the certificate to terminate the connection and then decrypt requests from clients before sending them to the back-end instances. If you want to use SSL, but don\u2019t want to terminate the connection on the load balancer, use TCP for connections from the client to the load balancer, use the SSL protocol for connections from the load balancer to the back-end application , and deploy certificates on the back-end instances handling requests. If you use an HTTPS/SSL connection for your back end, you can enable authentication on the back-end instance. This authentication can be used to ensure that back-end instances accept only encrypted communication, and to ensure that the back-end instance has the correct certificates. ELB HTTPS listener does not support Client-Side SSL certificates","title":"9\u3001Listeners"},{"location":"chap3/4ELB/#10idle-connection-timeout","text":"For each request that a client makes through a load balancer, it maintains two connections, for each client request, one connection is with the client, and the other connection is to the back-end instance. For each connection, the load balancer manages an idle timeout that is triggered when no data is sent over the connection for a specified time period . If no data has been sent or received, it closes the connection after the idle timeout period (defaults to 60 seconds) has elapsed For lengthy operations, such as file uploads, the idle timeout setting for the connections should be adjusted to ensure that lengthy operations have time to complete.","title":"10\u3001Idle Connection Timeout"},{"location":"chap3/4ELB/#11x-forwarded-headers-proxy-protocol-support","text":"As the Elastic Load Balancer intercepts the traffic between the client and the back-end servers, the back-end server does not know the IP address, Protocol, and the Port used between the Client and the Load balancer. ELB provides X-Forwarded headers support to help back end servers track the same when using the HTTP protocol X-Forwarded-For request header to help back-end servers identify the IP address of a client when you use an HTTP or HTTPS load balancer. X-Forwarded-Proto request header to help back end servers identify the protocol (HTTP/S) that a client used to connect to the server X-Forwarded-Port request header to help back-end servers identify the port that an HTTP or HTTPS load balancer uses to connect to the client. ELB provides Proxy Protocol support to help back-end servers track the same when using non-HTTP protocol or when using HTTPS and not terminating the SSL connection on the load balancer. Proxy Protocol is an Internet protocol used to carry connection information from the source requesting the connection to the destination for which the connection was requested. Elastic Load Balancing uses Proxy Protocol version 1, which uses a human-readable header format with connection information such as the source IP address, destination IP address, and port numbers If the ELB is already behind a Proxy with the Proxy protocol enabled, enabling the Proxy Protocol on ELB would add the header twice","title":"11\u3001X-Forwarded Headers &amp; Proxy Protocol Support"},{"location":"chap3/4ELB/#12cross-zone","text":"By default, the load balancer distributes incoming requests evenly across its enabled Availability Zones for e.g. If AZ-a has 5 instances and AZ-b has 2 instances, the load will still be distributed 50% across each of the AZs Enabling Cross-Zone load balancing allows the ELB to distribute incoming requests evenly across all the back-end instances , regardless of the AZ Elastic Load Balancing creates a load balancer node in the AZ. By default, each load balancer node distributes traffic across the registered targets in its AZ only. If you enable cross-zone load balancing, each load balancer node distributes traffic across the registered targets in all enabled AZs. Cross-zone load balancer reduces the need to maintain equivalent numbers of back-end instances in each Availability Zone, and improves application\u2019s ability to handle the loss of one or more back-end instances. It is still recommended to maintain approximately equivalent numbers of instances in each Availability Zone for higher fault tolerance. With cross-zone load balancing, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones . ALB -> Cross Zone load balancing is enabled by default and free CLB -> Cross Zone load balancing is disabled, by default, and can be enabled and free NLB -> Cross Zone load balancing is disabled, by default, and can be enabled and but charged for inter-az data transfer.","title":"12\u3001Cross-Zone"},{"location":"chap3/4ELB/#13connection-draining-deregistration-delay","text":"By default, if a registered EC2 instance with the ELB is deregistered or becomes unhealthy, the load balancer immediately closes the connection Connection draining can help the load balancer to complete the in-flight requests made while keeping the existing connections open, and preventing any new requests from being sent to the instances that are de-registering or unhealthy . Connection draining helps perform maintenance such as deploying software upgrades or replacing back-end instances without affecting customers\u2019 experience Connection draining allows you to specify a maximum time (between 1 and 3,600 seconds and default 300 seconds) to keep the connections alive before reporting the instance as de-registered. The maximum timeout limit does not apply to connections to unhealthy instances. If the instances are part of an Auto Scaling group and connection draining is enabled for your load balancer, Auto Scaling waits for the in-flight requests to complete, or for the maximum timeout to expire, before terminating instances due to a scaling event or health check replacement.","title":"13\u3001Connection Draining (Deregistration Delay)"},{"location":"chap3/4ELB/#14sticky-sessions-session-affinity","text":"ELB can be configured to use Sticky Session feature (also called session affinity) which enables it to bind a user\u2019s session to an instance and ensures all requests are sent to the same instance. Stickiness remains for a period of time which can be controlled by the application\u2019s session cookie if one exists, or through a cookie, named AWSELB , created through Elastic Load balancer . Sticky sessions for CLB and ALB are disabled, by default . NLB does not support sticky sessions","title":"14\u3001sticky Sessions (Session Affinity)"},{"location":"chap3/4ELB/#14-1-requirements","text":"An HTTP/HTTPS load balancer. SSL traffic should be terminated on the ELB. ELB does session stickiness on an HTTP/HTTPS listener by utilizing an HTTP cookie. ELB has no visibility into the HTTP headers if the SSL traffic is not terminated on the ELB and is terminated on the back-end instance.\u201c At least one healthy instance in each Availability Zone.","title":"14-1 Requirements"},{"location":"chap3/4ELB/#14-2-duration-based-session-stickiness","text":"Duration-Based Session Stickiness is maintained by ELB using a special cookie created to track the instance for each request to each listener. When the load balancer receives a request, it first checks to see if this cookie is present in the request. If so, the request is sent to the instance specified in the cookie. If there is no cookie, the ELB chooses an instance based on the existing load balancing algorithm and a cookie is inserted into the response for binding subsequent requests from the same user to that instance. Stickiness policy configuration defines a cookie expiration, which establishes the duration of validity for each cookie. Cookie is automatically updated after its duration expires.","title":"14-2 Duration-Based Session Stickiness"},{"location":"chap3/4ELB/#14-3-application-controlled-session-stickiness","text":"Load balancer uses a special cookie only to associate the session with the instance that handled the initial request, but follows the lifetime of the application cookie specified in the policy configuration. Load balancer only inserts a new stickiness cookie if the application response includes a new application cookie. The load balancer stickiness cookie does not update with each request. If the application cookie is explicitly removed or expires, the session stops being sticky until a new application cookie is issued. If an instance fails or becomes unhealthy, the load balancer stops routing request to that instance, instead chooses a new healthy instance based on the existing load balancing algorithm. The load balancer treats the session as now \u201cstuck\u201d to the new healthy instance, and continues routing requests to that instance even if the failed instance comes back.","title":"14-3 Application-Controlled Session Stickiness"},{"location":"chap3/4ELB/#14-4-load-balancer-deletion","text":"Deleting a load balancer does not affect the instances registered with the load balancer and they would continue to run","title":"14-4 Load Balancer Deletion"},{"location":"chap3/4ELB/#15-elb-with-autoscaling","text":"","title":"15 ELB with Autoscaling"},{"location":"chap3/4ELB/#15-1-auto-scaling-elb","text":"Auto Scaling dynamically adds and removes EC2 instances, while Elastic Load Balancing manages incoming requests by optimally routing traffic so that no one instance is overwhelmed Auto Scaling helps to automatically increase the number of EC2 instances when the user demand goes up, and decrease the number of EC2 instances when demand goes down ELB service helps to distribute the incoming web traffic (called the load) automatically among all the running EC2 instances ELB uses load balancers to monitor traffic and handle requests that come through the Internet. makes it easy to route traffic across a dynamically changing fleet of EC2 instances load balancer acts as a single point of contact for all incoming traffic to the instances in an Auto Scaling group.","title":"15-1 Auto Scaling &amp; ELB"},{"location":"chap3/4ELB/#15-2-attachingdetaching-elb-with-auto-scaling-group","text":"Auto Scaling integrates with Elastic Load Balancing and enables to attach one or more load balancers to an existing Auto Scaling group. ELB registers the EC2 instance using its IP address and routes requests to the primary IP address of the primary interface (eth0) of the instance . After the ELB is attached, it automatically registers the instances in the group and distributes incoming traffic across the instances When ELB is detached, it enters the Removing state while deregistering the instances in the group . If connection draining is enabled, ELB waits for in-flight requests to complete before deregistering the instances . Instances remain running after they are deregistered from the ELB Auto Scaling adds instances to the ELB as they are launched, but this can be suspended. Instances launched during the suspension period are not added to the load balancer, after the resumption, and must be registered manually.","title":"15-2 Attaching/Detaching ELB with Auto Scaling Group"},{"location":"chap3/4ELB/#15-3-high-availability-redundancy","text":"Auto Scaling can span across multiple AZs, within the same region When one AZ becomes unhealthy or unavailable, Auto Scaling launches new instances in an unaffected AZ When the unhealthy AZs recovers, Auto Scaling redistributes the traffic across all the healthy AZs Elastic Load balancer can be setup to distribute incoming requests across EC2 instances in a single AZ or multiple AZs within a region Using Auto Scaling & ELB by spanning Auto Scaling groups across multiple AZs within a region and then setting up ELB to distribute incoming traffic across those AZs helps take advantage of the safety and reliability of geographic redundancy Incoming traffic is load balanced equally across all the AZs enabled for ELB","title":"15-3 High Availability &amp; Redundancy"},{"location":"chap3/4ELB/#15-4-health-checks","text":"Auto Scaling group determines the health state of each instance by periodically checking the results of EC2 instance status checks Auto Scaling marks the instance as unhealthy and replaces the instance, if the instance fails the EC2 instance status check ELB also performs health checks on the EC2 instances that are registered with the it for e.g. application is available by pinging an health check page Auto Scaling, by default, does not replace the instance, if the ELB health check fails ELB health check with the instances should be used to ensure that traffic is routed only to the healthy instances After a load balancer is registered with an Auto Scaling group, it can be configured to use the results of the ELB health check in addition to the EC2 instance status checks to determine the health of the EC2 instances in the Auto Scaling group.","title":"15-4 Health Checks"},{"location":"chap3/4ELB/#15-5-monitoring","text":"Elastic Load Balancing sends data about the load balancers and EC2 instances to Amazon CloudWatch. CloudWatch collects data about the performance of your resources and presents it as metrics . After registering one or more load balancers with the Auto Scaling group, Auto Scaling group can be configured to use ELB metrics (such as request latency or request count) to scale the application automatically","title":"15-5 Monitoring"},{"location":"chap3/4ELB/#15-6-elb-with-autoscaling-exam","text":"A company is building a two-tier web application to serve dynamic transaction-based content. The data tier is leveraging an Online Transactional Processing (OLTP) database. What services should you leverage to enable an elastic and scalable web tier? Elastic Load Balancing, Amazon EC2, and Auto Scaling Elastic Load Balancing, Amazon RDS with Multi-AZ, and Amazon S3 Amazon RDS with Multi-AZ and Auto Scaling Amazon EC2, Amazon DynamoDB, and Amazon S3 You have been given a scope to deploy some AWS infrastructure for a large organization. The requirements are that you will have a lot of EC2 instances but may need to add more when the average utilization of your Amazon EC2 fleet is high and conversely remove them when CPU utilization is low. Which AWS services would be best to use to accomplish this? Amazon CloudFront, Amazon CloudWatch and Elastic Load Balancing Auto Scaling, Amazon CloudWatch and AWS CloudTrail Auto Scaling, Amazon CloudWatch and Elastic Load Balancing Auto Scaling, Amazon CloudWatch and AWS Elastic Beanstalk A user has configured ELB with Auto Scaling. The user suspended the Auto Scaling AddToLoadBalancer, which adds instances to the load balancer. process for a while. What will happen to the instances launched during the suspension period ? The instances will not be registered with ELB and the user has to manually register when the process is resumed The instances will be registered with ELB only once the process has resumed Auto Scaling will not launch the instance during this period due to process suspension It is not possible to suspend only the AddToLoadBalancer process You have an Auto Scaling group associated with an Elastic Load Balancer (ELB). You have noticed that instances launched via the Auto Scaling group are being marked unhealthy due to an ELB health check, but these unhealthy instances are not being terminated . What do you need to do to ensure trial instances marked unhealthy by the ELB will be terminated and replaced? Change the thresholds set on the Auto Scaling group health check Add an Elastic Load Balancing health check to your Auto Scaling group Increase the value for the Health check interval set on the Elastic Load Balancer Change the health check set on the Elastic Load Balancer to use TCP rather than HTTP checks You are responsible for a web application that consists of an Elastic Load Balancing (ELB) load balancer in front of an Auto Scaling group of Amazon Elastic Compute Cloud (EC2) instances. For a recent deployment of a new version of the application, a new Amazon Machine Image (AMI) was created, and the Auto Scaling group was updated with a new launch configuration that refers to this new AMI. During the deployment, you received complaints from users that the website was responding with errors. All instances passed the ELB health checks. What should you do in order to avoid errors for future deployments? (Choose 2 answer) [ PROFESSIONAL ] Add an Elastic Load Balancing health check to the Auto Scaling group. Set a short period for the health checks to operate as soon as possible in order to prevent premature registration of the instance to the load balancer. Enable EC2 instance CloudWatch alerts to change the launch configuration\u2019s AMI to the previous one. Gradually terminate instances that are using the new AMI. Set the Elastic Load Balancing health check configuration to target a part of the application that fully tests application health and returns an error if the tests fail. Create a new launch configuration that refers to the new AMI, and associate it with the group. Double the size of the group, wait for the new instances to become healthy, and reduce back to the original size. If new instances do not become healthy, associate the previous launch configuration . Increase the Elastic Load Balancing Unhealthy Threshold to a higher value to prevent an unhealthy instance from going into service behind the load balancer . What is the order of most-to-least rapidly-scaling (fastest to scale first)? A) EC2 + ELB + Auto Scaling B) Lambda C) RDS B, A, C (Lambda is designed to scale instantly. EC2 + ELB + Auto Scaling require single-digit minutes to scale out. RDS will take at least 15 minutes, and will apply OS patches or any other updates when applied.) C, B, A C, A, B A, C, B A user has hosted an application on EC2 instances. The EC2 instances are configured with ELB and Auto Scaling. The application server session time out is 2 hours. The user wants to configure connection draining to ensure that all in-flight requests are supported by ELB even though the instance is being deregistered. What time out period should the user specify for connection draining ? 5 minutes 1 hour (max allowed is 3600 secs that is close to 2 hours to keep the in flight requests alive) 30 minutes 2 hours","title":"15-6 ELB with Autoscaling Exam"},{"location":"chap3/4ELB/#16elb-exams","text":"A user has configured an HTTPS listener on an ELB. The user has not configured any security policy which can help to negotiate SSL between the client and ELB. What will ELB do in this scenario? By default ELB will select the first version of the security policy By default ELB will select the latest version of the policy ELB creation will fail without a security policy It is not required to have a security policy since SSL is already installed A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. The ELB security policy supports various ciphers. Which of the below mentioned options helps identify the matching cipher at the client side to the ELB cipher list when client is requesting ELB DNS over SSL Cipher Protocol Client Configuration Preference Server Order Preference Load Balancer Preference A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. Which of the below mentioned security policies is supported by ELB? Dynamic Security Policy All the other options Predefined Security Policy Default Security Policy A user has configured ELB with SSL using a security policy for secure negotiation between the client and load balancer. Which of the below mentioned SSL protocols is not supported by the security policy ? TLS 1.3 TLS 1.2 SSL 2.0 SSL 3.0 A user has configured ELB with a TCP listener at ELB as well as on the back-end instances. The user wants to enable a proxy protocol to capture the source and destination IP information in the header. Which of the below mentioned statements helps the user understand a proxy protocol with TCP configuration ? If the end user is requesting behind a proxy server then the user should not enable a proxy protocol on ELB ELB does not support a proxy protocol when it is listening on both the load balancer and the back-end instances Whether the end user is requesting from a proxy server or directly, it does not make a difference for the proxy protocol If the end user is requesting behind the proxy then the user should add the \u201cisproxy\u201d flag to the ELB Configuration A user has enabled session stickiness with ELB. The user does not want ELB to manage the cookie; instead he wants the application to manage the cookie. What will happen when the server instance, which is bound to a cookie, crashes? The response will have a cookie but stickiness will be deleted The session will not be sticky until a new cookie is inserted ELB will throw an error due to cookie unavailability The session will be sticky and ELB will route requests to another server as ELB keeps replicating the Cookie A user has created an ELB with Auto Scaling. Which of the below mentioned offerings from ELB helps the user to stop sending new requests traffic from the load balancer to the EC2 instance when the instance is being deregistered while continuing in-flight requests? ELB sticky session ELB deregistration check ELB connection draining ELB auto registration Off When using an Elastic Load Balancer to serve traffic to web servers, which one of the following is true? Web servers must be publicly accessible The same security group must be applied to both the ELB and EC2 instances ELB and EC2 instance must be in the same subnet ELB and EC2 instances must be in the same VPC A user has configured Elastic Load Balancing by enabling a Secure Socket Layer (SSL) negotiation configuration known as a Security Policy. Which of the below mentioned options is not part of this secure policy while negotiating the SSL connection between the user and the client? SSL Protocols Client Order Preference SSL Ciphers Server Order Preference A user has created an ELB with the availability zone us-east-1. The user wants to add more zones to ELB to achieve High Availability. How can the user add more zones to the existing ELB? It is not possible to add more zones to the existing ELB Only option is to launch instances in different zones and add to ELB The user should stop the ELB and add zones and instances as required The user can add zones on the fly from the AWS console A user has launched an ELB which has 5 instances registered with it. The user deletes the ELB by mistake. What will happen to the instances? ELB will ask the user whether to delete the instances or not Instances will be terminated ELB cannot be deleted if it has running instances registered with it Instances will keep running A Sys-admin has created a shopping cart application and hosted it on EC2. The EC2 instances are running behind ELB. The admin wants to ensure that the end user request will always go to the EC2 instance where the user session has been created. How can the admin configure this? Enable ELB cross zone load balancing Enable ELB cookie setup Enable ELB sticky session Enable ELB connection draining A user has setup connection draining with ELB to allow in-flight requests to continue while the instance is being deregistered through Auto Scaling. If the user has not specified the draining time, how long will ELB allow inflight requests traffic to continue? 600 seconds 3600 seconds 300 seconds 0 seconds A customer has a web application that uses cookie Based sessions to track logged in users. It is deployed on AWS using ELB and Auto Scaling. The customer observes that when load increases Auto Scaling launches new Instances but the load on the existing Instances does not decrease, causing all existing users to have a sluggish experience. Which two answer choices independently describe a behavior that could be the cause of the sluggish user experience? ELB\u2019s normal behavior sends requests from the same user to the same backend instance (its not by default) ELB\u2019s behavior when sticky sessions are enabled causes ELB to send requests in the same session to the same backend A faulty browser is not honoring the TTL of the ELB DNS name (DNS TTL would only impact the ELB instances if scaled and not the EC2 instances to which the traffic is routed) The web application uses long polling such as comet or websockets. Thereby keeping a connection open to a web server tor a long time A customer has an online store that uses the cookie-based sessions to track logged-in customers. It is deployed on AWS using ELB and autoscaling. When the load increases, Auto scaling automatically launches new web servers, but the load on the web servers do not decrease. This causes the customers a poor experience. What could be causing the issue ? ELB DNS records Time to Live is set too high (DNS TTL would only impact the ELB instances if scaled and not the EC2 instances to which the traffic is routed) ELB is configured to send requests with previously established sessions Website uses CloudFront which is keeping sessions alive New Instances are not being added to the ELB during the Auto Scaling cool down period You are designing a multi-platform web application for AWS. The application will run on EC2 instances and will be accessed from PCs, tablets and smart phones. Supported accessing platforms are Windows, MACOS, IOS and Android. Separate sticky session and SSL certificate setups are required for different platform types. Which of the following describes the most cost effective and performance efficient architecture setup? Setup a hybrid architecture to handle session state and SSL certificates on-prem and separate EC2 Instance groups running web applications for different platform types running in a VPC. Set up one ELB for all platforms to distribute load among multiple instance under it. Each EC2 instance implements all functionality for a particular platform. Set up two ELBs. The first ELB handles SSL certificates for all platforms and the second ELB handles session stickiness for all platforms for each ELB run separate EC2 instance groups to handle the web application for each platform. Assign multiple ELBs to an EC2 instance or group of EC2 instances running the common components of the web application, one ELB for each platform type. Session stickiness and SSL termination are done at the ELBs. (Session stickiness requires HTTPS listener with SSL termination on the ELB and ELB does not support multiple SSL certs so one is required for each cert) You are migrating a legacy client-server application to AWS. The application responds to a specific DNS domain (e.g. www.example.com) and has a 2-tier architecture, with multiple application servers and a database server. Remote clients use TCP to connect to the application servers. The application servers need to know the IP address of the clients in order to function properly and are currently taking that information from the TCP socket. A Multi-AZ RDS MySQL instance will be used for the database. During the migration you can change the application code but you have to file a change request. How would you implement the architecture on AWS in order to maximize scalability and high availability? File a change request to implement Proxy Protocol support In the application. Use an ELB with a TCP Listener and Proxy Protocol enabled to distribute load on two application servers in different AZs. (ELB with TCP listener and proxy protocol will allow IP to be passed ) File a change request to Implement Cross-Zone support in the application. Use an ELB with a TCP Listener and Cross-Zone Load Balancing enabled, two application servers in different AZs. File a change request to implement Latency Based Routing support in the application. Use Route 53 with Latency Based Routing enabled to distribute load on two application servers in different AZs. File a change request to implement Alias Resource support in the application Use Route 53 Alias Resource Record to distribute load on two application servers in different AZs. A user has created an ELB with three instances. How many security groups will ELB create by default? 3 5 2 (One for ELB to allow inbound and Outbound to listener and health check port of instances and One for the Instances to allow inbound from ELB) 1 You have a web-style application with a stateless but CPU and memory-intensive web tier running on a cc2 8xlarge EC2 instance inside of a VPC The instance when under load is having problems returning requests within the SLA as defined by your business The application maintains its state in a DynamoDB table, but the data tier is properly provisioned and responses are consistently fast. How can you best resolve the issue of the application responses not meeting your SLA? Add another cc2 8xlarge application instance, and put both behind an Elastic Load Balancer Move the cc2 8xlarge to the same Availability Zone as the DynamoDB table (Does not improve the response time and performance) Cache the database responses in ElastiCache for more rapid access (Data tier is responding fast) Move the database from DynamoDB to RDS MySQL in scale-out read-replica configuration (Data tier is responding fast) An organization has configured a VPC with an Internet Gateway (IGW). pairs of public and private subnets (each with one subnet per Availability Zone), and an Elastic Load Balancer (ELB) configured to use the public subnets. The applications web tier leverages the ELB, Auto Scaling and a Multi-AZ RDS database instance. The organization would like to eliminate any potential single points of failure in this design. What step should you take to achieve this organization\u2019s objective? Nothing, there are no single points of failure in this architecture . Create and attach a second IGW to provide redundant internet connectivity. (VPC can be attached only 1 IGW) Create and configure a second Elastic Load Balancer to provide a redundant load balancer. (ELB scales by itself with multiple availability zones configured with it) Create a second multi-AZ RDS instance in another Availability Zone and configure replication to provide a redundant database. (Multi AZ requires 2 different AZ for setup and already has a standby) Your application currently leverages AWS Auto Scaling to grow and shrink as load Increases/ decreases and has been performing well. Your marketing team expects a steady ramp up in traffic to follow an upcoming campaign that will result in a 20x growth in traffic over 4 weeks. Your forecast for the approximate number of Amazon EC2 instances necessary to meet the peak demand is 175. What should you do to avoid potential service disruptions during the ramp up in traffic? Ensure that you have pre-allocated 175 Elastic IP addresses so that each server will be able to obtain one as it launches (max limit 5 EIP and a service request needs to be submitted) Check the service limits in Trusted Advisor and adjust as necessary so the forecasted count remains within limits . Change your Auto Scaling configuration to set a desired capacity of 175 prior to the launch of the marketing campaign (Will cause 175 instances to be launched and running but not gradually scale) Pre-warm your Elastic Load Balancer to match the requests per second anticipated during peak demand (Does not need pre warming as the load is increasing steadily) Which of the following features ensures even distribution of traffic to Amazon EC2 instances in multiple Availability Zones registered with a load balancer? Elastic Load Balancing request routing An Amazon Route 53 weighted routing policy (does not control traffic to EC2 instance) Elastic Load Balancing cross-zone load balancing An Amazon Route 53 latency routing policy (does not control traffic to EC2 instance) Your web application front end consists of multiple EC2 instances behind an Elastic Load Balancer. You configured ELB to perform health checks on these EC2 instances, if an instance fails to pass health checks, which statement will be true? The instance gets terminated automatically by the ELB (it is done by Autoscaling) The instance gets quarantined by the ELB for root cause analysis. The instance is replaced automatically by the ELB. (it is done by Autoscaling) The ELB stops sending traffic to the instance that failed its health check You have a web application running on six Amazon EC2 instances, consuming about 45% of resources on each instance. You are using auto-scaling to make sure that six instances are running at all times. The number of requests this application processes is consistent and does not experience spikes. The application is critical to your business and you want high availability at all times. You want the load to be distributed evenly between all instances. You also want to use the same Amazon Machine Image (AMI) for all instances. Which of the following architectural choices should you make? Deploy 6 EC2 instances in one availability zone and use Amazon Elastic Load Balancer. (Single AZ will not provide High Availability) Deploy 3 EC2 instances in one region and 3 in another region and use Amazon Elastic Load Balancer. (Different region, AMI would not be available unless copied) Deploy 3 EC2 instances in one availability zone and 3 in another availability zone and use Amazon Elastic Load Balancer. Deploy 2 EC2 instances in three regions and use Amazon Elastic Load Balancer. (Different region, AMI would not be available unless copied) You are designing an SSL/TLS solution that requires HTTPS clients to be authenticated by the Web server using client certificate authentication. The solution must be resilient. Which of the following options would you consider for configuring the web server infrastructure? (Choose 2 answers) Configure ELB with TCP listeners on TCP/443. And place the Web servers behind it. (terminate SSL on the instance using client-side certificate) Configure your Web servers with EIPs. Place the Web servers in a Route53 Record Set and configure health checks against all Web servers. (Remove ELB and use Web Servers directly with Route 53 ) Configure ELB with HTTPS listeners, and place the Web servers behind it. (ELB with HTTPs does not support Client-Side certificates) Configure your web servers as the origins for a CloudFront distribution. Use custom SSL certificates on your CloudFront distribution (CloudFront does not Client-Side ssl certificates) You are designing an application that contains protected health information. Security and compliance requirements for your application mandate that all protected health information in the application use encryption at rest and in transit. The application uses a three-tier architecture where data flows through the load balancer and is stored on Amazon EBS volumes for processing, and the results are stored in Amazon S3 using the AWS SDK. Which of the following two options satisfy the security requirements? Choose 2 answers Use SSL termination on the load balancer, Amazon EBS encryption on Amazon EC2 instances, and Amazon S3 with server-side encryption. (connection between ELB and EC2 not encrypted) Use SSL termination with a SAN SSL certificate on the load balancer, Amazon EC2 with all Amazon EBS volumes using Amazon EBS encryption, and Amazon S3 with server-side encryption with customer-managed keys. Use TCP load balancing on the load balancer, SSL termination on the Amazon EC2 instances, OS-level disk encryption on the Amazon EBS volumes, and Amazon S3 with server-side encryption. Use TCP load balancing on the load balancer, SSL termination on the Amazon EC2 instances, and Amazon S3 with server-side encryption. (Does not mention EBS encryption) Use SSL termination on the load balancer, an SSL listener on the Amazon EC2 instances, Amazon EBS encryption on EBS volumes containing PHI, and Amazon S3 with server-side encryption. A startup deploys its photo-sharing site in a VPC. An elastic load balancer distributes web traffic across two subnets. The load balancer session stickiness is configured to use the AWS-generated session cookie, with a session TTL of 5 minutes. The web server Auto Scaling group is configured as min-size=4, max-size=4. The startup is preparing for a public launch, by running load-testing software installed on a single Amazon Elastic Compute Cloud (EC2) instance running in us-west-2a . After 60 minutes of load-testing, the web server logs show the following:WEBSERVER LOGS | # of HTTP requests from load-tester | # of HTTP requests from private beta users || webserver #1 (subnet in us-west-2a): | 19,210 | 434 || webserver #2 (subnet in us-west-2a): | 21,790 | 490 || webserver #3 (subnet in us-west-2b): | 0 | 410 || webserver #4 (subnet in us-west-2b): | 0 | 428 |Which recommendations can help ensure that load-testing HTTP requests are evenly distributed across the four web servers? Choose 2 answers Launch and run the load-tester Amazon EC2 instance from us-east-1 instead. Configure Elastic Load Balancing session stickiness to use the app-specific session cookie. Re-configure the load-testing software to re-resolve DNS for each web request . Configure Elastic Load Balancing and Auto Scaling to distribute across us-west-2a and us-west-2b. Use a third-party load-testing service which offers globally distributed test clients. To serve Web traffic for a popular product your chief financial officer and IT director have purchased 10 m1.large heavy utilization Reserved Instances (RIs) evenly spread across two availability zones: Route 53 is used to deliver the traffic to an Elastic Load Balancer (ELB). After several months, the product grows even more popular and you need additional capacity As a result, your company purchases two c3.2xlarge medium utilization RIs You register the two c3.2xlarge instances with your ELB and quickly find that the ml large instances are at 100% of capacity and the c3.2xlarge instances have significant capacity that\u2019s unused Which option is the most cost effective and uses EC2 capacity most effectively? Use a separate ELB for each instance type and distribute load to ELBs with Route 53 weighted round robin Configure Autoscaling group and Launch Configuration with ELB to add up to 10 more on-demand mi large instances when triggered by CloudWatch shut off c3.2xlarge instances (increase cost as you still pay for the RI) Route traffic to EC2 m1.large and c3.2xlarge instances directly using Route 53 latency based routing and health checks shut off ELB (will not still use the capacity effectively) Configure ELB with two c3.2xlarge Instances and use on-demand Autoscailng group for up to two additional c3.2xlarge instances Shut on m1.large instances(Increases cost, as you still pay for the 10 m1.large RI) Which header received at the EC2 instance identifies the port used by the client while requesting ELB? X-Forwarded-Proto X-Requested-Proto X-Forwarded-Port X-Requested-Port A user has configured ELB with two instances running in separate AZs of the same region? Which of the below mentioned statements is true? Multi AZ instances will provide HA with ELB (ELB provides HA to route traffic to healthy instances only it does not provide scalability) Multi AZ instances are not possible with a single ELB Multi AZ instances will provide scalability with ELB The user can achieve both HA and scalability with ELB A user is configuring the HTTPS protocol on a front end ELB and the SSL protocol for the back-end listener in ELB. What will ELB do? It will allow you to create the configuration, but the instance will not pass the health check Receives requests on HTTPS and sends it to the back end instance on SSL It will not allow you to create this configuration (Will give error \u201cLoad Balancer protocol is an application layer protocol, but instance protocol is not. Both the Load Balancer protocol and the instance protocol should be at the same layer. Please fix.\u201d) It will allow you to create the configuration, but ELB will not work as expected An ELB is diverting traffic across 5 instances. One of the instances was unhealthy only for 20 minutes. What will happen after 20 minutes when the instance becomes healthy? ELB will never divert traffic back to the same instance ELB will not automatically send traffic to the same instance. However, the user can configure to start sending traffic to the same instance ELB starts sending traffic to the instance once it is healthy ELB terminates the instance once it is unhealthy. Thus, the instance cannot be healthy after 10 minutes A user has hosted a website on AWS and uses ELB to load balance the multiple instances. The user application does not have any cookie management. How can the user bind the session of the requestor with a particular instance? Bind the IP address with a sticky cookie Create a cookie at the application level to set at ELB Use session synchronization with ELB Let ELB generate a cookie for a specified duration A user has configured a website and launched it using the Apache web server on port 80. The user is using ELB with the EC2 instances for Load Balancing. What should the user do to ensure that the EC2 instances accept requests only from ELB? Open the port for an ELB static IP in the EC2 security group Configure the security group of EC2, which allows access to the ELB source security group Configure the EC2 instance so that it only listens on the ELB port Configure the security group of EC2, which allows access only to the ELB listener AWS Elastic Load Balancer supports SSL termination. For specific availability zones only False For specific regions only For all regions User has launched five instances with ELB. How can the user add the sixth EC2 instance to ELB? The user can add the sixth instance on the fly. The user must stop the ELB and add the sixth instance. The user can add the instance and change the ELB config file. The ELB can only have a maximum of five instances.","title":"16\u3001ELB Exams"},{"location":"chap3/5ALB/","text":"L5 AWS Application Load Balancer \u2013 ALB 1 AWS Application Load Balancer - ALB Application Load Balancer operates at layer 7 (application layer) and allows defining routing rules based on content across multiple services or containers running on one or more EC2 instances scales the load balancer as traffic to the application changes over time. can scale to the vast majority of workloads automatically. supports health checks, used to monitor the health of registered targets so that the load balancer can send requests only to the healthy targets. 2\u3001Application Load Balancer Components 2-1 A load balancer serves as the single point of contact for clients. distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple AZs, which increases the availability of the application. one or more listeners can be added to the load balancer. 2-2 A listener checks for connection requests from clients, using the configured protocol and port rules defined determine, how the load balancer routes requests to its registered targets. each rule consists of a priority, one or more actions, and one or more conditions. when the conditions for a rule are met, its actions are performed. a default rule for each listener must be defined, and optionally additional rules can be defined 2-3 Target group routes requests to one or more registered targets, such as EC2 instances, using the specified protocol and port number a target can be registered with multiple target groups . health checks can be configured on a per target group basis. Health checks are performed on all targets registered to a target group that is specified in a listener rule for the load balancer. target group supports EC2 instances (can be managed as a part of Autoscaling group) ECS tasks Lambda functions IP Addresses \u2013 must be private IP When a load balancer receives a request, it evaluates the listener rules in priority order to determine which rule to apply and then selects a target from the target group for the rule action. Listener rules can be configured to r oute requests to different target groups based on the content of the application traffic. Routing is performed independently for each target group, even when a target is registered with multiple target groups. Routing algorithm used can be configured at the target group level . Default routing algorithm is round-robin ; alternatively, the least outstanding requests routing algorithm can also be specified 3\u3001Application Load Balancer Benefits Support for Path-based routing, where listener rules can be configured to forward requests based on the URL in the request . This enables structuring application as smaller services (microservices), and route requests to the correct service based on the content of the URL. Support for routing requests to multiple services on a single EC2 instance by registering the instance using multiple ports . Support for containerized applications . EC2 Container Service (ECS) can select an unused port when scheduling a task and register the task with a target group using this port, enabling efficient use of the clusters. Support for monitoring the health of each service independently, as health checks are defined at the target group level and many CloudWatch metrics are reported at the target group level. Attaching a target group to an Auto Scaling group enables scaling each service dynamically based on demand . 4\u3001Application Load Balancer Features supports load balancing of applications using HTTP and HTTPS (Secure HTTP) protocols supports HTTP/2 , which is enabled natively. Clients that support HTTP/2 can connect over TLS supports WebSockets and Secure WebSockets natively supports Request tracing , by default. request tracing can be used to track HTTP requests from clients to targets or other services. Load balancer upon receiving a request from a client, adds or updates the X-Amzn-Trace-Id header before sending the request to the target Any services or applications between the load balancer and the target can also add or update this header. supports Sticky Sessions (Session Affinity) using load balancer generated cookies, to route requests from the same client to the same target supports SSL termination , to decrypt the request on ALB before sending it to the underlying targets. an SSL certificate can be installed on the load balancer. the load balancer uses this certificate to terminate the connection and then decrypt requests from clients before sending them to targets. supports layer 7 specific features like X-Forwarded-For headers to help determine the actual client IP, port and protocol automatically scales its request handling capacity in response to incoming application traffic. supports hybrid load balancing , If an application runs on targets distributed between a VPC and an on-premises location, they can be added to the same target group using their IP addresses provides High Availability , by allowing you to specify more than one AZ and distribution of incoming traffic across multiple AZs. integrates with ACM to provision and bind an SSL/TLS certificate to the load balancer thereby making the entire SSL offload process very easy supports multiple certificates for the same domain to a secure listener supports IPv6 addressing, for an Internet-facing load balancer supports Cross-zone load balancing, by default supports Security Groups to control the traffic allowed to and from the load balancer. provides Access Logs , to record all requests sent to the load balancer, and store the logs in S3 for later analysis in compressed format provides Delete Protection , to prevent the ALB from accidental deletion supports Connection Idle Timeout \u2013 ALB maintains two connections for each request one with the Client (front end) and one with the target instance (back end). If no data has been sent or received by the time that the idle timeout period elapses, ALB closes the front-end connection integrates with CloudWatch to provide metrics, such as request counts, error counts, error types, and request latency integrates with AWS WAF , a web application firewall that helps protect web applications from attacks by allowing rules configuration based on IP addresses, HTTP headers, and custom URI strings integrates with CloudTrail to receive a history of ALB API calls made on the AWS account\u2018\u2019 5\u3001Application Load Balancer Listeners A listener is a process that checks for connection requests, using the configured protocol and port Listener supports HTTP & HTTPS protocol with Ports from 1-65535 ALB supports SSL Termination for HTTPS listener, which helps to offload the work of encryption and decryption so that the targets can focus on their main work. HTTPS listener must have at least one SSL server certificate on the listener WebSockets with both HTTP and HTTPS listeners (Secure WebSockets) Supports HTTP/2 with HTTPS listeners 128 requests can be sent in parallel using one HTTP/2 connection. ALB converts these to individual HTTP/1.1 requests and distributes them across the healthy targets in the target group using the round robin routing algorithm. HTTP/2 uses front-end connections more efficiently resulting in fewer connections between clients and the load balancer. Server-push feature of HTTP/2 is not supported Each listener has a default rule, and can optionally define additional rules. Each rule consists of a priority, action, optional host condition, and optional path condition . Priority \u2013 Rules are evaluated in priority order, from the lowest value to the highest value. The default rule has the lowest priority Action \u2013 Each rule action has a type and a target group. Currently, the only supported type is forward, which forwards requests to the target group. You can change the target group for a rule at any time. Condition \u2013 There are two types of rule conditions: host and path. When the conditions for a rule are met, then its action is taken 5-1 Host Condition or Host-based routing Host conditions can be used to define rules that forward requests to different target groups based on the hostname in the host header This enables support for multiple domains using a single ALB for e.g. orders.example.com , images.example.com , registration.example.com Each host condition has one hostname. If the hostname in 5-2 Path Condition or path-based routing Path conditions can be used to define rules that forward requests to different target groups based on the URL in the request Each path condition has one path pattern for e.g. example.com/orders , example.com/images , example.com/registration If the URL in a request matches the path pattern in a listener rule exactly, the request is routed using that rule. 6\u3001Advantages over Classic Load Balancer Support for path-based routing , where rules can be configured for the listener to forward requests based on the content of the URL Support for host-based routing , where rules can be configured for the listener to forward requests based on the host field in the HTTP header. Support for routing based on fields in the request, such as standard and custom HTTP headers and methods, query parameters, and source IP address Support for routing requests to multiple applications on a single EC2 instance. Each instance or IP address can be registered with the same target group using multiple ports Support for registering targets by IP address , including targets outside the VPC for the load balancer. Support for redirecting requests from one URL to another . Support for returning a custom HTTP response. Support for registering Lambda functions as targets. Support for the load balancer to authenticate users of the applications through their corporate or social identities before routing requests. Support containerized applications with ECS using Dynamic port mapping Support monitoring the health of each service independently , as health checks and many CloudWatch metrics are defined at the target group level Attaching the target group to an Auto Scaling group enables scaling of each service dynamically based on demand Access logs contain additional information & stored in compressed format Improved load balancer performance . 7\u3001Application Load Balancer Pricing charged for each hour or partial hour that an ALB is running and the number of Load Balancer Capacity Units (LCU) used per hour. An LCU is a new metric for determining ALB pricing An LCU defines the maximum resource consumed in any one of the dimensions (new connections, active connections, bandwidth and rule evaluations) the Application Load Balancer processes the traffic. 8\u3001Application Load Balancer Exam You are designing an application which requires websockets support, to exchange real-time messages with end-users without the end users having to request (or poll) the server for an update? Which ELB option should you choose? Use Application Load Balancer and enable comet support Use Classic Load Balancer which supports WebSockets Use Application Load Balancer which supports WebSockets Use Classic Load Balancer and enable comet support Which of the following Internet protocols does an AWS Application Load Balancer Support? Choose 2 answers A. ICMP B. UDP C. HTTP D. SNTP E. Websocket Your organization has configured an application behind ALB. However, Clients are complaining that they cannot connect to an Internet-facing load balancer. What cannot be the issue? Internet-facing load balancer is attached to a private subnet ALB Security Groups does not allow the traffic Subnet NACLs do not allow the traffic ALB was not assigned an EIP To protect your ALB from accidental deletion, you should enable Multi-Factor Authentication (MFA) protected access enable Delete Protection on the ALB enabled Termination Protection on the ALB ALB does not provide any feature to prevent accidental deletion Your organization is using ALB for servicing requests. One of the API request is facing consistent performance issues. Upon checking the flow, you find that the request flows through multiple services. How can you track the performance or timing issues in the application stack at the granularity of an individual request? Track the request using \u201cX-Amzn-Trace-Id\u201d HTTP header Track the request using \u201cX-Amzn-Track-Id\u201d HTTP header Track the request using \u201cX-Aws-Track-Id\u201d HTTP header Track the request using \u201cX-Aws-Trace-Id\u201d HTTP header","title":"L5 AWS Application Load Balancer \u2013 ALB"},{"location":"chap3/5ALB/#l5-aws-application-load-balancer-alb","text":"","title":"L5 AWS Application Load Balancer \u2013 ALB"},{"location":"chap3/5ALB/#1-aws-application-load-balancer-alb","text":"Application Load Balancer operates at layer 7 (application layer) and allows defining routing rules based on content across multiple services or containers running on one or more EC2 instances scales the load balancer as traffic to the application changes over time. can scale to the vast majority of workloads automatically. supports health checks, used to monitor the health of registered targets so that the load balancer can send requests only to the healthy targets.","title":"1 AWS Application Load Balancer - ALB"},{"location":"chap3/5ALB/#2application-load-balancer-components","text":"","title":"2\u3001Application Load Balancer Components"},{"location":"chap3/5ALB/#2-1-a-load-balancer","text":"serves as the single point of contact for clients. distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple AZs, which increases the availability of the application. one or more listeners can be added to the load balancer.","title":"2-1 A load balancer"},{"location":"chap3/5ALB/#2-2-a-listener","text":"checks for connection requests from clients, using the configured protocol and port rules defined determine, how the load balancer routes requests to its registered targets. each rule consists of a priority, one or more actions, and one or more conditions. when the conditions for a rule are met, its actions are performed. a default rule for each listener must be defined, and optionally additional rules can be defined","title":"2-2 A listener"},{"location":"chap3/5ALB/#2-3-target-group","text":"routes requests to one or more registered targets, such as EC2 instances, using the specified protocol and port number a target can be registered with multiple target groups . health checks can be configured on a per target group basis. Health checks are performed on all targets registered to a target group that is specified in a listener rule for the load balancer. target group supports EC2 instances (can be managed as a part of Autoscaling group) ECS tasks Lambda functions IP Addresses \u2013 must be private IP When a load balancer receives a request, it evaluates the listener rules in priority order to determine which rule to apply and then selects a target from the target group for the rule action. Listener rules can be configured to r oute requests to different target groups based on the content of the application traffic. Routing is performed independently for each target group, even when a target is registered with multiple target groups. Routing algorithm used can be configured at the target group level . Default routing algorithm is round-robin ; alternatively, the least outstanding requests routing algorithm can also be specified","title":"2-3 Target group"},{"location":"chap3/5ALB/#3application-load-balancer-benefits","text":"Support for Path-based routing, where listener rules can be configured to forward requests based on the URL in the request . This enables structuring application as smaller services (microservices), and route requests to the correct service based on the content of the URL. Support for routing requests to multiple services on a single EC2 instance by registering the instance using multiple ports . Support for containerized applications . EC2 Container Service (ECS) can select an unused port when scheduling a task and register the task with a target group using this port, enabling efficient use of the clusters. Support for monitoring the health of each service independently, as health checks are defined at the target group level and many CloudWatch metrics are reported at the target group level. Attaching a target group to an Auto Scaling group enables scaling each service dynamically based on demand .","title":"3\u3001Application Load Balancer Benefits"},{"location":"chap3/5ALB/#4application-load-balancer-features","text":"supports load balancing of applications using HTTP and HTTPS (Secure HTTP) protocols supports HTTP/2 , which is enabled natively. Clients that support HTTP/2 can connect over TLS supports WebSockets and Secure WebSockets natively supports Request tracing , by default. request tracing can be used to track HTTP requests from clients to targets or other services. Load balancer upon receiving a request from a client, adds or updates the X-Amzn-Trace-Id header before sending the request to the target Any services or applications between the load balancer and the target can also add or update this header. supports Sticky Sessions (Session Affinity) using load balancer generated cookies, to route requests from the same client to the same target supports SSL termination , to decrypt the request on ALB before sending it to the underlying targets. an SSL certificate can be installed on the load balancer. the load balancer uses this certificate to terminate the connection and then decrypt requests from clients before sending them to targets. supports layer 7 specific features like X-Forwarded-For headers to help determine the actual client IP, port and protocol automatically scales its request handling capacity in response to incoming application traffic. supports hybrid load balancing , If an application runs on targets distributed between a VPC and an on-premises location, they can be added to the same target group using their IP addresses provides High Availability , by allowing you to specify more than one AZ and distribution of incoming traffic across multiple AZs. integrates with ACM to provision and bind an SSL/TLS certificate to the load balancer thereby making the entire SSL offload process very easy supports multiple certificates for the same domain to a secure listener supports IPv6 addressing, for an Internet-facing load balancer supports Cross-zone load balancing, by default supports Security Groups to control the traffic allowed to and from the load balancer. provides Access Logs , to record all requests sent to the load balancer, and store the logs in S3 for later analysis in compressed format provides Delete Protection , to prevent the ALB from accidental deletion supports Connection Idle Timeout \u2013 ALB maintains two connections for each request one with the Client (front end) and one with the target instance (back end). If no data has been sent or received by the time that the idle timeout period elapses, ALB closes the front-end connection integrates with CloudWatch to provide metrics, such as request counts, error counts, error types, and request latency integrates with AWS WAF , a web application firewall that helps protect web applications from attacks by allowing rules configuration based on IP addresses, HTTP headers, and custom URI strings integrates with CloudTrail to receive a history of ALB API calls made on the AWS account\u2018\u2019","title":"4\u3001Application Load Balancer Features"},{"location":"chap3/5ALB/#5application-load-balancer-listeners","text":"A listener is a process that checks for connection requests, using the configured protocol and port Listener supports HTTP & HTTPS protocol with Ports from 1-65535 ALB supports SSL Termination for HTTPS listener, which helps to offload the work of encryption and decryption so that the targets can focus on their main work. HTTPS listener must have at least one SSL server certificate on the listener WebSockets with both HTTP and HTTPS listeners (Secure WebSockets) Supports HTTP/2 with HTTPS listeners 128 requests can be sent in parallel using one HTTP/2 connection. ALB converts these to individual HTTP/1.1 requests and distributes them across the healthy targets in the target group using the round robin routing algorithm. HTTP/2 uses front-end connections more efficiently resulting in fewer connections between clients and the load balancer. Server-push feature of HTTP/2 is not supported Each listener has a default rule, and can optionally define additional rules. Each rule consists of a priority, action, optional host condition, and optional path condition . Priority \u2013 Rules are evaluated in priority order, from the lowest value to the highest value. The default rule has the lowest priority Action \u2013 Each rule action has a type and a target group. Currently, the only supported type is forward, which forwards requests to the target group. You can change the target group for a rule at any time. Condition \u2013 There are two types of rule conditions: host and path. When the conditions for a rule are met, then its action is taken","title":"5\u3001Application Load Balancer Listeners"},{"location":"chap3/5ALB/#5-1-host-condition-or-host-based-routing","text":"Host conditions can be used to define rules that forward requests to different target groups based on the hostname in the host header This enables support for multiple domains using a single ALB for e.g. orders.example.com , images.example.com , registration.example.com Each host condition has one hostname. If the hostname in","title":"5-1 Host Condition or Host-based routing"},{"location":"chap3/5ALB/#5-2-path-condition-or-path-based-routing","text":"Path conditions can be used to define rules that forward requests to different target groups based on the URL in the request Each path condition has one path pattern for e.g. example.com/orders , example.com/images , example.com/registration If the URL in a request matches the path pattern in a listener rule exactly, the request is routed using that rule.","title":"5-2 Path Condition or path-based routing"},{"location":"chap3/5ALB/#6advantages-over-classic-load-balancer","text":"Support for path-based routing , where rules can be configured for the listener to forward requests based on the content of the URL Support for host-based routing , where rules can be configured for the listener to forward requests based on the host field in the HTTP header. Support for routing based on fields in the request, such as standard and custom HTTP headers and methods, query parameters, and source IP address Support for routing requests to multiple applications on a single EC2 instance. Each instance or IP address can be registered with the same target group using multiple ports Support for registering targets by IP address , including targets outside the VPC for the load balancer. Support for redirecting requests from one URL to another . Support for returning a custom HTTP response. Support for registering Lambda functions as targets. Support for the load balancer to authenticate users of the applications through their corporate or social identities before routing requests. Support containerized applications with ECS using Dynamic port mapping Support monitoring the health of each service independently , as health checks and many CloudWatch metrics are defined at the target group level Attaching the target group to an Auto Scaling group enables scaling of each service dynamically based on demand Access logs contain additional information & stored in compressed format Improved load balancer performance .","title":"6\u3001Advantages over Classic Load Balancer"},{"location":"chap3/5ALB/#7application-load-balancer-pricing","text":"charged for each hour or partial hour that an ALB is running and the number of Load Balancer Capacity Units (LCU) used per hour. An LCU is a new metric for determining ALB pricing An LCU defines the maximum resource consumed in any one of the dimensions (new connections, active connections, bandwidth and rule evaluations) the Application Load Balancer processes the traffic.","title":"7\u3001Application Load Balancer Pricing"},{"location":"chap3/5ALB/#8application-load-balancer-exam","text":"You are designing an application which requires websockets support, to exchange real-time messages with end-users without the end users having to request (or poll) the server for an update? Which ELB option should you choose? Use Application Load Balancer and enable comet support Use Classic Load Balancer which supports WebSockets Use Application Load Balancer which supports WebSockets Use Classic Load Balancer and enable comet support Which of the following Internet protocols does an AWS Application Load Balancer Support? Choose 2 answers A. ICMP B. UDP C. HTTP D. SNTP E. Websocket Your organization has configured an application behind ALB. However, Clients are complaining that they cannot connect to an Internet-facing load balancer. What cannot be the issue? Internet-facing load balancer is attached to a private subnet ALB Security Groups does not allow the traffic Subnet NACLs do not allow the traffic ALB was not assigned an EIP To protect your ALB from accidental deletion, you should enable Multi-Factor Authentication (MFA) protected access enable Delete Protection on the ALB enabled Termination Protection on the ALB ALB does not provide any feature to prevent accidental deletion Your organization is using ALB for servicing requests. One of the API request is facing consistent performance issues. Upon checking the flow, you find that the request flows through multiple services. How can you track the performance or timing issues in the application stack at the granularity of an individual request? Track the request using \u201cX-Amzn-Trace-Id\u201d HTTP header Track the request using \u201cX-Amzn-Track-Id\u201d HTTP header Track the request using \u201cX-Aws-Track-Id\u201d HTTP header Track the request using \u201cX-Aws-Trace-Id\u201d HTTP header","title":"8\u3001Application Load Balancer Exam"},{"location":"chap3/6NLB/","text":"L6 AWS Network Load Balancer \u2013 NLB 1\u3001AWS Network Load Balancer \u2013 NLB Network Load Balancer operates at the connection level (Layer 4), routing connections to targets \u2013 EC2 instances, and containers based on IP protocol data . Network Load Balancer is suited for load balancing of TCP traffic Network Load Balancer is capable of handling millions of requests per second while maintaining ultra-low latencies (~100 ms vs 400 ms for ALB) * Network Load Balancer is optimized to handle sudden and volatile traffic patterns while using a single static IP address per Availability Zone. Network Load Balancer also supports TLS termination , preserves the source IP of the clients, and provides stable IP support and Zonal isolation. NLB supports long-running connections that are very useful for WebSocket-type applications. NLB is integrated with other AWS services such as Auto Scaling, EC2 Container Service (ECS), and CloudFormation. NLB supports connections from clients over VPC peering, AWS-managed VPN, and third-party VPN solutions. For TCP traffic , the load balancer selects a target using a flow hash algorithm based on the protocol, source IP address, source port, destination IP address, destination port, and TCP sequence number. TCP connections from a client having different source ports and sequence numbers and can be routed to different targets. Each individual TCP connection is routed to a single target for the life of the connection. For UDP traffic , the load balancer selects a target using a flow hash algorithm based on the protocol, source IP address, source port, destination IP address, and destination port. A UDP flow has the same source and destination, so it is consistently routed to a single target throughout its lifetime. Different UDP flows have a different source IP addresses and ports, so they can be routed to different targets. back-end server authentication is not supported 2\u3001Network Load Balancer Features 2-1 Connection-based Layer 4 Load Balancing Allows load balancing of both TCP and UDP traffic, routing connections to targets \u2013 EC2 instances, microservices, and containers. 2-2 High Availability is highly available. accepts incoming traffic from clients and distributes this traffic across the targets within the same Availability Zone . monitors the health of its registered targets and routes the traffic only to healthy targets if a health check fails and an unhealthy target is detected, it stops routing traffic to that target and reroutes traffic to remaining healthy targets. if configured with multiple AZs and if all the targets in a single AZ fail, it routes traffic to healthy targets in the other AZs 2-3 High Throughput is designed to handle traffic as it grows and can load balance millions of requests/sec. can also handle sudden volatile traffic patterns. 2-4 Low Latency offers extremely low latencies for latency-sensitive applications. 2-5 Cross Zone Load Balancing enable cross-zone loading balancing only after creating the NLB is disabled, by default, and charges apply for inter-az traffic . 2-6 Sticky Sessions Sticky sessions (source IP affinity) are a mechanism to route requests from the same client to the same target. Stickiness is defined at the target group level 2-7 Load Balancing using IP addresses as Targets allows load balancing of any application hosted in AWS or on-premises using IP addresses of the application backends as targets . allows load balancing to an application backend hosted on any IP address and any interface on an instance. ability to load balance across AWS and on-premises resources helps migrate-to-cloud, burst-to-cloud or failover-to-cloud applications hosted in on-premises locations can be used as targets over a Direct Connect connection and EC2-Classic (using ClassicLink). 2-8 Preserve source IP address preserves client-side source IP allowing the back-end to see client IP address Target groups can be created with target type as instance ID or IP address . If targets registered by instance ID, the source IP addresses of the clients are preserved and provided to the applications . If register targets registered by IP address, the source IP addresses are the private IP addresses of the load balancer nodes . 2-9 Static IP support automatically provides a static IP per Availability Zone (subnet) that can be used by applications as the front-end IP of the load balancer . Elastic Load Balancing creates a network interface for each enabled Availability Zone. Each load balancer node in the AZ uses this network interface to get a static IP address. Internet-facing load balancer can optionally associate one Elastic IP address per subnet. 2-10 Elastic IP support an Elastic IP per Availability Zone (subnet) can also be assigned, optionally, thereby providing a fixed IP. 2-11 Health Checks supports both network and application target health checks. Network-level health check is based on the overall response of the underlying target (instance or a container) to normal traffic. target is marked unavailable if it is slow or unable to respond to new connection requests Application-level health check is based on a specific URL on a given target to test the application health deeper 2-11 DNS Fail-over integrates with Route 53 Route 53 will direct traffic to load balancer nodes in other AZs, if there are no healthy targets with NLB or if the NLB itself is unhealthy if NLB is unresponsive, Route 53 will remove the unavailable load balancer IP address from service and direct traffic to an alternate Network Load Balancer in another region. 3\u3001Integration with AWS Services is integrated with other AWS services such as Auto Scaling, EC2 Container Service (ECS), CloudFormation, CodeDeploy, and AWS Config. 3-1 Long-lived TCP Connections supports long-lived TCP connections ideal for WebSocket type of applications 3-2 Central API Support uses the same API as Application Load Balancer . enables you to work with target groups, health checks, and load balance across multiple ports on the same EC2 instance to support containerized applications. 3-3 Robust Monitoring and Auditing integrated with CloudWatch to report Network Load Balancer metrics. CloudWatch provides metrics such as Active Flow count, Healthy Host Count, New Flow Count, Processed bytes, and more. integrated with CloudTrail to track API calls to the NLB 3-4 Enhanced Logging use the Flow Logs feature to record all requests sent to the load balancer. Flow Logs capture information about the IP traffic going to and from network interfaces in the VPC Flow log data is stored using CloudWatch Logs 3-5 Zonal Isolation is designed for application architectures in a single zone. can be enabled in a single AZ to support architectures that require zonal isolation automatically fails-over to other healthy AZs, if something fails in an AZ it\u2019s recommended to configure the load balancer and targets in multiple AZs for achieving high availability 4 Advantages over Classic Load Balancer Ability to handle volatile workloads and scale to millions of requests per second, without the need of pre-warming Support for static IP/Elastic IP addresses for the load balancer Support for registering targets by IP address, including targets outside the VPC (on-premises) for the load balancer. Support for routing requests to multiple applications on a single EC2 instance . A single instance or IP address can be registered with the same target group using multiple ports. Support for containerized applications. Using Dynamic port mapping, ECS can select an unused port when scheduling a task and register the task with a target group using this port . Support for monitoring the health of each service independently, as health checks are defined at the target group level and many CloudWatch metrics are reported at the target group level. Attaching a target group to an Auto Scaling group enables scaling each service dynamically based on demand 5 Network Load Balancer Pricing charged for each hour or partial hour that an NLB is running and the number of Load Balancer Capacity Units (LCU) used per hour. An LCU is a new metric for determining NLB pricing An LCU defines the maximum resource consumed in any one of the dimensions (new connections/flows, active connections/flows, bandwidth and rule evaluations) the Network Load Balancer processes your traffic. 6\u3001AWS Certification Exam Practice Questions 1.A company wants to use load balancer for their application. However, the company wants to forward the requests without any header modification . What service should the company use? Classic Load Balancer Network Load Balancer Application Load Balancer Use Route 53 2.A company is hosting an application in AWS for third party access. The third party needs to whitelist the application based on the IP. Which AWS service can the company use in the whitelisting of the IP address? AWS Application Load Balancer AWS Classic Load balancer AWS Network Load Balancer AWS Route 53","title":"L6 AWS Network Load Balancer \u2013 NLB"},{"location":"chap3/6NLB/#l6-aws-network-load-balancer-nlb","text":"","title":"L6 AWS Network Load Balancer \u2013 NLB"},{"location":"chap3/6NLB/#1aws-network-load-balancer-nlb","text":"Network Load Balancer operates at the connection level (Layer 4), routing connections to targets \u2013 EC2 instances, and containers based on IP protocol data . Network Load Balancer is suited for load balancing of TCP traffic Network Load Balancer is capable of handling millions of requests per second while maintaining ultra-low latencies (~100 ms vs 400 ms for ALB) * Network Load Balancer is optimized to handle sudden and volatile traffic patterns while using a single static IP address per Availability Zone. Network Load Balancer also supports TLS termination , preserves the source IP of the clients, and provides stable IP support and Zonal isolation. NLB supports long-running connections that are very useful for WebSocket-type applications. NLB is integrated with other AWS services such as Auto Scaling, EC2 Container Service (ECS), and CloudFormation. NLB supports connections from clients over VPC peering, AWS-managed VPN, and third-party VPN solutions. For TCP traffic , the load balancer selects a target using a flow hash algorithm based on the protocol, source IP address, source port, destination IP address, destination port, and TCP sequence number. TCP connections from a client having different source ports and sequence numbers and can be routed to different targets. Each individual TCP connection is routed to a single target for the life of the connection. For UDP traffic , the load balancer selects a target using a flow hash algorithm based on the protocol, source IP address, source port, destination IP address, and destination port. A UDP flow has the same source and destination, so it is consistently routed to a single target throughout its lifetime. Different UDP flows have a different source IP addresses and ports, so they can be routed to different targets. back-end server authentication is not supported","title":"1\u3001AWS Network Load Balancer \u2013 NLB"},{"location":"chap3/6NLB/#2network-load-balancer-features","text":"","title":"2\u3001Network Load Balancer Features"},{"location":"chap3/6NLB/#2-1-connection-based-layer-4-load-balancing","text":"Allows load balancing of both TCP and UDP traffic, routing connections to targets \u2013 EC2 instances, microservices, and containers.","title":"2-1 Connection-based Layer 4 Load Balancing"},{"location":"chap3/6NLB/#2-2-high-availability","text":"is highly available. accepts incoming traffic from clients and distributes this traffic across the targets within the same Availability Zone . monitors the health of its registered targets and routes the traffic only to healthy targets if a health check fails and an unhealthy target is detected, it stops routing traffic to that target and reroutes traffic to remaining healthy targets. if configured with multiple AZs and if all the targets in a single AZ fail, it routes traffic to healthy targets in the other AZs","title":"2-2 High Availability"},{"location":"chap3/6NLB/#2-3-high-throughput","text":"is designed to handle traffic as it grows and can load balance millions of requests/sec. can also handle sudden volatile traffic patterns.","title":"2-3 High Throughput"},{"location":"chap3/6NLB/#2-4-low-latency","text":"offers extremely low latencies for latency-sensitive applications.","title":"2-4 Low Latency"},{"location":"chap3/6NLB/#2-5-cross-zone-load-balancing","text":"enable cross-zone loading balancing only after creating the NLB is disabled, by default, and charges apply for inter-az traffic .","title":"2-5 Cross Zone Load Balancing"},{"location":"chap3/6NLB/#2-6-sticky-sessions","text":"Sticky sessions (source IP affinity) are a mechanism to route requests from the same client to the same target. Stickiness is defined at the target group level","title":"2-6 Sticky Sessions"},{"location":"chap3/6NLB/#2-7-load-balancing-using-ip-addresses-as-targets","text":"allows load balancing of any application hosted in AWS or on-premises using IP addresses of the application backends as targets . allows load balancing to an application backend hosted on any IP address and any interface on an instance. ability to load balance across AWS and on-premises resources helps migrate-to-cloud, burst-to-cloud or failover-to-cloud applications hosted in on-premises locations can be used as targets over a Direct Connect connection and EC2-Classic (using ClassicLink).","title":"2-7 Load Balancing using IP addresses as Targets"},{"location":"chap3/6NLB/#2-8-preserve-source-ip-address","text":"preserves client-side source IP allowing the back-end to see client IP address Target groups can be created with target type as instance ID or IP address . If targets registered by instance ID, the source IP addresses of the clients are preserved and provided to the applications . If register targets registered by IP address, the source IP addresses are the private IP addresses of the load balancer nodes .","title":"2-8 Preserve source IP address"},{"location":"chap3/6NLB/#2-9-static-ip-support","text":"automatically provides a static IP per Availability Zone (subnet) that can be used by applications as the front-end IP of the load balancer . Elastic Load Balancing creates a network interface for each enabled Availability Zone. Each load balancer node in the AZ uses this network interface to get a static IP address. Internet-facing load balancer can optionally associate one Elastic IP address per subnet.","title":"2-9 Static IP support"},{"location":"chap3/6NLB/#2-10-elastic-ip-support","text":"an Elastic IP per Availability Zone (subnet) can also be assigned, optionally, thereby providing a fixed IP.","title":"2-10 Elastic IP support"},{"location":"chap3/6NLB/#2-11-health-checks","text":"supports both network and application target health checks. Network-level health check is based on the overall response of the underlying target (instance or a container) to normal traffic. target is marked unavailable if it is slow or unable to respond to new connection requests Application-level health check is based on a specific URL on a given target to test the application health deeper","title":"2-11 Health Checks"},{"location":"chap3/6NLB/#2-11-dns-fail-over","text":"integrates with Route 53 Route 53 will direct traffic to load balancer nodes in other AZs, if there are no healthy targets with NLB or if the NLB itself is unhealthy if NLB is unresponsive, Route 53 will remove the unavailable load balancer IP address from service and direct traffic to an alternate Network Load Balancer in another region.","title":"2-11 DNS Fail-over"},{"location":"chap3/6NLB/#3integration-with-aws-services","text":"is integrated with other AWS services such as Auto Scaling, EC2 Container Service (ECS), CloudFormation, CodeDeploy, and AWS Config.","title":"3\u3001Integration with AWS Services"},{"location":"chap3/6NLB/#3-1-long-lived-tcp-connections","text":"supports long-lived TCP connections ideal for WebSocket type of applications","title":"3-1 Long-lived TCP Connections"},{"location":"chap3/6NLB/#3-2-central-api-support","text":"uses the same API as Application Load Balancer . enables you to work with target groups, health checks, and load balance across multiple ports on the same EC2 instance to support containerized applications.","title":"3-2 Central API Support"},{"location":"chap3/6NLB/#3-3-robust-monitoring-and-auditing","text":"integrated with CloudWatch to report Network Load Balancer metrics. CloudWatch provides metrics such as Active Flow count, Healthy Host Count, New Flow Count, Processed bytes, and more. integrated with CloudTrail to track API calls to the NLB","title":"3-3 Robust Monitoring and Auditing"},{"location":"chap3/6NLB/#3-4-enhanced-logging","text":"use the Flow Logs feature to record all requests sent to the load balancer. Flow Logs capture information about the IP traffic going to and from network interfaces in the VPC Flow log data is stored using CloudWatch Logs","title":"3-4 Enhanced Logging"},{"location":"chap3/6NLB/#3-5-zonal-isolation","text":"is designed for application architectures in a single zone. can be enabled in a single AZ to support architectures that require zonal isolation automatically fails-over to other healthy AZs, if something fails in an AZ it\u2019s recommended to configure the load balancer and targets in multiple AZs for achieving high availability","title":"3-5 Zonal Isolation"},{"location":"chap3/6NLB/#4-advantages-over-classic-load-balancer","text":"Ability to handle volatile workloads and scale to millions of requests per second, without the need of pre-warming Support for static IP/Elastic IP addresses for the load balancer Support for registering targets by IP address, including targets outside the VPC (on-premises) for the load balancer. Support for routing requests to multiple applications on a single EC2 instance . A single instance or IP address can be registered with the same target group using multiple ports. Support for containerized applications. Using Dynamic port mapping, ECS can select an unused port when scheduling a task and register the task with a target group using this port . Support for monitoring the health of each service independently, as health checks are defined at the target group level and many CloudWatch metrics are reported at the target group level. Attaching a target group to an Auto Scaling group enables scaling each service dynamically based on demand","title":"4 Advantages over Classic Load Balancer"},{"location":"chap3/6NLB/#5-network-load-balancer-pricing","text":"charged for each hour or partial hour that an NLB is running and the number of Load Balancer Capacity Units (LCU) used per hour. An LCU is a new metric for determining NLB pricing An LCU defines the maximum resource consumed in any one of the dimensions (new connections/flows, active connections/flows, bandwidth and rule evaluations) the Network Load Balancer processes your traffic.","title":"5 Network Load Balancer Pricing"},{"location":"chap3/6NLB/#6aws-certification-exam-practice-questions","text":"1.A company wants to use load balancer for their application. However, the company wants to forward the requests without any header modification . What service should the company use? Classic Load Balancer Network Load Balancer Application Load Balancer Use Route 53 2.A company is hosting an application in AWS for third party access. The third party needs to whitelist the application based on the IP. Which AWS service can the company use in the whitelisting of the IP address? AWS Application Load Balancer AWS Classic Load balancer AWS Network Load Balancer AWS Route 53","title":"6\u3001AWS Certification Exam Practice Questions"},{"location":"chap3/7EAN_LB/","text":"L7 AWS Classic Load Balancer vs Application Load Balancer vs Network Load Balancer 1 AWS Classic Load Balancer vs Application Load Balancer vs Network Load Balancer Elastic Load Balancing supports three types of load balancers: Application Load Balancer, Network Load Balancer and Classic Load Balancers. While there is some overlap in the features, AWS does not maintain feature parity between the different types of load balancers. 2 Classic Load Balancer vs Application Load Balancer vs Network Load Balancer Usage Patterns 2-1 Classic Load Balancer provides basic load balancing across multiple EC2 instances and operates at both the request level and connection level. is intended for applications that were built within the EC2-Classic network. is ideal for simple load balancing of traffic across multiple EC2 instances . 2-2 Application Load Balancer is ideal for microservices or container-based architectures where there is a need to route traffic to multiple services or load balance across multiple ports on the same EC2 instance . operates at the request level (layer 7), routing traffic to targets \u2013 EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request . is ideal for advanced load balancing of HTTP and HTTPS traffic, and provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications. simplifies and improves the security of the application, by ensuring that the latest SSL/TLS ciphers and protocols are used at all times. 2-3 Network Load Balancer operates at the connection level (Layer 4), routing connections to targets \u2013 EC2 instances, microservices, and containers \u2013 within VPC based on IP protocol data. is ideal for load balancing of both TCP and UDP traffic , is capable of handling millions of requests per second while maintaining ultra-low latencies. is optimized to handle sudden and volatile traffic patterns while using a single static IP address per AZ is integrated with other popular AWS services such as Auto Scaling, ECS, CloudFormation, and AWS Certificate Manager (ACM). AWS recommends using Application Load Balancer for Layer 7 and Network Load Balancer for Layer 4 when using VPC. 3\u3001Supported Protocols Classic ELB operates at layer 4 and supports HTTP, HTTPS, TCP, SSL ALB operates at layer 7 and supports HTTP, HTTPS, HTTP/2, WebSockets NLB operates at the connection level (Layer 4) 4\u3001Load Balancing to multiple ports on the same instance Only ALB & NLB supports Load Balancing to multiple ports on the same instance 5\u3001Host-based Routing & Path-based Routing Host-based routing use host conditions to define rules that forward requests to different target groups based on the hostname in the host header. This enables ALB to support multiple domains using a single load balancer. Path-based routing use path conditions to define rules that forward requests to different target groups based on the URL in the request. Each path condition has one path pattern. If the URL in a request matches the path pattern in a listener rule exactly, the request is routed using that rule. Only ALB supports Host-based & Path-based routing. 5\u3001Classic Load Balancer vs Application Load Balancer vs Network Load Balancer Common Features 5-1 Slow Start By default, a target starts to receive its full share of requests as soon as it is registered with a target group and passes an initial health check. Using slow start mode gives targets time to warm up before the load balancer sends them a full share of requests. Only ALB supports slow start mode 5-2 Static IP and Elastic IP Address NLB automatically provides a static IP per AZ (subnet) that can be used by applications as the front-end IP of the load balancer. NLB also allows the option to assign an Elastic IP per AZ (subnet) thereby providing your own fixed IP. Classic ELB and ALB does not support Static and Elastic IP address 5-3 Connection Draining Connection draining enables the load balancer to complete in-flight requests made to instances that are de-registering or unhealthy All Load Balancer types support connection draining 5-4 Idle Connection Timeout Idle Connection Timeout helps specify a time period, which ELB uses to close the connection if no data has been sent or received by the time that the idle timeout period elapses Both Classic ELB & ALB supports idle connection timeout NLB does not support idle connection timeout 5-5 PrivateLink Support CLB and ALB does not support PrivateLink (TCP, TLS) Only NLB supports PrivateLink (TCP, TLS) 5-6 Zonal Isolation Only NLB supports Zonal Isolation which supports application architectures in a single zone. It automatically fails-over to other healthy AZs, if something fails in an AZ CLB and ALB does not support Zonal Isolation 5-7 Deletion Protection Only ALB & NLB supports Deletion Protection, wherein a load balancer can\u2019t be deleted if deletion protection is enabled CLB does not support deletion protection 5-8 Preserve source IP address NLB preserves the client-side source IP allowing the back-end to see the IP address of the client . Classic ELB and ALB do not preserve the client-side source IP. It needs to be retrieved using X-Forward-Header or proxy protocol 5-9 Health Checks All Load Balancer types support Health checks to determine if the instance is healthy or unhealthy ALB provides health check improvements that allow detailed error codes from 200-399 to be configured 5-10 Supported Platforms Classic ELB supports both EC2-Classic and EC2-VPC ALB and NLB supports only EC2-VPC 5-11 WebSockets CLB does not support WebSockets Only ALB and NLB supports WebSockets 5-12 Cross-zone Load Balancing By default, Load Balancer will evenly distribute requests evenly across its enabled AZs, irrespective of the instances it hosts. Cross-zone Load Balancing help distribute incoming requests evenly across all instances in its enabled AZs. All Load Balancer types support Cross-zone load balancing, however, for Classic it needs to be enabled while for ALB it is always enabled 5-13 Stick Sessions (Cookies) Stick Sessions (Session Affinity) enables the load balancer to bind a user\u2019s session to a specific instance, which ensures that all requests from the user during the session are sent to the same instance Classic ELB, ALB, and NLB supports sticky sessions to maintain session affinity NLB now supports sticky sessions 5-14 Classic Load Balancer vs Application Load Balancer vs Network Load Balancer Security Features 5-14 SSL Termination/Offloading SSL Termination helps decrypt requests from clients before sending them to targets and hence reducing the load. SSL certificate must be installed on the load balancer. All load balancers types support SSL Termination 5-15 Server Name Indication CLB only supports a single certificate and does not support SNI ALB and NLB support multiple certificates and uses SNI to serve multiple secure websites using a single TLS listener . If the hostname in the client matches multiple certificates, the load balancer selects the best certificate to use based on a smart selection algorithm. 6\u3001Classic Load Balancer vs Application Load Balancer vs Network Load Balancer Operational Features 6-1 CloudWatch Metrics All Load Balancer types integrate with CloudWatch to provide metrics, with ALB providing additional metrics 6-2 Access Logs Access logs capture detailed information about requests sent to the load balancer. Each log contains information such as request received time, client\u2019s IP address, latencies, request paths, and server responses All Load Balancer types provide access logs, with ALB providing additional attributes 7\u3001AWS Certification Exam Practice Questions A company wants to use load balancer for their application. However, the company wants to forward the requests without any header modification . What service should the company use? Classic Load Balancer Network Load Balancer Application Load Balancer Use Route 53 Solutions Architect is building an Amazon ECS-based web application that requires that headers are not modified when being forwarded to Amazon ECS. Which load balancer should the Architect use? Application Load Balancer Network Load Balancer A virtual load balancer appliance from AWS marketplace Classic Load Balancer An application tier currently hosts two web services on the same set of instances, listening on different ports. Which AWS service should a solutions architect use to route traffic to the service based on the incoming request? AWS Application Load Balancer Amazon CloudFront Amazon Route 53 AWS Classic Load Balancer A Solutions Architect needs to deploy an HTTP/HTTPS service on Amazon EC2 instances with support for WebSockets using load balancers. How can the Architect meet these requirements? Configure a Network Load balancer. Configure an Application Load Balancer. Configure a Classic Load Balancer. Configure a Layer-4 Load Balancer. A company is hosting an application in AWS for third party access. The third party needs to whitelist the application based on the IP. Which AWS service can the company use in the whitelisting of the IP address? AWS Application Load Balancer AWS Classic Load balancer AWS Network Load Balancer AWS Route 53","title":"L7 AWS Classic Load Balancer vs Application Load Balancer vs Network Load Balancer"},{"location":"chap3/7EAN_LB/#l7-aws-classic-load-balancer-vs-application-load-balancer-vs-network-load-balancer","text":"","title":"L7 AWS Classic Load Balancer vs Application Load Balancer vs Network Load Balancer"},{"location":"chap3/7EAN_LB/#1-aws-classic-load-balancer-vs-application-load-balancer-vs-network-load-balancer","text":"Elastic Load Balancing supports three types of load balancers: Application Load Balancer, Network Load Balancer and Classic Load Balancers. While there is some overlap in the features, AWS does not maintain feature parity between the different types of load balancers.","title":"1 AWS Classic Load Balancer vs Application Load Balancer vs Network Load Balancer"},{"location":"chap3/7EAN_LB/#2-classic-load-balancer-vs-application-load-balancer-vs-network-load-balancer-usage-patterns","text":"","title":"2 Classic Load Balancer vs Application Load Balancer vs Network Load Balancer Usage Patterns"},{"location":"chap3/7EAN_LB/#2-1-classic-load-balancer","text":"provides basic load balancing across multiple EC2 instances and operates at both the request level and connection level. is intended for applications that were built within the EC2-Classic network. is ideal for simple load balancing of traffic across multiple EC2 instances .","title":"2-1 Classic Load Balancer"},{"location":"chap3/7EAN_LB/#2-2-application-load-balancer","text":"is ideal for microservices or container-based architectures where there is a need to route traffic to multiple services or load balance across multiple ports on the same EC2 instance . operates at the request level (layer 7), routing traffic to targets \u2013 EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request . is ideal for advanced load balancing of HTTP and HTTPS traffic, and provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications. simplifies and improves the security of the application, by ensuring that the latest SSL/TLS ciphers and protocols are used at all times.","title":"2-2 Application Load Balancer"},{"location":"chap3/7EAN_LB/#2-3-network-load-balancer","text":"operates at the connection level (Layer 4), routing connections to targets \u2013 EC2 instances, microservices, and containers \u2013 within VPC based on IP protocol data. is ideal for load balancing of both TCP and UDP traffic , is capable of handling millions of requests per second while maintaining ultra-low latencies. is optimized to handle sudden and volatile traffic patterns while using a single static IP address per AZ is integrated with other popular AWS services such as Auto Scaling, ECS, CloudFormation, and AWS Certificate Manager (ACM). AWS recommends using Application Load Balancer for Layer 7 and Network Load Balancer for Layer 4 when using VPC.","title":"2-3 Network Load Balancer"},{"location":"chap3/7EAN_LB/#3supported-protocols","text":"Classic ELB operates at layer 4 and supports HTTP, HTTPS, TCP, SSL ALB operates at layer 7 and supports HTTP, HTTPS, HTTP/2, WebSockets NLB operates at the connection level (Layer 4)","title":"3\u3001Supported Protocols"},{"location":"chap3/7EAN_LB/#4load-balancing-to-multiple-ports-on-the-same-instance","text":"Only ALB & NLB supports Load Balancing to multiple ports on the same instance","title":"4\u3001Load Balancing to multiple ports on the same instance"},{"location":"chap3/7EAN_LB/#5host-based-routing-path-based-routing","text":"Host-based routing use host conditions to define rules that forward requests to different target groups based on the hostname in the host header. This enables ALB to support multiple domains using a single load balancer. Path-based routing use path conditions to define rules that forward requests to different target groups based on the URL in the request. Each path condition has one path pattern. If the URL in a request matches the path pattern in a listener rule exactly, the request is routed using that rule. Only ALB supports Host-based & Path-based routing.","title":"5\u3001Host-based Routing &amp; Path-based Routing"},{"location":"chap3/7EAN_LB/#5classic-load-balancer-vs-application-load-balancer-vs-network-load-balancer-common-features","text":"","title":"5\u3001Classic Load Balancer vs Application Load Balancer vs Network Load Balancer Common Features"},{"location":"chap3/7EAN_LB/#5-1-slow-start","text":"By default, a target starts to receive its full share of requests as soon as it is registered with a target group and passes an initial health check. Using slow start mode gives targets time to warm up before the load balancer sends them a full share of requests. Only ALB supports slow start mode","title":"5-1 Slow Start"},{"location":"chap3/7EAN_LB/#5-2-static-ip-and-elastic-ip-address","text":"NLB automatically provides a static IP per AZ (subnet) that can be used by applications as the front-end IP of the load balancer. NLB also allows the option to assign an Elastic IP per AZ (subnet) thereby providing your own fixed IP. Classic ELB and ALB does not support Static and Elastic IP address","title":"5-2 Static IP and Elastic IP Address"},{"location":"chap3/7EAN_LB/#5-3-connection-draining","text":"Connection draining enables the load balancer to complete in-flight requests made to instances that are de-registering or unhealthy All Load Balancer types support connection draining","title":"5-3 Connection Draining"},{"location":"chap3/7EAN_LB/#5-4-idle-connection-timeout","text":"Idle Connection Timeout helps specify a time period, which ELB uses to close the connection if no data has been sent or received by the time that the idle timeout period elapses Both Classic ELB & ALB supports idle connection timeout NLB does not support idle connection timeout","title":"5-4 Idle Connection Timeout"},{"location":"chap3/7EAN_LB/#5-5-privatelink-support","text":"CLB and ALB does not support PrivateLink (TCP, TLS) Only NLB supports PrivateLink (TCP, TLS)","title":"5-5 PrivateLink Support"},{"location":"chap3/7EAN_LB/#5-6-zonal-isolation","text":"Only NLB supports Zonal Isolation which supports application architectures in a single zone. It automatically fails-over to other healthy AZs, if something fails in an AZ CLB and ALB does not support Zonal Isolation","title":"5-6 Zonal Isolation"},{"location":"chap3/7EAN_LB/#5-7-deletion-protection","text":"Only ALB & NLB supports Deletion Protection, wherein a load balancer can\u2019t be deleted if deletion protection is enabled CLB does not support deletion protection","title":"5-7 Deletion Protection"},{"location":"chap3/7EAN_LB/#5-8-preserve-source-ip-address","text":"NLB preserves the client-side source IP allowing the back-end to see the IP address of the client . Classic ELB and ALB do not preserve the client-side source IP. It needs to be retrieved using X-Forward-Header or proxy protocol","title":"5-8 Preserve source IP address"},{"location":"chap3/7EAN_LB/#5-9-health-checks","text":"All Load Balancer types support Health checks to determine if the instance is healthy or unhealthy ALB provides health check improvements that allow detailed error codes from 200-399 to be configured","title":"5-9 Health Checks"},{"location":"chap3/7EAN_LB/#5-10-supported-platforms","text":"Classic ELB supports both EC2-Classic and EC2-VPC ALB and NLB supports only EC2-VPC","title":"5-10 Supported Platforms"},{"location":"chap3/7EAN_LB/#5-11-websockets","text":"CLB does not support WebSockets Only ALB and NLB supports WebSockets","title":"5-11 WebSockets"},{"location":"chap3/7EAN_LB/#5-12-cross-zone-load-balancing","text":"By default, Load Balancer will evenly distribute requests evenly across its enabled AZs, irrespective of the instances it hosts. Cross-zone Load Balancing help distribute incoming requests evenly across all instances in its enabled AZs. All Load Balancer types support Cross-zone load balancing, however, for Classic it needs to be enabled while for ALB it is always enabled","title":"5-12 Cross-zone Load Balancing"},{"location":"chap3/7EAN_LB/#5-13-stick-sessions-cookies","text":"Stick Sessions (Session Affinity) enables the load balancer to bind a user\u2019s session to a specific instance, which ensures that all requests from the user during the session are sent to the same instance Classic ELB, ALB, and NLB supports sticky sessions to maintain session affinity NLB now supports sticky sessions","title":"5-13 Stick Sessions (Cookies)"},{"location":"chap3/7EAN_LB/#5-14-classic-load-balancer-vs-application-load-balancer-vs-network-load-balancer-security-features","text":"","title":"5-14 Classic Load Balancer vs Application Load Balancer vs Network Load Balancer Security Features"},{"location":"chap3/7EAN_LB/#5-14-ssl-terminationoffloading","text":"SSL Termination helps decrypt requests from clients before sending them to targets and hence reducing the load. SSL certificate must be installed on the load balancer. All load balancers types support SSL Termination","title":"5-14 SSL Termination/Offloading"},{"location":"chap3/7EAN_LB/#5-15-server-name-indication","text":"CLB only supports a single certificate and does not support SNI ALB and NLB support multiple certificates and uses SNI to serve multiple secure websites using a single TLS listener . If the hostname in the client matches multiple certificates, the load balancer selects the best certificate to use based on a smart selection algorithm.","title":"5-15 Server Name Indication"},{"location":"chap3/7EAN_LB/#6classic-load-balancer-vs-application-load-balancer-vs-network-load-balancer-operational-features","text":"","title":"6\u3001Classic Load Balancer vs Application Load Balancer vs Network Load Balancer Operational Features"},{"location":"chap3/7EAN_LB/#6-1-cloudwatch-metrics","text":"All Load Balancer types integrate with CloudWatch to provide metrics, with ALB providing additional metrics","title":"6-1 CloudWatch Metrics"},{"location":"chap3/7EAN_LB/#6-2-access-logs","text":"Access logs capture detailed information about requests sent to the load balancer. Each log contains information such as request received time, client\u2019s IP address, latencies, request paths, and server responses All Load Balancer types provide access logs, with ALB providing additional attributes","title":"6-2 Access Logs"},{"location":"chap3/7EAN_LB/#7aws-certification-exam-practice-questions","text":"A company wants to use load balancer for their application. However, the company wants to forward the requests without any header modification . What service should the company use? Classic Load Balancer Network Load Balancer Application Load Balancer Use Route 53 Solutions Architect is building an Amazon ECS-based web application that requires that headers are not modified when being forwarded to Amazon ECS. Which load balancer should the Architect use? Application Load Balancer Network Load Balancer A virtual load balancer appliance from AWS marketplace Classic Load Balancer An application tier currently hosts two web services on the same set of instances, listening on different ports. Which AWS service should a solutions architect use to route traffic to the service based on the incoming request? AWS Application Load Balancer Amazon CloudFront Amazon Route 53 AWS Classic Load Balancer A Solutions Architect needs to deploy an HTTP/HTTPS service on Amazon EC2 instances with support for WebSockets using load balancers. How can the Architect meet these requirements? Configure a Network Load balancer. Configure an Application Load Balancer. Configure a Classic Load Balancer. Configure a Layer-4 Load Balancer. A company is hosting an application in AWS for third party access. The third party needs to whitelist the application based on the IP. Which AWS service can the company use in the whitelisting of the IP address? AWS Application Load Balancer AWS Classic Load balancer AWS Network Load Balancer AWS Route 53","title":"7\u3001AWS Certification Exam Practice Questions"},{"location":"chap3/8elb_mon/","text":"L8 AWS ELB Monitoring 1 AWS ELB Monitoring Elastic Load Balancing publishes data points to CloudWatch about the load balancers and back-end instances Elastic Load Balancing reports metrics to CloudWatch only when requests are flowing through the load balancer. If there are requests flowing through the load balancer, Elastic Load Balancing measures and sends its metrics in 60-second intervals. If there are no requests flowing through the load balancer or no data for a metric, the metric is not reported. 2\u3001CloudWatch Metrics 2-1 HealthyHostCount, UnHealthyHostCount Number of healthy and unhealthy instances registered with the load balancer. Most useful statistics are average, min, and max 2-2 RequestCount Number of requests completed or connections made during the specified interval (1 or 5 minutes). Most useful statistic is sum 2-3 Latency Time elapsed, in seconds, after the request leaves the load balancer until the headers of the response are received . Most useful statistic is average 2-4 SurgeQueueLength Total number of requests that are pending routing. Load balancer queues a request if it is unable to establish a connection with a healthy instance in order to route the request. Maximum size of the queue is 1,024. Additional requests are rejected when the queue is full. Most useful statistic is max, because it represents the peak of queued requests. 2-5 SpilloverCount The total number of requests that were rejected because the surge queue is full. Should ideally be 0 Most useful statistic is sum. 2-6 HTTPCode_ELB_4XX , HTTPCode_ELB_5XX Client and Server error code generated by the load balancer Most useful statistic is sum. 2-7 HTTPCode_Backend_2XX, HTTPCode_Backend_3XX, HTTPCode_Backend_4XX, HTTPCode_Backend_5XX Number of HTTP response codes generated by registered instances Most useful statistic is sum. 2-8 Elastic Load Balancer Access Logs Elastic Load Balancing provides access logs that capture detailed information about all requests sent to your load balancer. Each log contains information such as the time the request was received, the client\u2019s IP address, latencies, request paths, and server responses . Elastic Load Balancing captures the logs and stores them in the Amazon S3 bucket Access logging is disabled by default and can be enabled without any additional charge. You are only charged for S3 storage ### 2-9 CloudTrail Logs \\ * AWS CloudTrail can be used to capture all calls to the Elastic Load Balancing API made by or on behalf of your AWS account and either made using Elastic Load Balancing API directly or indirectly through the AWS Management Console or AWS CLI * CloudTrail stores the information as log files in an Amazon S3 bucket that you specify. * Logs collected by CloudTrail can be used to monitor the activity of your load balancers and determine what API call was made, what source IP address was used, who made the call, when it was made, and so on 3 AWS Certification Exam Practice Questions An admin is planning to monitor the ELB. Which of the below mentioned services does not help the admin capture the monitoring information about the ELB activity ELB Access logs ELB health check CloudWatch metrics ELB API calls with CloudTrail A customer needs to capture all client connection information from their load balancer every five minutes. The company wants to use this data for analyzing traffic patterns and troubleshooting their applications. Which of the following options meets the customer requirements? Enable AWS CloudTrail for the load balancer. Enable access logs on the load balancer. Install the Amazon CloudWatch Logs agent on the load balancer. Enable Amazon CloudWatch metrics on the load balancer Your supervisor has requested a way to analyze traffic patterns for your application. You need to capture all connection information from your load balancer every 10 minutes. Pick a solution from below. Choose the correct answer: Enable access logs on the load balancer Create a custom metric CloudWatch filter on your load balancer Use a CloudWatch Logs Agent Use AWS CloudTrail with your load balancer","title":"L8 AWS ELB Monitoring"},{"location":"chap3/8elb_mon/#l8-aws-elb-monitoring","text":"","title":"L8 AWS ELB Monitoring"},{"location":"chap3/8elb_mon/#1-aws-elb-monitoring","text":"Elastic Load Balancing publishes data points to CloudWatch about the load balancers and back-end instances Elastic Load Balancing reports metrics to CloudWatch only when requests are flowing through the load balancer. If there are requests flowing through the load balancer, Elastic Load Balancing measures and sends its metrics in 60-second intervals. If there are no requests flowing through the load balancer or no data for a metric, the metric is not reported.","title":"1 AWS ELB Monitoring"},{"location":"chap3/8elb_mon/#2cloudwatch-metrics","text":"","title":"2\u3001CloudWatch Metrics"},{"location":"chap3/8elb_mon/#2-1-healthyhostcount-unhealthyhostcount","text":"Number of healthy and unhealthy instances registered with the load balancer. Most useful statistics are average, min, and max","title":"2-1 HealthyHostCount, UnHealthyHostCount"},{"location":"chap3/8elb_mon/#2-2-requestcount","text":"Number of requests completed or connections made during the specified interval (1 or 5 minutes). Most useful statistic is sum","title":"2-2 RequestCount"},{"location":"chap3/8elb_mon/#2-3-latency","text":"Time elapsed, in seconds, after the request leaves the load balancer until the headers of the response are received . Most useful statistic is average","title":"2-3 Latency"},{"location":"chap3/8elb_mon/#2-4-surgequeuelength","text":"Total number of requests that are pending routing. Load balancer queues a request if it is unable to establish a connection with a healthy instance in order to route the request. Maximum size of the queue is 1,024. Additional requests are rejected when the queue is full. Most useful statistic is max, because it represents the peak of queued requests.","title":"2-4 SurgeQueueLength"},{"location":"chap3/8elb_mon/#2-5-spillovercount","text":"The total number of requests that were rejected because the surge queue is full. Should ideally be 0 Most useful statistic is sum.","title":"2-5 SpilloverCount"},{"location":"chap3/8elb_mon/#2-6-httpcode_elb_4xxhttpcode_elb_5xx","text":"Client and Server error code generated by the load balancer Most useful statistic is sum.","title":"2-6 HTTPCode_ELB_4XX,HTTPCode_ELB_5XX"},{"location":"chap3/8elb_mon/#2-7-httpcode_backend_2xx-httpcode_backend_3xx-httpcode_backend_4xx-httpcode_backend_5xx","text":"Number of HTTP response codes generated by registered instances Most useful statistic is sum.","title":"2-7 HTTPCode_Backend_2XX, HTTPCode_Backend_3XX, HTTPCode_Backend_4XX, HTTPCode_Backend_5XX"},{"location":"chap3/8elb_mon/#2-8-elastic-load-balancer-access-logs","text":"Elastic Load Balancing provides access logs that capture detailed information about all requests sent to your load balancer. Each log contains information such as the time the request was received, the client\u2019s IP address, latencies, request paths, and server responses . Elastic Load Balancing captures the logs and stores them in the Amazon S3 bucket Access logging is disabled by default and can be enabled without any additional charge. You are only charged for S3 storage ### 2-9 CloudTrail Logs \\ * AWS CloudTrail can be used to capture all calls to the Elastic Load Balancing API made by or on behalf of your AWS account and either made using Elastic Load Balancing API directly or indirectly through the AWS Management Console or AWS CLI * CloudTrail stores the information as log files in an Amazon S3 bucket that you specify. * Logs collected by CloudTrail can be used to monitor the activity of your load balancers and determine what API call was made, what source IP address was used, who made the call, when it was made, and so on","title":"2-8 Elastic Load Balancer Access Logs"},{"location":"chap3/8elb_mon/#3-aws-certification-exam-practice-questions","text":"An admin is planning to monitor the ELB. Which of the below mentioned services does not help the admin capture the monitoring information about the ELB activity ELB Access logs ELB health check CloudWatch metrics ELB API calls with CloudTrail A customer needs to capture all client connection information from their load balancer every five minutes. The company wants to use this data for analyzing traffic patterns and troubleshooting their applications. Which of the following options meets the customer requirements? Enable AWS CloudTrail for the load balancer. Enable access logs on the load balancer. Install the Amazon CloudWatch Logs agent on the load balancer. Enable Amazon CloudWatch metrics on the load balancer Your supervisor has requested a way to analyze traffic patterns for your application. You need to capture all connection information from your load balancer every 10 minutes. Pick a solution from below. Choose the correct answer: Enable access logs on the load balancer Create a custom metric CloudWatch filter on your load balancer Use a CloudWatch Logs Agent Use AWS CloudTrail with your load balancer","title":"3 AWS Certification Exam Practice Questions"},{"location":"chap3/9route53/","text":"L9 AWS Certification \u2013 Route 53 Overview 1 Route 53 Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web service. Amazon Route 53 provides three main functions: 1-1 Domain registration allows you to register domain names 1-2 Domain Name System (DNS) service translates friendly domains names like www.example.com into IP addresses like 192.0.2.1 responds to DNS queries using a global network of authoritative DNS servers, which reduces latency can route Internet traffic to CloudFront, Elastic Beanstalk, ELB, or S3 . There\u2019s no charge for DNS queries to these resources 1-3 Health checking can monitor the health of resources such as web and email servers. sends automated requests over the Internet to the application to verify that it\u2019s reachable, available, and functional CloudWatch alarms can be configured for the health checks to send notification when a resource becomes unavailable. can be configured to route Internet traffic away from resources that are unavailable 2 Supported DNS Resource Record Types A (Address) Format is an IPv4 address in dotted decimal notation for e.g. 192.0.2.1 AAAA Format is an IPv6 address in colon-separated hexadecimal format CNAME Format is the same format as a domain name DNS protocol does not allow creation of a CNAME record for the top node of a DNS namespace, also known as the zone apex for e.g. the DNS name example.com registration, the zone apex is example.com, a CNAME record for example.com cannot be created, but CNAME records can be created for www.example.com, newproduct.example.com etc . If a CNAME record is created for a subdomain, any other resource record sets for that subdomain cannot be created for e.g. if a CNAME created for www.example.com, not other resource record sets for which the value of the Name field is www.example.com can be created MX (Mail Xchange) Format contains a decimal number that represents the priority of the MX record, and the domain name of an email server NS (Name Server) Format An NS record identifies the name servers for the hosted zone. The value for an NS record is the domain name of a name server. P TR Format A PTR record Value element is the same format as a domain name. SOA (Start of Authority) Format SOA record provides information about a domain and the corresponding Amazon Route 53 hosted zone SPF (Sender Policy Framework) Format SPF records were formerly used to verify the identity of the sender of email messages, however is not recommended Instead of an SPF record, a TXT record that contains the applicable value is recommended SRV Format An SRV record Value element consists of four space-separated values.The first three values are decimal numbers representing priority, weight, and port. The fourth value is a domain name for e.g. 10 5 80 hostname.example.com TXT (Text) Format A TXT record contains a space-separated list of double-quoted strings. A single string include a maximum of 255 characters. In addition to the characters that are permitted unescaped in domain names, space is allowed in TXT strings 3 Alias resource record sets Route 53 supports alias resource record sets, which enables routing of queries to a CloudFront distribution, Elastic Beanstalk, ELB, an S3 bucket configured as a static website, or another Route 53 resource record set Alias records are not standard for DNS RFC and are a Route 53 extension to DNS functionality Alias record is similar to a CNAME record, but can create an alias record both for the root domain or apex zone, such as example.com, and for subdomains, such as www.example.com. CNAME records can be used only for subdomains . Route 53 automatically recognizes changes in the resource record sets that the alias resource record set refers to for e.g. for a site pointing to an load balancer, if the ip of the load balancer changes, Route 53 will reflect those changes automatically in the DNS answers without any changes to the hosted zone that contains resource record sets If an alias resource record set points to a CloudFront distribution, a load balancer, or an S3 bucket, the time to live (TTL) can\u2019t be set; Route 53 uses the CloudFront, load balancer, or Amazon S3 TTLs. 4 Route 53 Hosted Zone Hosted Zone is A container for records, which include information about how to route traffic for a domain (such as example.com) and all of its subdomains (such as www.example.com, retail.example.com, and seattle.accounting.example.com). A hosted zone has the same name as the corresponding domain. Routing Traffic to the Resources Create a hosted zone with either a public hosted zone or a private hosted zone: Public Hosted Zone \u2013 for routing internet traffic to your resources for a specific domain and its subdomains Private hosted zone \u2013 for routing traffic within an VPC Create records in the hosted zone Records define where to route traffic for each domain name or subdomain name. name of each record in a hosted zone must end with the name of the hosted zone. 5 Route 53 Split-view (Split-horizon) DNS Route 53 Split-view (Split-horizon) DNS enables you to access an internal version of your website using the same domain name that is used publicly You can maintain both a private and public hosted zone with the same domain name for split-view DNS with Route 53 Ensure that DNS resolution and DNS hostnames are enabled on the source VPC. DNS queries will respond with answers based on the source of the request. From within the VPC, answers will come from the private hosted zone, while public queries will return answers from the public hosted zone. 6 AWS Certification Exam Practice Questions What does Amazon Route53 provide? * A global Content Delivery Network. None of these. A scalable Domain Name System An SSH endpoint for Amazon EC2. Does Amazon Route 53 support NS Records? Yes, it supports Name Service records. No It supports only MX records. Yes, it supports Name Server records . Does Route 53 support MX Records? Yes It supports CNAME records, but not MX records. No Only Primary MX records. Secondary MX records are not supported. Which of the following statements are true about Amazon Route 53 resource records? Choose 2 answers An Alias record can map one DNS name to another Amazon Route 53 DNS name. A CNAME record can be created for your zone apex. An Amazon Route 53 CNAME record can point to any DNS record hosted anywhere . TTL can be set for an Alias record in Amazon Route 53. An Amazon Route 53 Alias record can point to any DNS record hosted anywhere. Which statements are true about Amazon Route 53? (Choose 2 answers) Amazon Route 53 is a region-level service You can register your domain name Amazon Route 53 can perform health checks and failovers to a backup site in the even of the primary site failure Amazon Route 53 only supports Latency-based routing A customer is hosting their company website on a cluster of web servers that are behind a public-facing load balancer. The customer also uses Amazon Route 53 to manage their public DNS. How should the customer configure the DNS zone apex record to point to the load balancer? Create an A record pointing to the IP address of the load balancer Create a CNAME record pointing to the load balancer DNS name. Create a CNAME record aliased to the load balancer DNS name. Create an A record aliased to the load balancer DNS name A user has configured ELB with three instances. The user wants to achieve High Availability as well as redundancy with ELB. Which of the below mentioned AWS services helps the user achieve this for ELB? Route 53 AWS Mechanical Turk Auto Scaling AWS EMR How can the domain\u2019s zone apex for example \u201cmyzoneapexdomain com\u201d be pointed towards an Elastic Load Balancer? By using an AAAA record By using an A record By using an Amazon Route 53 CNAME record By using an Amazon Route 53 Alias record You need to create a simple, holistic check for your system\u2019s general availability and uptime. Your system presents itself as an HTTP-speaking API. What is the simplest tool on AWS to achieve this with? Route53 Health Checks CloudWatch Health Checks AWS ELB Health Checks EC2 Health Checks Your organization\u2019s corporate website must be available on www.acme.com and acme.com. How should you configure Amazon Route 53 to meet this requirement? Configure acme.com with an ALIAS record targeting the ELB. www.acme.com with an ALIAS record targeting the ELB . Configure acme.com with an A record targeting the ELB. www.acme.com with a CNAME record targeting the acme.com record. Configure acme.com with a CNAME record targeting the ELB. www.acme.com with a CNAME record targeting the acme.com record. Configure acme.com using a second ALIAS record with the ELB target. www.acme.com using a PTR record with the acme.com record target.","title":"L9 AWS Certification \u2013 Route 53 Overview"},{"location":"chap3/9route53/#l9-aws-certification-route-53-overview","text":"","title":"L9 AWS Certification \u2013 Route 53 Overview"},{"location":"chap3/9route53/#1-route-53","text":"Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web service. Amazon Route 53 provides three main functions:","title":"1 Route 53"},{"location":"chap3/9route53/#1-1-domain-registration","text":"allows you to register domain names","title":"1-1 Domain registration"},{"location":"chap3/9route53/#1-2-domain-name-system-dns-service","text":"translates friendly domains names like www.example.com into IP addresses like 192.0.2.1 responds to DNS queries using a global network of authoritative DNS servers, which reduces latency can route Internet traffic to CloudFront, Elastic Beanstalk, ELB, or S3 . There\u2019s no charge for DNS queries to these resources","title":"1-2 Domain Name System (DNS) service"},{"location":"chap3/9route53/#1-3-health-checking","text":"can monitor the health of resources such as web and email servers. sends automated requests over the Internet to the application to verify that it\u2019s reachable, available, and functional CloudWatch alarms can be configured for the health checks to send notification when a resource becomes unavailable. can be configured to route Internet traffic away from resources that are unavailable","title":"1-3 Health checking"},{"location":"chap3/9route53/#2-supported-dns-resource-record-types","text":"A (Address) Format is an IPv4 address in dotted decimal notation for e.g. 192.0.2.1 AAAA Format is an IPv6 address in colon-separated hexadecimal format CNAME Format is the same format as a domain name DNS protocol does not allow creation of a CNAME record for the top node of a DNS namespace, also known as the zone apex for e.g. the DNS name example.com registration, the zone apex is example.com, a CNAME record for example.com cannot be created, but CNAME records can be created for www.example.com, newproduct.example.com etc . If a CNAME record is created for a subdomain, any other resource record sets for that subdomain cannot be created for e.g. if a CNAME created for www.example.com, not other resource record sets for which the value of the Name field is www.example.com can be created MX (Mail Xchange) Format contains a decimal number that represents the priority of the MX record, and the domain name of an email server NS (Name Server) Format An NS record identifies the name servers for the hosted zone. The value for an NS record is the domain name of a name server. P TR Format A PTR record Value element is the same format as a domain name. SOA (Start of Authority) Format SOA record provides information about a domain and the corresponding Amazon Route 53 hosted zone SPF (Sender Policy Framework) Format SPF records were formerly used to verify the identity of the sender of email messages, however is not recommended Instead of an SPF record, a TXT record that contains the applicable value is recommended SRV Format An SRV record Value element consists of four space-separated values.The first three values are decimal numbers representing priority, weight, and port. The fourth value is a domain name for e.g. 10 5 80 hostname.example.com TXT (Text) Format A TXT record contains a space-separated list of double-quoted strings. A single string include a maximum of 255 characters. In addition to the characters that are permitted unescaped in domain names, space is allowed in TXT strings","title":"2 Supported DNS Resource Record Types"},{"location":"chap3/9route53/#3-alias-resource-record-sets","text":"Route 53 supports alias resource record sets, which enables routing of queries to a CloudFront distribution, Elastic Beanstalk, ELB, an S3 bucket configured as a static website, or another Route 53 resource record set Alias records are not standard for DNS RFC and are a Route 53 extension to DNS functionality Alias record is similar to a CNAME record, but can create an alias record both for the root domain or apex zone, such as example.com, and for subdomains, such as www.example.com. CNAME records can be used only for subdomains . Route 53 automatically recognizes changes in the resource record sets that the alias resource record set refers to for e.g. for a site pointing to an load balancer, if the ip of the load balancer changes, Route 53 will reflect those changes automatically in the DNS answers without any changes to the hosted zone that contains resource record sets If an alias resource record set points to a CloudFront distribution, a load balancer, or an S3 bucket, the time to live (TTL) can\u2019t be set; Route 53 uses the CloudFront, load balancer, or Amazon S3 TTLs.","title":"3 Alias resource record sets"},{"location":"chap3/9route53/#4-route-53-hosted-zone","text":"Hosted Zone is A container for records, which include information about how to route traffic for a domain (such as example.com) and all of its subdomains (such as www.example.com, retail.example.com, and seattle.accounting.example.com). A hosted zone has the same name as the corresponding domain. Routing Traffic to the Resources Create a hosted zone with either a public hosted zone or a private hosted zone: Public Hosted Zone \u2013 for routing internet traffic to your resources for a specific domain and its subdomains Private hosted zone \u2013 for routing traffic within an VPC Create records in the hosted zone Records define where to route traffic for each domain name or subdomain name. name of each record in a hosted zone must end with the name of the hosted zone.","title":"4 Route 53 Hosted Zone"},{"location":"chap3/9route53/#5-route-53-split-view-split-horizon-dns","text":"Route 53 Split-view (Split-horizon) DNS enables you to access an internal version of your website using the same domain name that is used publicly You can maintain both a private and public hosted zone with the same domain name for split-view DNS with Route 53 Ensure that DNS resolution and DNS hostnames are enabled on the source VPC. DNS queries will respond with answers based on the source of the request. From within the VPC, answers will come from the private hosted zone, while public queries will return answers from the public hosted zone.","title":"5 Route 53 Split-view (Split-horizon) DNS"},{"location":"chap3/9route53/#6-aws-certification-exam-practice-questions","text":"What does Amazon Route53 provide? * A global Content Delivery Network. None of these. A scalable Domain Name System An SSH endpoint for Amazon EC2. Does Amazon Route 53 support NS Records? Yes, it supports Name Service records. No It supports only MX records. Yes, it supports Name Server records . Does Route 53 support MX Records? Yes It supports CNAME records, but not MX records. No Only Primary MX records. Secondary MX records are not supported. Which of the following statements are true about Amazon Route 53 resource records? Choose 2 answers An Alias record can map one DNS name to another Amazon Route 53 DNS name. A CNAME record can be created for your zone apex. An Amazon Route 53 CNAME record can point to any DNS record hosted anywhere . TTL can be set for an Alias record in Amazon Route 53. An Amazon Route 53 Alias record can point to any DNS record hosted anywhere. Which statements are true about Amazon Route 53? (Choose 2 answers) Amazon Route 53 is a region-level service You can register your domain name Amazon Route 53 can perform health checks and failovers to a backup site in the even of the primary site failure Amazon Route 53 only supports Latency-based routing A customer is hosting their company website on a cluster of web servers that are behind a public-facing load balancer. The customer also uses Amazon Route 53 to manage their public DNS. How should the customer configure the DNS zone apex record to point to the load balancer? Create an A record pointing to the IP address of the load balancer Create a CNAME record pointing to the load balancer DNS name. Create a CNAME record aliased to the load balancer DNS name. Create an A record aliased to the load balancer DNS name A user has configured ELB with three instances. The user wants to achieve High Availability as well as redundancy with ELB. Which of the below mentioned AWS services helps the user achieve this for ELB? Route 53 AWS Mechanical Turk Auto Scaling AWS EMR How can the domain\u2019s zone apex for example \u201cmyzoneapexdomain com\u201d be pointed towards an Elastic Load Balancer? By using an AAAA record By using an A record By using an Amazon Route 53 CNAME record By using an Amazon Route 53 Alias record You need to create a simple, holistic check for your system\u2019s general availability and uptime. Your system presents itself as an HTTP-speaking API. What is the simplest tool on AWS to achieve this with? Route53 Health Checks CloudWatch Health Checks AWS ELB Health Checks EC2 Health Checks Your organization\u2019s corporate website must be available on www.acme.com and acme.com. How should you configure Amazon Route 53 to meet this requirement? Configure acme.com with an ALIAS record targeting the ELB. www.acme.com with an ALIAS record targeting the ELB . Configure acme.com with an A record targeting the ELB. www.acme.com with a CNAME record targeting the acme.com record. Configure acme.com with a CNAME record targeting the ELB. www.acme.com with a CNAME record targeting the acme.com record. Configure acme.com using a second ALIAS record with the ELB target. www.acme.com using a PTR record with the acme.com record target.","title":"6 AWS Certification Exam Practice Questions"},{"location":"chap4/1AWS_IAM/","text":"L1 AWS Identity Access Management \u2013 IAM 1 AWS IAM Overview AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources for your users. IAM is used to control Identity \u2013 who can use your AWS resources (authentication) Identity \u2013 who can use your AWS resources (authentication) IAM also enables access to resources across AWS accounts. 2 IAM Features Shared access to your AWS account Grant other people permission to administer and use resources in your AWS account without having to share your password or access key. Granular permissions Each user can be granted with different set granular permissions as required to perform their job Secure access to AWS resources for applications that run on EC2 IAM can help provide applications running on EC2 instance temporary credentials that they need in order to access other AWS resources Identity federation IAM allows users to access AWS resources, without requiring the user to have accounts with AWS, by providing temporary credentials for e.g. through corporate network or Google or Amazon authentication Identity information for assurance CloudTrail can be used to receive log records that include information about those who made requests for resources in the account. PCI DSS Compliance IAM supports the processing, storage, and transmission of credit card data by a merchant or service provider, and has been validated as being Payment Card Industry Data Security Standard (PCI DSS) complian t Integrated with many AWS services IAM integrates with almost all the AWS services Eventually Consistent IAM, like many other AWS services, is eventually consistent and achieves high availability by replicating data across multiple servers within Amazon\u2019s data centers around the world. Changes made to IAM would be eventually consistent and hence would take some time to reflect Free to use IAM is offered at no additional charge and charges are applied only for use of other AWS products by your IAM users. AWS Security Token Service IAM provide STS which is an included feature of the AWS account offered at no additional charge. AWS charges only for the use of other AWS services accessed by the AWS STS temporary security credentials. 3 Identities IAM identities determine who can access and help to provide authentication for people and processes in your AWS account 4 Account Root User Root Account Credentials are the email address and password with which you sign-in into the AWS account Root Credentials has full unrestricted access to AWS account including the account security credentials which include sensitive information IAM Best Practice \u2013 Do not use or share the Root account once the AWS account is created, instead create a separate user with admin privilege An Administrator account can be created for all the activities which too has full access to the AWS account except the accounts security credentials, billing information and ability to change password 5 IAM Users IAM user represents the person or service who uses the access to interact with AWS. IAM Best Practice \u2013 Create Individual Users User credentials can consist of the following Password to access AWS services through AWS Management Console Access Key/Secret Access Key to access AWS services through API, CLI or SDK IAM user starts with no permissions and is not authorized to perform any AWS actions on any AWS resources and should be granted permissions as per the job function requirement IAM Best Practice \u2013 Grant least Privilege Each IAM user is associated with one and only one AWS account. IAM User cannot be renamed from AWS management console and has to be done from CLI or SDK tools. IAM handles the renaming of user w.r.t unique id, groups, policies where the user was mentioned as a principal. However, you need to handle the renaming in the policies where the user was mentioned as a resource 6 IAM Groups IAM group is a collection of IAM users IAM groups can be used to specify permissions for a collection of users sharing the same job function making it easier to manage IAM Best Practice \u2013 Use groups to assign permissions to IAM Users A group is not truly an identity because it cannot be identified as a Principal in an access policy. It is only a way to attach policies to multiple users at one time A group can have multiple users, while a user can belong to multiple groups (10 max) Groups cannot be nested and can only have users within it AWS does not provide any default group to hold all users in it and if one is required it should be created with all users assigned to it. Renaming of a group name or path, IAM handles the renaming w.r.t to policies attached to the group, unique ids, users within the group. However, IAM does not update the policies where the group is mentioned as a resource and must be handled manually Deletion of the groups requires you to detach users and managed policies and delete any inline policies before deleting the group. With AWS management console, the deletion and detachment is taken care of. 7 IAM Roles 7-1 AWS IAM Role IAM role is very similar to a user, in that it is an identity with permission policies that determine what the identity can and cannot do in AWS. IAM role is not intended to be uniquely associated with a particular user, group or service and is intended to be assumable by anyone who needs it. Role does not have any credentials (password or access keys) associated with it and whoever assumes the role is provided with a dynamic temporary credentials Role helps in access delegation to grant permissions to someone that allows access to resources that you control Roles can help to prevent accidental access to or modification of sensitive resources Modification of a Role can be done anytime and the changes are reflected across all the entities associated with the Role immediately IAM Role plays a very important role in the following scenarios Services like EC2 instance running an application that needs to access other AWS services Allowing users from different AWS accounts have access to AWS resources in different account, instead of having to create users Company uses a Corporate Authentication mechanism and don\u2019t want the User to authenticate twice or create duplicate users in AWS Applications allowing login through external authentication mechanism e.g. Amazon, Facebook, Google etc Role can be assumed by IAM user within the same AWS account IAM user from a different AWS account AWS service such as EC2, EMR to interact with other services An external user authenticated by an external identity provider (IdP) service that is compatible with SAML 2.0 or OpenID Connect (OIDC), or a custom-built identity broker. Role involves defining two policies Trust policy Trust policy defines \u2013 who can assume the role Trust policy involves setting up a trust between the account that owns the resource (trusting account) and the account who owns the user that needs access to the resources (trusted account) Permissions policy Permissions policy defines \u2013 what they can access Permissions policy determines authorization, which grants the user of the role with the needed permissions to carry out the desired tasks on the resource Federation is creating a trust relationship between an external Identity Provider (IdP) and AWS Users can also sign in to an enterprise identity system that is compatible with SAML Users can sign in to a web identity provider, such as Login with Amazon, Facebook, Google, or any IdP that is compatible with OpenID connect (OIDC). When using OIDC and SAML 2.0 to configure a trust relationship between these external identity providers and AWS, the user is assigned to an IAM role and receives temporary credentials that enables the user to access AWS resources IAM Best Practice \u2013 Use roles for applications running on EC2 instances IAM Best Practice \u2013 Delegate using roles instead of sharing credentials 7-2 Role types AWS Security Token Service (STS) helps create and provide trusted users with temporary security credentials that control access to AWS resources STS is a global service with a single endpoint https://sts.amazonaws.com AWS STS API calls can be made either to a global endpoint or to one of the regional endpoints. Regional endpoint can help reduce latency and improve the performance of the API calls Temporary Credentials are similar to Long Term Credentials except for are short term and are regularly rotated can be configured to last from few minutes to several hours do not have to be embedded or distributed are not stored or attached with the User, but are generated dynamically and provided to the user as and when requested 7-3 Role types AWS Service Roles Some AWS services need to interact with other AWS services for e.g. EC2 interacting with S3, SQS etc Best practice is to assign these services with IAM roles instead of embedding or passing IAM user credentials directly into an instance, because distributing and rotating long-term credentials to multiple instances is challenging to manage and a potential security risk. AWS automatically provides temporary security credentials for these services e.g. Amazon EC2 instance to use on behalf of its applications Deleting a role or instance profile that is associated with a running EC2 instance will break any applications running on the instance Complete Process Flow Create a IAM role with services who would use it for e.g. EC2 as trusted entity and define permission policies with the access the service needs Associated a Role (actually an Instance profile) with the EC2 service when the instance is launched Temporary security credentials are available on the instance and are automatically rotated before they expire so that a valid set is always available Application can retrieve the temporary credentials either using the Instance metadata directly or through AWS SDK Applications running on the EC2 instance can now use the permissions defined in the Role to access other AWS resources Application, if caching the credentials, needs to make sure it uses the correct credentials before they expire Instance Profile An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. If a Role is created for EC2 instance or any other service that uses EC2 through AWS Management Console, AWS creates a Instance profile automatically with the same name as the Role. However, if the Role is created through CLI the instance profile needs to created as well An instance profile can contain only one IAM role. However, a role can be included in multiple instance profiles. 7-4 Cross-Account access Roles IAM users can be granted permission to switch roles within the same AWS account or to roles defined in other AWS accounts that you own. Roles can also be used to delegate permissions to IAM users from AWS accounts owned by Third parties You must explicitly grant the users permission to assume the role. Users must actively switch to the role using the AWS Management Console. Multi-factor authentication (MFA) protection can be enabled for the role so that only users who sign in with an MFA device can assume the role However, only One set of permissions are applicable at a time. User who assumes a role temporarily gives up his or her own permissions and instead takes on the permissions of the role. When the user exits, or stops using the role, the original user permissions are restored. 7-5 Complete Process Flow Trusting account creates a IAM Role with a Trust policy which defines the account (trusted account) as a principal who can access the resources and a Permissions policy to define what resources can the user in the trusted account access Trusting account provides the Account ID and the Role name (or the ARN) to the trusted account If the Trusting account is own by Third Party it can optionally provide an External ID (recommended for additional security), required to uniquely identify the trusted account, which can be added to the trust policy as a condition Trusted account creates a IAM user who has permissions (Permission to call the AWS Security Token Service (AWS STS) AssumeRole API for the role) to assume the role/switch to the role. IAM User in the Trusted account switches to the Role/assumes the role and passes the ARN of the role Trusted account belonging to the Third party would also pass the External ID mapped to the Trusting account AWS STS verifies the request for the role ARN, External ID if any and if it is from the trusted resource matching the roles\u2019s trust policy and AWS STS upon successful verification returns temporary credentials Temporary credentials allow the user to access the resources of the Trusting account When the user exits the role, the user\u2019s permissions revert to the original permissions held before switching to the role 7-6 External ID and Confused Deputy Problem External ID allows the user that is assuming the role to assert the circumstances in which they are operating. External ID provides a way for the account owner to permit the role to be assumed only under specific circumstances and prevents an unauthorized customer from gaining access to your resources Primary function of the external ID is to address and prevent the \u201cconfused deputy\u201d problem. Confused Deputy Problem Example Corp\u2019s AWS Account provides the services (access, analyze and process data and provide back reports) to multiple different AWS accounts Preferred mechanism is to have each AWS account customer define a Role which Example Corp\u2019s AWS Account users can assume and act upon You provide Example Corp\u2019s AWS Account access to your AWS account through Role and providing Role ARN Example Corp when working on your account assumes the IAM role and provides the ARN with the request As Example Corp is already trusted by your account it will received the temporary security credentials and gain access to your resources If an other AWS account is able to know or guess your ARN (Role with Account ID), it can provide the same to Example Corp Example Corp\u2019s would use the ARN (belonging to your AWS account) to process the data but would provide the same data to the other AWS account This form of privilege escalation is known as the confused deputy problem 7-7 Address Confused Deputy Problem using External ID Using External ID, Example Corp\u2019s generates a unique External ID for each of its Customer which is known only to them and is kept secret Example Corp provides you an External ID which needs to added as a condition while defining the trust policy You provide Example Corp\u2019s AWS Account access to your AWS account through Role and providing Role ARN Example Corp when working on your account uses the IAM role and provides the ARN along with the External ID and as it is already trusted would be able to gain access Other AWS account registered with Example Corp would have a Unique External ID assigned to it If the Other AWS account is able to know or guess your ARN (Role with Account ID), it can provide the same to Example Corp Example Corp\u2019s would request access to your Account using the ARN (belonging to your AWS account) but with the External ID belonging to Other AWS account as the request was made on its behalf As the External ID provided by Example Corp does not match the condition defined in the Role trust policy, the authentication would fail and hence denied access 7-8 AWS Certification Exam for IAM Role A company is building software on AWS that requires access to various AWS services. Which configuration should be used to ensure that AWS credentials (i.e., Access Key ID/Secret Access Key combination) are not compromised? Enable Multi-Factor Authentication for your AWS root account. Assign an IAM role to the Amazon EC2 instance. Store the AWS Access Key ID/Secret Access Key combination in software comments. Assign an IAM user to the Amazon EC2 Instance. A company is preparing to give AWS Management Console access to developers. Company policy mandates identity federation and role-based access control. Roles are currently assigned using groups in the corporate Active Directory. What combination of the following will give developers access to the AWS console? (Select 2) Choose 2 answers AWS Directory Service AD Connector AWS Directory Service Simple AD AWS Identity and Access Management groups AWS identity and Access Management roles AWS identity and Access Management users A customer needs corporate IT governance and cost oversight of all AWS resources consumed by its divisions. The divisions want to maintain administrative control of the discrete AWS resources they consume and keep those resources separate from the resources of other divisions. Which of the following options, when used together will support the autonomy/control of divisions while enabling corporate IT to maintain governance and cost oversight? Choose 2 answers Use AWS Consolidated Billing and disable AWS root account access for the child accounts. Enable IAM cross-account access for all corporate IT administrators in each child account. (Provides IT governance) Create separate VPCs for each division within the corporate IT AWS account. Use AWS Consolidated Billing to link the divisions\u2019 accounts to a parent corporate account. (Will provide cost oversight) Write all child AWS CloudTrail and Amazon CloudWatch logs to each child account\u2019s Amazon S3 \u2018Log\u2019 bucket. Which of the following items are required to allow an application deployed on an EC2 instance to write data to a DynamoDB table? Assume that no security keys are allowed to be stored on the EC2 instance. (Choose 2 answers) Create an IAM Role that allows write access to the DynamoDB table Add an IAM Role to a running EC2 instance. (With latest enhancement from AWS, IAM role can be assigned to a running EC2 instance) Create an IAM User that allows write access to the DynamoDB table. Add an IAM User to a running EC2 instance. Launch an EC2 Instance with the IAM Role included in the launch configuration (This was the correct answer before, as AWS did not allow IAM role to be added to an existing instance) You are looking to migrate your Development (Dev) and Test environments to AWS. You have decided to use separate AWS accounts to host each environment. You plan to link each accounts bill to a Master AWS account using Consolidated Billing. To make sure you Keep within budget you would like to implement a way for administrators in the Master account to have access to stop, delete and/or terminate resources in both the Dev and Test accounts. Identify which option will allow you to achieve this goal. [PROFESSIONAL] Create IAM users in the Master account with full Admin permissions. Create cross-account roles in the Dev and Test accounts that grant the Master account access to the resources in the account by inheriting permissions from the Master account. Create IAM users and a cross-account role in the Master account that grants full Admin permissions to the Dev and Test accounts. Create IAM users in the Master account Create cross-account roles in the Dev and Test accounts that have full Admin permissions and grant the Master account access Link the accounts using Consolidated Billing. This will give IAM users in the Master account access to resources in the Dev and Test accounts You have an application running on an EC2 Instance which will allow users to download flies from a private S3 bucket using a pre-assigned URL. Before generating the URL the application should verify the existence of the file in S3. How should the application use AWS credentials to access the S3 bucket securely? [PROFESSIONAL] Use the AWS account access Keys the application retrieves the credentials from the source code of the application. Create a IAM user for the application with permissions that allow list access to the S3 bucket launch the instance as the IAM user and retrieve the IAM user\u2019s credentials from the EC2 instance user data. Create an IAM role for EC2 that allows list access to objects in the S3 bucket. Launch the instance with the role, and retrieve the role\u2019s credentials from the EC2 Instance metadata Create an IAM user for the application with permissions that allow list access to the S3 bucket. The application retrieves the IAM user credentials from a temporary directory with permissions that allow read access only to the application user. An administrator is using Amazon CloudFormation to deploy a three tier web application that consists of a web tier and application tier that will utilize Amazon DynamoDB for storage when creating the CloudFormation template which of the following would allow the application instance access to the DynamoDB tables without exposing API credentials? [PROFESSIONAL] Create an Identity and Access Management Role that has the required permissions to read and write from the required DynamoDB table and associate the Role to the application instances by referencing an instance profile. Use the Parameter section in the Cloud Formation template to nave the user input Access and Secret Keys from an already created IAM user that has me permissions required to read and write from the required DynamoDB table. Create an Identity and Access Management Role that has the required permissions to read and write from the required DynamoDB table and reference the Role in the instance profile property of the application instance . Create an identity and Access Management user in the CloudFormation template that has permissions to read and write from the required DynamoDB table, use the GetAtt function to retrieve the Access and secret keys and pass them to the application instance through user-data. An enterprise wants to use a third-party SaaS application. The SaaS application needs to have access to issue several API commands to discover Amazon EC2 resources running within the enterprise\u2019s account. The enterprise has internal security policies that require any outside access to their environment must conform to the principles of least privilege and there must be controls in place to ensure that the credentials used by the SaaS vendor cannot be used by any other third party. Which of the following would meet all of these conditions? [PROFESSIONAL] From the AWS Management Console, navigate to the Security Credentials page and retrieve the access and secret key for your account. Create an IAM user within the enterprise account assign a user policy to the IAM user that allows only the actions required by the SaaS application create a new access and secret key for the user and provide these credentials to the SaaS provider. Create an IAM role for cross-account access allows the SaaS provider\u2019s account to assume the role and assign it a policy that allows only the actions required by the SaaS application. Create an IAM role for EC2 instances, assign it a policy mat allows only the actions required tor the SaaS application to work, provide the role ARM to the SaaS provider to use when launching their application instances. A user has created an application which will be hosted on EC2. The application makes calls to DynamoDB to fetch certain data. The application is using the DynamoDB SDK to connect with from the EC2 instance. Which of the below mentioned statements is true with respect to the best practice for security in this scenario? The user should attach an IAM role with DynamoDB access to the EC2 instance The user should create an IAM user with DynamoDB access and use its credentials within the application to connect with DynamoDB The user should create an IAM role, which has EC2 access so that it will allow deploying the application The user should create an IAM user with DynamoDB and EC2 access. Attach the user with the application so that it does not use the root account credentials A customer is in the process of deploying multiple applications to AWS that are owned and operated by different development teams. Each development team maintains the authorization of its users independently from other teams. The customer\u2019s information security team would like to be able to delegate user authorization to the individual development teams but independently apply restrictions to the users permissions based on factors such as the users device and location. For example, the information security team would like to grant read-only permissions to a user who is defined by the development team as read/write whenever the user is authenticating from outside the corporate network. What steps can the information security team take to implement this capability? [PROFESSIONAL] Operate an authentication service that generates AWS STS tokens with IAM policies from application-defined IAM roles. (no user separation, will just help generate temporary tokens) Add additional IAM policies to the application IAM roles that deny user privileges based on information security policy. (Different policy with deny rules based on location, device and more restrictive wins) Configure IAM policies that restrict modification of the application IAM roles only to the information security team. (Authorization should still be in developers control) Enable federation with the internal LDAP directory and grant the application teams permissions to modify users. You are creating an Auto Scaling group whose Instances need to insert a custom metric into CloudWatch. Which method would be the best way to authenticate your CloudWatch PUT request? Create an IAM role with the Put MetricData permission and modify the Auto Scaling launch configuration to launch instances in that role Create an IAM user with the PutMetricData permission and modify the Auto Scaling launch configuration to inject the users credentials into the instance User Data Modify the appropriate Cloud Watch metric policies to allow the Put MetricData permission to instances from the Auto Scaling group Create an IAM user with the PutMetricData permission and put the credentials in a private repository and have applications on the server pull the credentials as needed 7 MultiFactor Authentication (MFA) For increased security and to help protect the AWS resources, Multi-Factor authentication can be configured IAM Best Practice \u2013 Enable MFA on Root accounts and privilege users Multi-Factor Authentication can be configured using Security token-based AWS Root user or IAM user can be assigned a hardware/virtual MFA device Device generates a six digit numeric code based upon a time-synchronized one-time password algorithm which needs to be provided during authentication SMS text message-based (Preview Mode) IAM user can be configured with the phone number of the user\u2019s SMS-compatible mobile device which would receive a 6 digit code from AWS SMS-based MFA is available only for IAM users and does not work for AWS root account MFA needs to enabled on the Root user and IAM user separately as they are distinct entities. Enabling MFA on Root does not enable it for all other users MFA device can be associated with only one AWS account or IAM user and vice versa If the MFA device stops working or is lost, you won\u2019t be able to login into the AWS console and would need to reach out to AWS support to deactivate MFA MFA protection can be enabled for service api\u2019s calls using \u201cCondition\u201d: {\u201cBool\u201d: {\u201caws:MultiFactorAuthPresent\u201d: \u201ctrue\u201d}} and is available only if the service supports temporary security credentials. 8 AWS IAM Access Management IAM Access Management is all about Permissions and Policies Permission allows you to define who has access and what actions can they perform IAM Policy helps to fine tune the permissions granted to the policy owner IAM Policy is a document that formally states one or more permissions. Most restrictive Policy always wins IAM Policy is defined in the JSON (JavaScript Object Notation) format IAM policy basically states \u201c Principal A is allowed or denied (effect) to perform Action B on Resource C given Conditions D are satisfied\u201d { \u201cVersion\u201d: \u201c2012-10-17\u201d, \u201cStatement\u201d: { \u201cPrincipal\u201c: {\u201cAWS\u201d: [\u201carn:aws:iam::ACCOUNT-ID-WITHOUT-HYPHENS:root\u201d]}, \u201cAction\u201c: \u201cs3:ListBucket\u201d, \u201cEffect\u201c: \u201cAllow\u201d, \u201cResource\u201c: \u201carn:aws:s3:::example_bucket\u201d, \u201cCondition\u201c: {\u201cStringLike\u201d: { \u201cs3:prefix\u201d: [ \u201chome/${aws:username}/\u201d ] } } } } An Entity can be associated with Multiple Policies and a Policy can have multiple statements where each statement in a policy refers to a single permission. If your policy includes multiple statements, a logical OR is applied across the statements at evaluation time. Similarly, if multiple policies are applicable to a request, a logical OR is applied across the policies at evaluation time. Principal can either be specified within the Policy for resource Based policies while for user bases policies the principal is the user, group or role to which the policy is attached 8-1 Identity-Based vs Resource-Based Permissions Identity-based, or IAM permissions Identity-based, or IAM permissions are attached to an IAM user, group, or role and specify what the user, group or role can do User, group, or role itself acts as a Principal IAM permissions can be applied to almost all the AWS services IAM Policies can either be inline or managed IAM Policy version has to be 2012-10-17 Resource-based permissions Resource-based permissions are attached to a resource for e.g. S3, SNS Resource-based permissions specifies both who has access to the resource (Principal) and what actions they can perform on it (Actions) Resource-based policies are inline only, not managed. Resource-based permissions are supported only by some AWS services Resource-based policies are always attached inline policy and are not managed Resource-based policies can be defined with version 2012-10-17 or 2008-10-17 8-2 Managed Policies and Inline Policies Managed policies Managed policies are Standalone policies that can be attached to multiple users, groups, and roles in an AWS account. Managed policies apply only to identities (users, groups, and roles) but not to resources. Managed policies allow reusability Managed policy changes are implemented as versions (limited to 5), an new change to the existing policy creates a new version which is useful to compare the changes and revert back, if needed Managed policies have their own ARN Two types of managed policies: AWS managed policies Managed policies that are created and managed by AWS. AWS maintains and can upgrades these policies for e.g. if a new service is introduced, the changes automatically effects all the existing principals attached to the policy AWS takes care of not breaking the policies for e.g. adding an restriction of removal of permission Managed policies cannot be modified Customer managed policies Managed policies are standalone and custom policies created and administered by you. Customer managed policies allows more precise control over the policies than when using AWS managed policies. Inline policies Inline policies are created and managed by you, and are embedded directly into a single user, group, or role. Deletion of the Entity (User, Group or Role) or Resource deletes the In-Line policy as well 8-3 IAM Policy Simulator IAM Policy Simulator helps test and troubleshoot IAM and resource-based policies IAM Policy Simulator can help test the following ways :- Test IAM based policies. If multiple policies attached, you can test all the policies, or select individual policies to test. You can test which actions are allowed or denied by the selected policies for specific resources. Test Resource based policies. However, Resource based policies cannot be tested standalone and have to be attached with the Resource Test new IAM policies that are not yet attached to a user, group, or role by typing or copying them into the simulator. These are used only in the simulation and are not saved. Test the policies with selected services, actions, and resources Simulate real-world scenarios by providing context keys, such as an IP address or date, that are included in Condition elements in the policies being tested. Identify which specific statement in a policy results in allowing or denying access to a particular resource or action. IAM Policy Simulator does not make an actual AWS service request and hence does not make unwanted changes to the AWS live environment IAM Policy Simulator just reports the result Allowed or Denied IAM Policy Simulator allows to you modify the policy and test. These changes are not propogated to the actual policies attached to the entities Introductory Video for Policy Simulator 8-4 IAM Policy Evaluation When determining if permission is allowed, the hierarchy is followed Decision allows starts with Deny IAM combines and evaluates all the policies Explicit Deny First IAM checks for an explicit denial policy. Explicit Deny overrides everything and if something is explicitly deined it can never be allowed Explicit Allow If one does not exist, it then checks for an explicit allow policy. For granting User any permission, the permission must be explicitly allowed Implicit Deny If neither an explicit deny or explicit allow policy exist, it reverts to the default: implicit deny. All permissions are implicity denied by default 8-5 IAM Policy Variables Policy variables provide a feature to specify placeholders in a policy. When the policy is evaluated, the policy variables are replaced with values that come from the request itself Policy variables allow a single policy to be applied to a group of users to control access for e.g. all user having access to S3 bucket folder with their name only Policy variable is marked using a $ prefix followed by a pair of curly braces ({ }) . Inside the ${ } characters, with the name of the value from the request that you want to use in the policy Policy variables work only with policies defined with Version 2012-10-17 Policy variables can only be used in the Resource element and in string comparisons in the Condition element Policy variables are case sensitive and include variables like aws:username, aws:userid, aws:SourceIp, aws:CurrentTime etc. 8-6 IAM Policy Variables IAM\u2019s Policy Evaluation Logic always starts with a default _ _____ for every request, except for those that use the AWS account\u2019s root security credentials b Permit Deny Cancel An organization has created 10 IAM users. The organization wants each of the IAM users to have access to a separate DynamoDB table. All the users are added to the same group and the organization wants to setup a group level policy for this. How can the organization achieve this? Define the group policy and add a condition which allows the access based on the IAM name Create a DynamoDB table with the same name as the IAM user name and define the policy rule which grants access based on the DynamoDB ARN using a variable Create a separate DynamoDB database for each user and configure a policy in the group based on the DB variable It is not possible to have a group level policy which allows different IAM users to different DynamoDB Tables An organization has setup multiple IAM users. The organization wants that each IAM user accesses the IAM console only within the organization and not from outside. How can it achieve this? Create an IAM policy with the security group and use that security group for AWS console login Create an IAM policy with a condition which denies access when the IP address range is not from the organization Configure the EC2 instance security group which allows traffic only from the organization\u2019s IP range Create an IAM policy with VPC and allow a secure gateway between the organization and AWS Console Can I attach more than one policy to a particular entity? Yes always Only if within GovCloud No Only if within VPC A _ ___ is a document that provides a formal statement of one or more permissions. policy permission Role resource A _ ___ is the concept of allowing (or disallowing) an entity such as a user, group, or role some type of access to one or more resources. user AWS Account resource permission True or False: When using IAM to control access to your RDS resources, the key names that can be used are case sensitive. For example, aws:CurrentTime is NOT equivalent to AWS:currenttime. TRUE FALSE A user has set an IAM policy where it allows all requests if a request from IP 10.10.10.1/32. Another policy allows all the requests between 5 PM to 7 PM. What will happen when a user is requesting access from IP 10.10.10.1/32 at 6 PM? IAM will throw an error for policy conflict It is not possible to set a policy based on the time or IP It will deny access It will allow access Which of the following are correct statements with policy evaluation logic in AWS Identity and Access Management? Choose 2 answers. By default, all requests are denied An explicit allow overrides an explicit deny An explicit allow overrides default deny An explicit deny does not override an explicit allow By default, all request are allowed A web design company currently runs several FTP servers that their 250 customers use to upload and download large graphic files. They wish to move this system to AWS to make it more scalable, but they wish to maintain customer privacy and keep costs to a minimum. What AWS architecture would you recommend? [PROFESSIONAL] Ask their customers to use an S3 client instead of an FTP client. Create a single S3 bucket. Create an IAM user for each customer. Put the IAM Users in a Group that has an IAM policy that permits access to subdirectories within the bucket via use of the \u2018username\u2019 Policy variable . Create a single S3 bucket with Reduced Redundancy Storage turned on and ask their customers to use an S3 client instead of an FTP client. Create a bucket for each customer with a Bucket Policy that permits access only to that one customer. (Creating bucket for each user is not a scalable model, also 100 buckets are a limit earlier without extending which has since changed link) Create an auto-scaling group of FTP servers with a scaling policy to automatically scale-in when minimum network traffic on the auto-scaling group is below a given threshold. Load a central list of ftp users from S3 as part of the user Data startup script on each Instance (Expensive) Create a single S3 bucket with Requester Pays turned on and ask their customers to use an S3 client instead of an FTP client. Create a bucket tor each customer with a Bucket Policy that permits access only to that one customer. (Creating bucket for each user is not a scalable model, also 100 buckets are a limit earlier without extending which has since changed link) 8 Credential Report IAM allows you to generate and download a credential report that lists all users in the account and the status of their various credentials, including passwords, access keys, and MFA devices. Credential report can be used to assist in auditing and compliance efforts Credential report can be used to audit the effects of credential lifecycle requirements, such as password and access key rotation. IAM Best Practice \u2013 Perform Audits and Remove all unused users and credentials Credential report is generated as often as once every four hours. If the existing report was generated less than four hours, the same is available for download. If more then four hours, IAM generates and downloads a new report. 9 AWS IAM AWS Certification Which service enables AWS customers to manage users and permissions in AWS? AWS Access Control Service (ACS) AWS Identity and Access Management (IAM) AWS Identity Manager (AIM) IAM provides several policy templates you can use to automatically assign permissions to the groups you create. The _____ policy template gives the Admins group permission to access all account resources, except your AWS account information Read Only Access Power User Access AWS Cloud Formation Read Only Access Administrator Access Every user you create in the IAM system starts with _ __. Partial permissions Full permissions No permissions Groups can\u2019t _____. be nested more than 3 levels be nested at all be nested more than 4 levels be nested more than 2 levels The _____ service is targeted at organizations with multiple users or systems that use AWS products such as Amazon EC2, Amazon SimpleDB, and the AWS Management Console. Amazon RDS AWS Integrity Management AWS Identity and Access Management Amazon EMR An AWS customer is deploying an application that is composed of an AutoScaling group of EC2 Instances. The customers security policy requires that every outbound connection from these instances to any other service within the customers Virtual Private Cloud must be authenticated using a unique x.509 certificate that contains the specific instanceid. In addition an x.509 certificates must be designed by the customer\u2019s Key management service in order to be trusted for authentication. Which of the following configurations will support these requirements? Configure an IAM Role that grants access to an Amazon S3 object containing a signed certificate and configure the Auto Scaling group to launch instances with this role. Have the instances bootstrap get the certificate from Amazon S3 upon first boot. Embed a certificate into the Amazon Machine Image that is used by the Auto Scaling group. Have the launched instances generate a certificate signature request with the instance\u2019s assigned instance-id to the Key management service for signature. Configure the Auto Scaling group to send an SNS notification of the launch of a new instance to the trusted key management service. Have the Key management service generate a signed certificate and send it directly to the newly launched instance. Configure the launched instances to generate a new certificate upon first boot. Have the Key management service poll the AutoScaling group for associated instances and send new instances a certificate signature that contains the specific instance-id. When assessing an organization AWS use of AWS API access credentials which of the following three credentials should be evaluated? Choose 3 answers Key pairs Console passwords Access keys Signing certificates Security Group memberships (required for EC2 instance access) An organization has created 50 IAM users. The organization wants that each user can change their password but cannot change their access keys. How can the organization achieve this? The organization has to create a special password policy and attach it to each user The root account owner has to use CLI which forces each IAM user to change their password on first login By default each IAM user can modify their passwords Root account owner can set the policy from the IAM console under the password policy screen An organization has created 50 IAM users. The organization has introduced a new policy which will change the access of an IAM user. How can the organization implement this effectively so that there is no need to apply the policy at the individual user level? Use the IAM groups and add users as per their role to different groups and apply policy to group The user can create a policy and apply it to multiple users in a single go with the AWS CLI Add each user to the IAM role as per their organization role to achieve effective policy setup Use the IAM role and implement access at the role level Your organization\u2019s security policy requires that all privileged users either use frequently rotated passwords or one-time access credentials in addition to username/password. Which two of the following options would allow an organization to enforce this policy for AWS users? Choose 2 answers Configure multi-factor authentication for privileged IAM users Create IAM users for privileged accounts (can set password policy) Implement identity federation between your organization\u2019s Identity provider leveraging the IAM Security Token Service Enable the IAM single-use password policy option for privileged users (no such option the password expiration can be set from 1 to 1095 days) Your organization is preparing for a security assessment of your use of AWS. In preparation for this assessment, which two IAM best practices should you consider implementing? Choose 2 answers Create individual IAM users for everyone in your organization Configure MFA on the root account and for privileged IAM users Assign IAM users and groups configured with policies granting least privilege access Ensure all users have been assigned and are frequently rotating a password, access ID/secret key, and X.509 certificate A company needs to deploy services to an AWS region which they have not previously used. The company currently has an AWS identity and Access Management (IAM) role for the Amazon EC2 instances, which permits the instance to have access to Amazon DynamoDB. The company wants their EC2 instances in the new region to have the same privileges. How should the company achieve this? Create a new IAM role and associated policies within the new region Assign the existing IAM role to the Amazon EC2 instances in the new region Copy the IAM role and associated policies to the new region and attach it to the instances Create an Amazon Machine Image (AMI) of the instance and copy it to the desired region using the AMI Copy feature After creating a new IAM user which of the following must be done before they can successfully make API calls? Add a password to the user. Enable Multi-Factor Authentication for the user. Assign a Password Policy to the user. Create a set of Access Keys for the user An organization is planning to create a user with IAM. They are trying to understand the limitations of IAM so that they can plan accordingly. Which of the below mentioned statements is not true with respect to the limitations of IAM? One IAM user can be a part of a maximum of 5 groups Organization can create 100 groups per AWS account One AWS account can have a maximum of 5000 IAM users One AWS account can have 250 roles Within the IAM service a GROUP is regarded as a: A collection of AWS accounts It\u2019s the group of EC2 machines that gain the permissions specified in the GROUP. There\u2019s no GROUP in IAM, but only USERS and RESOURCES. A collection of users . Is there a limit to the number of groups you can have? Yes for all users except root No Yes unless special permission granted Yes for all users What is the default maximum number of MFA devices in use per AWS account (at the root account level)? 1 5 15 10 When you use the AWS Management Console to delete an IAM user, IAM also deletes any signing certificates and any access keys belonging to the user. FALSE This is configurable TRUE You are setting up a blog on AWS. In which of the following scenarios will you need AWS credentials? (Choose 3) Sign in to the AWS management console to launch an Amazon EC2 instance Sign in to the running instance to instance some software (needs ssh keys) Launch an Amazon RDS instance Log into your blog\u2019s content management system to write a blog post (need to authenticate using blog authentication) Post pictures to your blog on Amazon S3 An organization has 500 employees. The organization wants to set up AWS access for each department. Which of the below mentioned options is a possible solution? Create IAM roles based on the permission and assign users to each role Create IAM users and provide individual permission to each Create IAM groups based on the permission and assign IAM users to the groups It is not possible to manage more than 100 IAM users with AWS An organization has hosted an application on the EC2 instances. There will be multiple users connecting to the instance for setup and configuration of application. The organization is planning to implement certain security best practices. Which of the below mentioned pointers will not help the organization achieve better security arrangement? Apply the latest patch of OS and always keep it updated. Allow only IAM users to connect with the EC2 instances with their own secret access key . Disable the password-based login for all the users. All the users should use their own keys to connect with the instance securely. Create a procedure to revoke the access rights of the individual user when they are not required to connect to EC2 instance anymore for the purpose of application configuration.","title":"L1 AWS Identity Access Management \u2013 IAM(unfinished)"},{"location":"chap4/1AWS_IAM/#l1-aws-identity-access-management-iam","text":"","title":"L1 AWS Identity Access Management \u2013 IAM"},{"location":"chap4/1AWS_IAM/#1-aws-iam-overview","text":"AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources for your users. IAM is used to control Identity \u2013 who can use your AWS resources (authentication) Identity \u2013 who can use your AWS resources (authentication) IAM also enables access to resources across AWS accounts.","title":"1 AWS IAM Overview"},{"location":"chap4/1AWS_IAM/#2-iam-features","text":"Shared access to your AWS account Grant other people permission to administer and use resources in your AWS account without having to share your password or access key. Granular permissions Each user can be granted with different set granular permissions as required to perform their job Secure access to AWS resources for applications that run on EC2 IAM can help provide applications running on EC2 instance temporary credentials that they need in order to access other AWS resources Identity federation IAM allows users to access AWS resources, without requiring the user to have accounts with AWS, by providing temporary credentials for e.g. through corporate network or Google or Amazon authentication Identity information for assurance CloudTrail can be used to receive log records that include information about those who made requests for resources in the account. PCI DSS Compliance IAM supports the processing, storage, and transmission of credit card data by a merchant or service provider, and has been validated as being Payment Card Industry Data Security Standard (PCI DSS) complian t Integrated with many AWS services IAM integrates with almost all the AWS services Eventually Consistent IAM, like many other AWS services, is eventually consistent and achieves high availability by replicating data across multiple servers within Amazon\u2019s data centers around the world. Changes made to IAM would be eventually consistent and hence would take some time to reflect Free to use IAM is offered at no additional charge and charges are applied only for use of other AWS products by your IAM users. AWS Security Token Service IAM provide STS which is an included feature of the AWS account offered at no additional charge. AWS charges only for the use of other AWS services accessed by the AWS STS temporary security credentials.","title":"2 IAM Features"},{"location":"chap4/1AWS_IAM/#3-identities","text":"IAM identities determine who can access and help to provide authentication for people and processes in your AWS account","title":"3 Identities"},{"location":"chap4/1AWS_IAM/#4-account-root-user","text":"Root Account Credentials are the email address and password with which you sign-in into the AWS account Root Credentials has full unrestricted access to AWS account including the account security credentials which include sensitive information IAM Best Practice \u2013 Do not use or share the Root account once the AWS account is created, instead create a separate user with admin privilege An Administrator account can be created for all the activities which too has full access to the AWS account except the accounts security credentials, billing information and ability to change password","title":"4 Account Root User"},{"location":"chap4/1AWS_IAM/#5-iam-users","text":"IAM user represents the person or service who uses the access to interact with AWS. IAM Best Practice \u2013 Create Individual Users User credentials can consist of the following Password to access AWS services through AWS Management Console Access Key/Secret Access Key to access AWS services through API, CLI or SDK IAM user starts with no permissions and is not authorized to perform any AWS actions on any AWS resources and should be granted permissions as per the job function requirement IAM Best Practice \u2013 Grant least Privilege Each IAM user is associated with one and only one AWS account. IAM User cannot be renamed from AWS management console and has to be done from CLI or SDK tools. IAM handles the renaming of user w.r.t unique id, groups, policies where the user was mentioned as a principal. However, you need to handle the renaming in the policies where the user was mentioned as a resource","title":"5 IAM Users"},{"location":"chap4/1AWS_IAM/#6-iam-groups","text":"IAM group is a collection of IAM users IAM groups can be used to specify permissions for a collection of users sharing the same job function making it easier to manage IAM Best Practice \u2013 Use groups to assign permissions to IAM Users A group is not truly an identity because it cannot be identified as a Principal in an access policy. It is only a way to attach policies to multiple users at one time A group can have multiple users, while a user can belong to multiple groups (10 max) Groups cannot be nested and can only have users within it AWS does not provide any default group to hold all users in it and if one is required it should be created with all users assigned to it. Renaming of a group name or path, IAM handles the renaming w.r.t to policies attached to the group, unique ids, users within the group. However, IAM does not update the policies where the group is mentioned as a resource and must be handled manually Deletion of the groups requires you to detach users and managed policies and delete any inline policies before deleting the group. With AWS management console, the deletion and detachment is taken care of.","title":"6 IAM Groups"},{"location":"chap4/1AWS_IAM/#7-iam-roles","text":"","title":"7 IAM Roles"},{"location":"chap4/1AWS_IAM/#7-1-aws-iam-role","text":"IAM role is very similar to a user, in that it is an identity with permission policies that determine what the identity can and cannot do in AWS. IAM role is not intended to be uniquely associated with a particular user, group or service and is intended to be assumable by anyone who needs it. Role does not have any credentials (password or access keys) associated with it and whoever assumes the role is provided with a dynamic temporary credentials Role helps in access delegation to grant permissions to someone that allows access to resources that you control Roles can help to prevent accidental access to or modification of sensitive resources Modification of a Role can be done anytime and the changes are reflected across all the entities associated with the Role immediately IAM Role plays a very important role in the following scenarios Services like EC2 instance running an application that needs to access other AWS services Allowing users from different AWS accounts have access to AWS resources in different account, instead of having to create users Company uses a Corporate Authentication mechanism and don\u2019t want the User to authenticate twice or create duplicate users in AWS Applications allowing login through external authentication mechanism e.g. Amazon, Facebook, Google etc Role can be assumed by IAM user within the same AWS account IAM user from a different AWS account AWS service such as EC2, EMR to interact with other services An external user authenticated by an external identity provider (IdP) service that is compatible with SAML 2.0 or OpenID Connect (OIDC), or a custom-built identity broker. Role involves defining two policies Trust policy Trust policy defines \u2013 who can assume the role Trust policy involves setting up a trust between the account that owns the resource (trusting account) and the account who owns the user that needs access to the resources (trusted account) Permissions policy Permissions policy defines \u2013 what they can access Permissions policy determines authorization, which grants the user of the role with the needed permissions to carry out the desired tasks on the resource Federation is creating a trust relationship between an external Identity Provider (IdP) and AWS Users can also sign in to an enterprise identity system that is compatible with SAML Users can sign in to a web identity provider, such as Login with Amazon, Facebook, Google, or any IdP that is compatible with OpenID connect (OIDC). When using OIDC and SAML 2.0 to configure a trust relationship between these external identity providers and AWS, the user is assigned to an IAM role and receives temporary credentials that enables the user to access AWS resources IAM Best Practice \u2013 Use roles for applications running on EC2 instances IAM Best Practice \u2013 Delegate using roles instead of sharing credentials","title":"7-1 AWS IAM Role"},{"location":"chap4/1AWS_IAM/#7-2-role-types","text":"AWS Security Token Service (STS) helps create and provide trusted users with temporary security credentials that control access to AWS resources STS is a global service with a single endpoint https://sts.amazonaws.com AWS STS API calls can be made either to a global endpoint or to one of the regional endpoints. Regional endpoint can help reduce latency and improve the performance of the API calls Temporary Credentials are similar to Long Term Credentials except for are short term and are regularly rotated can be configured to last from few minutes to several hours do not have to be embedded or distributed are not stored or attached with the User, but are generated dynamically and provided to the user as and when requested","title":"7-2 Role types"},{"location":"chap4/1AWS_IAM/#7-3-role-types","text":"","title":"7-3 Role types"},{"location":"chap4/1AWS_IAM/#aws-service-roles","text":"Some AWS services need to interact with other AWS services for e.g. EC2 interacting with S3, SQS etc Best practice is to assign these services with IAM roles instead of embedding or passing IAM user credentials directly into an instance, because distributing and rotating long-term credentials to multiple instances is challenging to manage and a potential security risk. AWS automatically provides temporary security credentials for these services e.g. Amazon EC2 instance to use on behalf of its applications Deleting a role or instance profile that is associated with a running EC2 instance will break any applications running on the instance","title":"AWS Service Roles"},{"location":"chap4/1AWS_IAM/#complete-process-flow","text":"Create a IAM role with services who would use it for e.g. EC2 as trusted entity and define permission policies with the access the service needs Associated a Role (actually an Instance profile) with the EC2 service when the instance is launched Temporary security credentials are available on the instance and are automatically rotated before they expire so that a valid set is always available Application can retrieve the temporary credentials either using the Instance metadata directly or through AWS SDK Applications running on the EC2 instance can now use the permissions defined in the Role to access other AWS resources Application, if caching the credentials, needs to make sure it uses the correct credentials before they expire","title":"Complete Process Flow"},{"location":"chap4/1AWS_IAM/#instance-profile","text":"An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. If a Role is created for EC2 instance or any other service that uses EC2 through AWS Management Console, AWS creates a Instance profile automatically with the same name as the Role. However, if the Role is created through CLI the instance profile needs to created as well An instance profile can contain only one IAM role. However, a role can be included in multiple instance profiles.","title":"Instance Profile"},{"location":"chap4/1AWS_IAM/#7-4-cross-account-access-roles","text":"IAM users can be granted permission to switch roles within the same AWS account or to roles defined in other AWS accounts that you own. Roles can also be used to delegate permissions to IAM users from AWS accounts owned by Third parties You must explicitly grant the users permission to assume the role. Users must actively switch to the role using the AWS Management Console. Multi-factor authentication (MFA) protection can be enabled for the role so that only users who sign in with an MFA device can assume the role However, only One set of permissions are applicable at a time. User who assumes a role temporarily gives up his or her own permissions and instead takes on the permissions of the role. When the user exits, or stops using the role, the original user permissions are restored.","title":"7-4 Cross-Account access Roles"},{"location":"chap4/1AWS_IAM/#7-5-complete-process-flow","text":"Trusting account creates a IAM Role with a Trust policy which defines the account (trusted account) as a principal who can access the resources and a Permissions policy to define what resources can the user in the trusted account access Trusting account provides the Account ID and the Role name (or the ARN) to the trusted account If the Trusting account is own by Third Party it can optionally provide an External ID (recommended for additional security), required to uniquely identify the trusted account, which can be added to the trust policy as a condition Trusted account creates a IAM user who has permissions (Permission to call the AWS Security Token Service (AWS STS) AssumeRole API for the role) to assume the role/switch to the role. IAM User in the Trusted account switches to the Role/assumes the role and passes the ARN of the role Trusted account belonging to the Third party would also pass the External ID mapped to the Trusting account AWS STS verifies the request for the role ARN, External ID if any and if it is from the trusted resource matching the roles\u2019s trust policy and AWS STS upon successful verification returns temporary credentials Temporary credentials allow the user to access the resources of the Trusting account When the user exits the role, the user\u2019s permissions revert to the original permissions held before switching to the role","title":"7-5 Complete Process Flow"},{"location":"chap4/1AWS_IAM/#7-6-external-id-and-confused-deputy-problem","text":"External ID allows the user that is assuming the role to assert the circumstances in which they are operating. External ID provides a way for the account owner to permit the role to be assumed only under specific circumstances and prevents an unauthorized customer from gaining access to your resources Primary function of the external ID is to address and prevent the \u201cconfused deputy\u201d problem.","title":"7-6 External ID and Confused Deputy Problem"},{"location":"chap4/1AWS_IAM/#confused-deputy-problem","text":"Example Corp\u2019s AWS Account provides the services (access, analyze and process data and provide back reports) to multiple different AWS accounts Preferred mechanism is to have each AWS account customer define a Role which Example Corp\u2019s AWS Account users can assume and act upon You provide Example Corp\u2019s AWS Account access to your AWS account through Role and providing Role ARN Example Corp when working on your account assumes the IAM role and provides the ARN with the request As Example Corp is already trusted by your account it will received the temporary security credentials and gain access to your resources If an other AWS account is able to know or guess your ARN (Role with Account ID), it can provide the same to Example Corp Example Corp\u2019s would use the ARN (belonging to your AWS account) to process the data but would provide the same data to the other AWS account This form of privilege escalation is known as the confused deputy problem","title":"Confused Deputy Problem"},{"location":"chap4/1AWS_IAM/#7-7-address-confused-deputy-problem-using-external-id","text":"Using External ID, Example Corp\u2019s generates a unique External ID for each of its Customer which is known only to them and is kept secret Example Corp provides you an External ID which needs to added as a condition while defining the trust policy You provide Example Corp\u2019s AWS Account access to your AWS account through Role and providing Role ARN Example Corp when working on your account uses the IAM role and provides the ARN along with the External ID and as it is already trusted would be able to gain access Other AWS account registered with Example Corp would have a Unique External ID assigned to it If the Other AWS account is able to know or guess your ARN (Role with Account ID), it can provide the same to Example Corp Example Corp\u2019s would request access to your Account using the ARN (belonging to your AWS account) but with the External ID belonging to Other AWS account as the request was made on its behalf As the External ID provided by Example Corp does not match the condition defined in the Role trust policy, the authentication would fail and hence denied access","title":"7-7 Address Confused Deputy Problem using External ID"},{"location":"chap4/1AWS_IAM/#7-8-aws-certification-exam-for-iam-role","text":"A company is building software on AWS that requires access to various AWS services. Which configuration should be used to ensure that AWS credentials (i.e., Access Key ID/Secret Access Key combination) are not compromised? Enable Multi-Factor Authentication for your AWS root account. Assign an IAM role to the Amazon EC2 instance. Store the AWS Access Key ID/Secret Access Key combination in software comments. Assign an IAM user to the Amazon EC2 Instance. A company is preparing to give AWS Management Console access to developers. Company policy mandates identity federation and role-based access control. Roles are currently assigned using groups in the corporate Active Directory. What combination of the following will give developers access to the AWS console? (Select 2) Choose 2 answers AWS Directory Service AD Connector AWS Directory Service Simple AD AWS Identity and Access Management groups AWS identity and Access Management roles AWS identity and Access Management users A customer needs corporate IT governance and cost oversight of all AWS resources consumed by its divisions. The divisions want to maintain administrative control of the discrete AWS resources they consume and keep those resources separate from the resources of other divisions. Which of the following options, when used together will support the autonomy/control of divisions while enabling corporate IT to maintain governance and cost oversight? Choose 2 answers Use AWS Consolidated Billing and disable AWS root account access for the child accounts. Enable IAM cross-account access for all corporate IT administrators in each child account. (Provides IT governance) Create separate VPCs for each division within the corporate IT AWS account. Use AWS Consolidated Billing to link the divisions\u2019 accounts to a parent corporate account. (Will provide cost oversight) Write all child AWS CloudTrail and Amazon CloudWatch logs to each child account\u2019s Amazon S3 \u2018Log\u2019 bucket. Which of the following items are required to allow an application deployed on an EC2 instance to write data to a DynamoDB table? Assume that no security keys are allowed to be stored on the EC2 instance. (Choose 2 answers) Create an IAM Role that allows write access to the DynamoDB table Add an IAM Role to a running EC2 instance. (With latest enhancement from AWS, IAM role can be assigned to a running EC2 instance) Create an IAM User that allows write access to the DynamoDB table. Add an IAM User to a running EC2 instance. Launch an EC2 Instance with the IAM Role included in the launch configuration (This was the correct answer before, as AWS did not allow IAM role to be added to an existing instance) You are looking to migrate your Development (Dev) and Test environments to AWS. You have decided to use separate AWS accounts to host each environment. You plan to link each accounts bill to a Master AWS account using Consolidated Billing. To make sure you Keep within budget you would like to implement a way for administrators in the Master account to have access to stop, delete and/or terminate resources in both the Dev and Test accounts. Identify which option will allow you to achieve this goal. [PROFESSIONAL] Create IAM users in the Master account with full Admin permissions. Create cross-account roles in the Dev and Test accounts that grant the Master account access to the resources in the account by inheriting permissions from the Master account. Create IAM users and a cross-account role in the Master account that grants full Admin permissions to the Dev and Test accounts. Create IAM users in the Master account Create cross-account roles in the Dev and Test accounts that have full Admin permissions and grant the Master account access Link the accounts using Consolidated Billing. This will give IAM users in the Master account access to resources in the Dev and Test accounts You have an application running on an EC2 Instance which will allow users to download flies from a private S3 bucket using a pre-assigned URL. Before generating the URL the application should verify the existence of the file in S3. How should the application use AWS credentials to access the S3 bucket securely? [PROFESSIONAL] Use the AWS account access Keys the application retrieves the credentials from the source code of the application. Create a IAM user for the application with permissions that allow list access to the S3 bucket launch the instance as the IAM user and retrieve the IAM user\u2019s credentials from the EC2 instance user data. Create an IAM role for EC2 that allows list access to objects in the S3 bucket. Launch the instance with the role, and retrieve the role\u2019s credentials from the EC2 Instance metadata Create an IAM user for the application with permissions that allow list access to the S3 bucket. The application retrieves the IAM user credentials from a temporary directory with permissions that allow read access only to the application user. An administrator is using Amazon CloudFormation to deploy a three tier web application that consists of a web tier and application tier that will utilize Amazon DynamoDB for storage when creating the CloudFormation template which of the following would allow the application instance access to the DynamoDB tables without exposing API credentials? [PROFESSIONAL] Create an Identity and Access Management Role that has the required permissions to read and write from the required DynamoDB table and associate the Role to the application instances by referencing an instance profile. Use the Parameter section in the Cloud Formation template to nave the user input Access and Secret Keys from an already created IAM user that has me permissions required to read and write from the required DynamoDB table. Create an Identity and Access Management Role that has the required permissions to read and write from the required DynamoDB table and reference the Role in the instance profile property of the application instance . Create an identity and Access Management user in the CloudFormation template that has permissions to read and write from the required DynamoDB table, use the GetAtt function to retrieve the Access and secret keys and pass them to the application instance through user-data. An enterprise wants to use a third-party SaaS application. The SaaS application needs to have access to issue several API commands to discover Amazon EC2 resources running within the enterprise\u2019s account. The enterprise has internal security policies that require any outside access to their environment must conform to the principles of least privilege and there must be controls in place to ensure that the credentials used by the SaaS vendor cannot be used by any other third party. Which of the following would meet all of these conditions? [PROFESSIONAL] From the AWS Management Console, navigate to the Security Credentials page and retrieve the access and secret key for your account. Create an IAM user within the enterprise account assign a user policy to the IAM user that allows only the actions required by the SaaS application create a new access and secret key for the user and provide these credentials to the SaaS provider. Create an IAM role for cross-account access allows the SaaS provider\u2019s account to assume the role and assign it a policy that allows only the actions required by the SaaS application. Create an IAM role for EC2 instances, assign it a policy mat allows only the actions required tor the SaaS application to work, provide the role ARM to the SaaS provider to use when launching their application instances. A user has created an application which will be hosted on EC2. The application makes calls to DynamoDB to fetch certain data. The application is using the DynamoDB SDK to connect with from the EC2 instance. Which of the below mentioned statements is true with respect to the best practice for security in this scenario? The user should attach an IAM role with DynamoDB access to the EC2 instance The user should create an IAM user with DynamoDB access and use its credentials within the application to connect with DynamoDB The user should create an IAM role, which has EC2 access so that it will allow deploying the application The user should create an IAM user with DynamoDB and EC2 access. Attach the user with the application so that it does not use the root account credentials A customer is in the process of deploying multiple applications to AWS that are owned and operated by different development teams. Each development team maintains the authorization of its users independently from other teams. The customer\u2019s information security team would like to be able to delegate user authorization to the individual development teams but independently apply restrictions to the users permissions based on factors such as the users device and location. For example, the information security team would like to grant read-only permissions to a user who is defined by the development team as read/write whenever the user is authenticating from outside the corporate network. What steps can the information security team take to implement this capability? [PROFESSIONAL] Operate an authentication service that generates AWS STS tokens with IAM policies from application-defined IAM roles. (no user separation, will just help generate temporary tokens) Add additional IAM policies to the application IAM roles that deny user privileges based on information security policy. (Different policy with deny rules based on location, device and more restrictive wins) Configure IAM policies that restrict modification of the application IAM roles only to the information security team. (Authorization should still be in developers control) Enable federation with the internal LDAP directory and grant the application teams permissions to modify users. You are creating an Auto Scaling group whose Instances need to insert a custom metric into CloudWatch. Which method would be the best way to authenticate your CloudWatch PUT request? Create an IAM role with the Put MetricData permission and modify the Auto Scaling launch configuration to launch instances in that role Create an IAM user with the PutMetricData permission and modify the Auto Scaling launch configuration to inject the users credentials into the instance User Data Modify the appropriate Cloud Watch metric policies to allow the Put MetricData permission to instances from the Auto Scaling group Create an IAM user with the PutMetricData permission and put the credentials in a private repository and have applications on the server pull the credentials as needed","title":"7-8 AWS Certification Exam for IAM Role"},{"location":"chap4/1AWS_IAM/#7-multifactor-authentication-mfa","text":"For increased security and to help protect the AWS resources, Multi-Factor authentication can be configured IAM Best Practice \u2013 Enable MFA on Root accounts and privilege users Multi-Factor Authentication can be configured using Security token-based AWS Root user or IAM user can be assigned a hardware/virtual MFA device Device generates a six digit numeric code based upon a time-synchronized one-time password algorithm which needs to be provided during authentication SMS text message-based (Preview Mode) IAM user can be configured with the phone number of the user\u2019s SMS-compatible mobile device which would receive a 6 digit code from AWS SMS-based MFA is available only for IAM users and does not work for AWS root account MFA needs to enabled on the Root user and IAM user separately as they are distinct entities. Enabling MFA on Root does not enable it for all other users MFA device can be associated with only one AWS account or IAM user and vice versa If the MFA device stops working or is lost, you won\u2019t be able to login into the AWS console and would need to reach out to AWS support to deactivate MFA MFA protection can be enabled for service api\u2019s calls using \u201cCondition\u201d: {\u201cBool\u201d: {\u201caws:MultiFactorAuthPresent\u201d: \u201ctrue\u201d}} and is available only if the service supports temporary security credentials.","title":"7 MultiFactor Authentication (MFA)"},{"location":"chap4/1AWS_IAM/#8-aws-iam-access-management","text":"IAM Access Management is all about Permissions and Policies Permission allows you to define who has access and what actions can they perform IAM Policy helps to fine tune the permissions granted to the policy owner IAM Policy is a document that formally states one or more permissions. Most restrictive Policy always wins IAM Policy is defined in the JSON (JavaScript Object Notation) format IAM policy basically states \u201c Principal A is allowed or denied (effect) to perform Action B on Resource C given Conditions D are satisfied\u201d { \u201cVersion\u201d: \u201c2012-10-17\u201d, \u201cStatement\u201d: { \u201cPrincipal\u201c: {\u201cAWS\u201d: [\u201carn:aws:iam::ACCOUNT-ID-WITHOUT-HYPHENS:root\u201d]}, \u201cAction\u201c: \u201cs3:ListBucket\u201d, \u201cEffect\u201c: \u201cAllow\u201d, \u201cResource\u201c: \u201carn:aws:s3:::example_bucket\u201d, \u201cCondition\u201c: {\u201cStringLike\u201d: { \u201cs3:prefix\u201d: [ \u201chome/${aws:username}/\u201d ] } } } } An Entity can be associated with Multiple Policies and a Policy can have multiple statements where each statement in a policy refers to a single permission. If your policy includes multiple statements, a logical OR is applied across the statements at evaluation time. Similarly, if multiple policies are applicable to a request, a logical OR is applied across the policies at evaluation time. Principal can either be specified within the Policy for resource Based policies while for user bases policies the principal is the user, group or role to which the policy is attached","title":"8 AWS IAM Access Management"},{"location":"chap4/1AWS_IAM/#8-1-identity-based-vs-resource-based-permissions","text":"Identity-based, or IAM permissions Identity-based, or IAM permissions are attached to an IAM user, group, or role and specify what the user, group or role can do User, group, or role itself acts as a Principal IAM permissions can be applied to almost all the AWS services IAM Policies can either be inline or managed IAM Policy version has to be 2012-10-17 Resource-based permissions Resource-based permissions are attached to a resource for e.g. S3, SNS Resource-based permissions specifies both who has access to the resource (Principal) and what actions they can perform on it (Actions) Resource-based policies are inline only, not managed. Resource-based permissions are supported only by some AWS services Resource-based policies are always attached inline policy and are not managed Resource-based policies can be defined with version 2012-10-17 or 2008-10-17","title":"8-1 Identity-Based vs Resource-Based Permissions"},{"location":"chap4/1AWS_IAM/#8-2-managed-policies-and-inline-policies","text":"Managed policies Managed policies are Standalone policies that can be attached to multiple users, groups, and roles in an AWS account. Managed policies apply only to identities (users, groups, and roles) but not to resources. Managed policies allow reusability Managed policy changes are implemented as versions (limited to 5), an new change to the existing policy creates a new version which is useful to compare the changes and revert back, if needed Managed policies have their own ARN Two types of managed policies: AWS managed policies Managed policies that are created and managed by AWS. AWS maintains and can upgrades these policies for e.g. if a new service is introduced, the changes automatically effects all the existing principals attached to the policy AWS takes care of not breaking the policies for e.g. adding an restriction of removal of permission Managed policies cannot be modified Customer managed policies Managed policies are standalone and custom policies created and administered by you. Customer managed policies allows more precise control over the policies than when using AWS managed policies. Inline policies Inline policies are created and managed by you, and are embedded directly into a single user, group, or role. Deletion of the Entity (User, Group or Role) or Resource deletes the In-Line policy as well","title":"8-2 Managed Policies and Inline Policies"},{"location":"chap4/1AWS_IAM/#8-3-iam-policy-simulator","text":"IAM Policy Simulator helps test and troubleshoot IAM and resource-based policies IAM Policy Simulator can help test the following ways :- Test IAM based policies. If multiple policies attached, you can test all the policies, or select individual policies to test. You can test which actions are allowed or denied by the selected policies for specific resources. Test Resource based policies. However, Resource based policies cannot be tested standalone and have to be attached with the Resource Test new IAM policies that are not yet attached to a user, group, or role by typing or copying them into the simulator. These are used only in the simulation and are not saved. Test the policies with selected services, actions, and resources Simulate real-world scenarios by providing context keys, such as an IP address or date, that are included in Condition elements in the policies being tested. Identify which specific statement in a policy results in allowing or denying access to a particular resource or action. IAM Policy Simulator does not make an actual AWS service request and hence does not make unwanted changes to the AWS live environment IAM Policy Simulator just reports the result Allowed or Denied IAM Policy Simulator allows to you modify the policy and test. These changes are not propogated to the actual policies attached to the entities Introductory Video for Policy Simulator","title":"8-3 IAM Policy Simulator"},{"location":"chap4/1AWS_IAM/#8-4-iam-policy-evaluation","text":"When determining if permission is allowed, the hierarchy is followed Decision allows starts with Deny IAM combines and evaluates all the policies Explicit Deny First IAM checks for an explicit denial policy. Explicit Deny overrides everything and if something is explicitly deined it can never be allowed Explicit Allow If one does not exist, it then checks for an explicit allow policy. For granting User any permission, the permission must be explicitly allowed Implicit Deny If neither an explicit deny or explicit allow policy exist, it reverts to the default: implicit deny. All permissions are implicity denied by default","title":"8-4 IAM Policy Evaluation"},{"location":"chap4/1AWS_IAM/#8-5-iam-policy-variables","text":"Policy variables provide a feature to specify placeholders in a policy. When the policy is evaluated, the policy variables are replaced with values that come from the request itself Policy variables allow a single policy to be applied to a group of users to control access for e.g. all user having access to S3 bucket folder with their name only Policy variable is marked using a $ prefix followed by a pair of curly braces ({ }) . Inside the ${ } characters, with the name of the value from the request that you want to use in the policy Policy variables work only with policies defined with Version 2012-10-17 Policy variables can only be used in the Resource element and in string comparisons in the Condition element Policy variables are case sensitive and include variables like aws:username, aws:userid, aws:SourceIp, aws:CurrentTime etc.","title":"8-5 IAM Policy Variables"},{"location":"chap4/1AWS_IAM/#8-6-iam-policy-variables","text":"IAM\u2019s Policy Evaluation Logic always starts with a default _ _____ for every request, except for those that use the AWS account\u2019s root security credentials b Permit Deny Cancel An organization has created 10 IAM users. The organization wants each of the IAM users to have access to a separate DynamoDB table. All the users are added to the same group and the organization wants to setup a group level policy for this. How can the organization achieve this? Define the group policy and add a condition which allows the access based on the IAM name Create a DynamoDB table with the same name as the IAM user name and define the policy rule which grants access based on the DynamoDB ARN using a variable Create a separate DynamoDB database for each user and configure a policy in the group based on the DB variable It is not possible to have a group level policy which allows different IAM users to different DynamoDB Tables An organization has setup multiple IAM users. The organization wants that each IAM user accesses the IAM console only within the organization and not from outside. How can it achieve this? Create an IAM policy with the security group and use that security group for AWS console login Create an IAM policy with a condition which denies access when the IP address range is not from the organization Configure the EC2 instance security group which allows traffic only from the organization\u2019s IP range Create an IAM policy with VPC and allow a secure gateway between the organization and AWS Console Can I attach more than one policy to a particular entity? Yes always Only if within GovCloud No Only if within VPC A _ ___ is a document that provides a formal statement of one or more permissions. policy permission Role resource A _ ___ is the concept of allowing (or disallowing) an entity such as a user, group, or role some type of access to one or more resources. user AWS Account resource permission True or False: When using IAM to control access to your RDS resources, the key names that can be used are case sensitive. For example, aws:CurrentTime is NOT equivalent to AWS:currenttime. TRUE FALSE A user has set an IAM policy where it allows all requests if a request from IP 10.10.10.1/32. Another policy allows all the requests between 5 PM to 7 PM. What will happen when a user is requesting access from IP 10.10.10.1/32 at 6 PM? IAM will throw an error for policy conflict It is not possible to set a policy based on the time or IP It will deny access It will allow access Which of the following are correct statements with policy evaluation logic in AWS Identity and Access Management? Choose 2 answers. By default, all requests are denied An explicit allow overrides an explicit deny An explicit allow overrides default deny An explicit deny does not override an explicit allow By default, all request are allowed A web design company currently runs several FTP servers that their 250 customers use to upload and download large graphic files. They wish to move this system to AWS to make it more scalable, but they wish to maintain customer privacy and keep costs to a minimum. What AWS architecture would you recommend? [PROFESSIONAL] Ask their customers to use an S3 client instead of an FTP client. Create a single S3 bucket. Create an IAM user for each customer. Put the IAM Users in a Group that has an IAM policy that permits access to subdirectories within the bucket via use of the \u2018username\u2019 Policy variable . Create a single S3 bucket with Reduced Redundancy Storage turned on and ask their customers to use an S3 client instead of an FTP client. Create a bucket for each customer with a Bucket Policy that permits access only to that one customer. (Creating bucket for each user is not a scalable model, also 100 buckets are a limit earlier without extending which has since changed link) Create an auto-scaling group of FTP servers with a scaling policy to automatically scale-in when minimum network traffic on the auto-scaling group is below a given threshold. Load a central list of ftp users from S3 as part of the user Data startup script on each Instance (Expensive) Create a single S3 bucket with Requester Pays turned on and ask their customers to use an S3 client instead of an FTP client. Create a bucket tor each customer with a Bucket Policy that permits access only to that one customer. (Creating bucket for each user is not a scalable model, also 100 buckets are a limit earlier without extending which has since changed link)","title":"8-6 IAM Policy Variables"},{"location":"chap4/1AWS_IAM/#8-credential-report","text":"IAM allows you to generate and download a credential report that lists all users in the account and the status of their various credentials, including passwords, access keys, and MFA devices. Credential report can be used to assist in auditing and compliance efforts Credential report can be used to audit the effects of credential lifecycle requirements, such as password and access key rotation. IAM Best Practice \u2013 Perform Audits and Remove all unused users and credentials Credential report is generated as often as once every four hours. If the existing report was generated less than four hours, the same is available for download. If more then four hours, IAM generates and downloads a new report.","title":"8 Credential Report"},{"location":"chap4/1AWS_IAM/#9-aws-iam-aws-certification","text":"Which service enables AWS customers to manage users and permissions in AWS? AWS Access Control Service (ACS) AWS Identity and Access Management (IAM) AWS Identity Manager (AIM) IAM provides several policy templates you can use to automatically assign permissions to the groups you create. The _____ policy template gives the Admins group permission to access all account resources, except your AWS account information Read Only Access Power User Access AWS Cloud Formation Read Only Access Administrator Access Every user you create in the IAM system starts with _ __. Partial permissions Full permissions No permissions Groups can\u2019t _____. be nested more than 3 levels be nested at all be nested more than 4 levels be nested more than 2 levels The _____ service is targeted at organizations with multiple users or systems that use AWS products such as Amazon EC2, Amazon SimpleDB, and the AWS Management Console. Amazon RDS AWS Integrity Management AWS Identity and Access Management Amazon EMR An AWS customer is deploying an application that is composed of an AutoScaling group of EC2 Instances. The customers security policy requires that every outbound connection from these instances to any other service within the customers Virtual Private Cloud must be authenticated using a unique x.509 certificate that contains the specific instanceid. In addition an x.509 certificates must be designed by the customer\u2019s Key management service in order to be trusted for authentication. Which of the following configurations will support these requirements? Configure an IAM Role that grants access to an Amazon S3 object containing a signed certificate and configure the Auto Scaling group to launch instances with this role. Have the instances bootstrap get the certificate from Amazon S3 upon first boot. Embed a certificate into the Amazon Machine Image that is used by the Auto Scaling group. Have the launched instances generate a certificate signature request with the instance\u2019s assigned instance-id to the Key management service for signature. Configure the Auto Scaling group to send an SNS notification of the launch of a new instance to the trusted key management service. Have the Key management service generate a signed certificate and send it directly to the newly launched instance. Configure the launched instances to generate a new certificate upon first boot. Have the Key management service poll the AutoScaling group for associated instances and send new instances a certificate signature that contains the specific instance-id. When assessing an organization AWS use of AWS API access credentials which of the following three credentials should be evaluated? Choose 3 answers Key pairs Console passwords Access keys Signing certificates Security Group memberships (required for EC2 instance access) An organization has created 50 IAM users. The organization wants that each user can change their password but cannot change their access keys. How can the organization achieve this? The organization has to create a special password policy and attach it to each user The root account owner has to use CLI which forces each IAM user to change their password on first login By default each IAM user can modify their passwords Root account owner can set the policy from the IAM console under the password policy screen An organization has created 50 IAM users. The organization has introduced a new policy which will change the access of an IAM user. How can the organization implement this effectively so that there is no need to apply the policy at the individual user level? Use the IAM groups and add users as per their role to different groups and apply policy to group The user can create a policy and apply it to multiple users in a single go with the AWS CLI Add each user to the IAM role as per their organization role to achieve effective policy setup Use the IAM role and implement access at the role level Your organization\u2019s security policy requires that all privileged users either use frequently rotated passwords or one-time access credentials in addition to username/password. Which two of the following options would allow an organization to enforce this policy for AWS users? Choose 2 answers Configure multi-factor authentication for privileged IAM users Create IAM users for privileged accounts (can set password policy) Implement identity federation between your organization\u2019s Identity provider leveraging the IAM Security Token Service Enable the IAM single-use password policy option for privileged users (no such option the password expiration can be set from 1 to 1095 days) Your organization is preparing for a security assessment of your use of AWS. In preparation for this assessment, which two IAM best practices should you consider implementing? Choose 2 answers Create individual IAM users for everyone in your organization Configure MFA on the root account and for privileged IAM users Assign IAM users and groups configured with policies granting least privilege access Ensure all users have been assigned and are frequently rotating a password, access ID/secret key, and X.509 certificate A company needs to deploy services to an AWS region which they have not previously used. The company currently has an AWS identity and Access Management (IAM) role for the Amazon EC2 instances, which permits the instance to have access to Amazon DynamoDB. The company wants their EC2 instances in the new region to have the same privileges. How should the company achieve this? Create a new IAM role and associated policies within the new region Assign the existing IAM role to the Amazon EC2 instances in the new region Copy the IAM role and associated policies to the new region and attach it to the instances Create an Amazon Machine Image (AMI) of the instance and copy it to the desired region using the AMI Copy feature After creating a new IAM user which of the following must be done before they can successfully make API calls? Add a password to the user. Enable Multi-Factor Authentication for the user. Assign a Password Policy to the user. Create a set of Access Keys for the user An organization is planning to create a user with IAM. They are trying to understand the limitations of IAM so that they can plan accordingly. Which of the below mentioned statements is not true with respect to the limitations of IAM? One IAM user can be a part of a maximum of 5 groups Organization can create 100 groups per AWS account One AWS account can have a maximum of 5000 IAM users One AWS account can have 250 roles Within the IAM service a GROUP is regarded as a: A collection of AWS accounts It\u2019s the group of EC2 machines that gain the permissions specified in the GROUP. There\u2019s no GROUP in IAM, but only USERS and RESOURCES. A collection of users . Is there a limit to the number of groups you can have? Yes for all users except root No Yes unless special permission granted Yes for all users What is the default maximum number of MFA devices in use per AWS account (at the root account level)? 1 5 15 10 When you use the AWS Management Console to delete an IAM user, IAM also deletes any signing certificates and any access keys belonging to the user. FALSE This is configurable TRUE You are setting up a blog on AWS. In which of the following scenarios will you need AWS credentials? (Choose 3) Sign in to the AWS management console to launch an Amazon EC2 instance Sign in to the running instance to instance some software (needs ssh keys) Launch an Amazon RDS instance Log into your blog\u2019s content management system to write a blog post (need to authenticate using blog authentication) Post pictures to your blog on Amazon S3 An organization has 500 employees. The organization wants to set up AWS access for each department. Which of the below mentioned options is a possible solution? Create IAM roles based on the permission and assign users to each role Create IAM users and provide individual permission to each Create IAM groups based on the permission and assign IAM users to the groups It is not possible to manage more than 100 IAM users with AWS An organization has hosted an application on the EC2 instances. There will be multiple users connecting to the instance for setup and configuration of application. The organization is planning to implement certain security best practices. Which of the below mentioned pointers will not help the organization achieve better security arrangement? Apply the latest patch of OS and always keep it updated. Allow only IAM users to connect with the EC2 instances with their own secret access key . Disable the password-based login for all the users. All the users should use their own keys to connect with the instance securely. Create a procedure to revoke the access rights of the individual user when they are not required to connect to EC2 instance anymore for the purpose of application configuration.","title":"9 AWS IAM AWS Certification"},{"location":"chap7/1rds/","text":"L1 AWS RDS 1 Intro 1-1 Relational Database Service \u2013 RDS Relational Database Service (RDS) is a web service that makes it easier to set up, operate, and scale a relational database in the cloud. RDS provides cost-efficient, resizable capacity for an industry-standard relational database and manages common database administration tasks such as hardware provisioning, database setup, patching, and backups . RDS features & benefits CPU, memory, storage, and IOPs can be scaled independently. manages backups, software patching, automatic failure detection, and recovery. automated backups can be performed as needed, or manual backups can be triggered as well. Backups can be used to restore a database, and the RDS restore process works reliably and efficiently. provides high availability with a primary instance and a synchronous standby secondary instance that can be failover to seamlessly when a problem occurs. provides elasticity & scalability by enabling Read Replicas to increase read scaling. supports MySQL, MariaDB, PostgreSQL, Oracle, Microsoft SQL Server, and the new, MySQL-compatible Aurora DB engine supports IAM users and permissions to control who has access to the RDS database service databases can be further protected by putting them in a VPC , using SSL for data in transit and encryption for data in rest However, as it is a managed service, shell (root ssh) access to DB instances is not provided, and this restricts access to certain system procedures and tables that require advanced privileges. 1-2 RDS Components DB Instance is a basic building block of RDS is an isolated database environment in the cloud each DB instance runs a DB engine. AWS currently supports MySQL, MariaDB, PostgreSQL, Oracle, and Microsoft SQL Server & Aurora DB engines can be accessed from AWS command-line tools, RDS APIs, or the AWS Management RDS Console. computation and memory capacity of a DB instance is determined by its DB instance class, which can be selected as per the needs supports three storage types: Magnetic, General Purpose (SSD), and Provisioned IOPS (SSD), which differ in performance and price each DB instance has a DB instance identifier, which is a customer-supplied name and must be unique for that customer in an AWS region. It uniquely identifies the DB instance when interacting with the RDS API and AWS CLI commands. each DB instance can host multiple user-created databases or a single Oracle database with multiple schemas. can be hosted in an AWS VPC environment for better control Regions and Availability Zones AWS resources are housed in highly available data center facilities in different areas of the world, these data centers are called regions which further contain multiple distinct locations called Availability Zones Each AZ is engineered to be i solated from failures in other AZs and to provide inexpensive, low-latency network connectivity to other AZs in the same region DB instances can be hosted in different AZs, an option called a Multi-AZ deployment. RDS automatically provisions and maintains a synchronous standby replica of the DB instance in a different AZ. Primary DB instance is synchronously replicated across AZs to the standby replica Provides data redundancy, failover support, eliminates I/O freezes, and minimizes latency spikes during system backups. Security Groups security group controls the access to a DB instance, by allowing access to the specified IP address ranges or EC2 instances DB Parameter Groups A DB parameter group contains engine configuration values that can be applied to one or more DB instances of the same instance type help define configuration values specific to the selected DB Engine for e.g. max_connections , force_ssl , autocommit supports default parameter group, which cannot be edited. supports custom parameter group, to override values supports static and dynamic parameter groups changes to dynamic parameters are applied immediately (irrespective of apply immediately setting) changes to static parameters are NOT applied immediately and require a manual reboot. DB Option Groups Some DB engines offer tools or optional features that simplify managing the databases and making the best use of data. RDS makes such tools available through option groups for e.g. Oracle Application Express (APEX), SQL Server Transparent Data Encryption, and MySQL Memcached support. 1-3 RDS Interfaces RDS can be interacted with multiple interfaces AWS RDS Management console Command Line Interface Programmatic Interfaces which include SDKs, libraries in different languages, and RDS API 1-4 RDS Pricing Instance class Pricing is based on the class (e.g., micro, small, large, xlarge) of the DB instance consumed. Running time Usage is billed in one-second increments, with a minimum of 10 minutes Storage Storage capacity provisioned for the DB instance is billed per GB per month If the provisioned storage capacity is scaled within the month, the bill will be pro-rated. I/O requests per month Total number of storage I/O requests made in a billing cycle. Provisioned IOPS (per IOPS per month) Provisioned IOPS rate, regardless of IOPS consumed, for RDS Provisioned IOPS (SSD) storage only. Provisioned storage for EBS volumes is billed in one-second increments, with a minimum of 10 minutes. Backup storag e Automated backups & any active database snapshots consume storage Increasing backup retention period or taking additional database snapshots increases the backup storage consumed by the database. RDS provides backup storage up to 100% of the provisioned database storage at no additional charge for e.g., if you have 10 GB-months of provisioned database storage, RDS provides up to 10 GB-months of backup storage at no additional charge. Most databases require less raw storage for a backup than for the primary dataset, so if multiple backups are not maintained, you will never pay for backup storage. Backup storage is free only for active DB instances. Data transfer Internet data transfer in and out of your DB instance. Reserved Instances In addition to regular RDS pricing, reserved DB instances can be purchased 1-5 QA What does Amazon RDS stand for? Regional Data Server. Relational Database Service Regional Database Service. How many relational database engines does RDS currently support? MySQL, Postgres, MariaDB, Oracle, and Microsoft SQL Server Just two: MySQL and Oracle. Five: MySQL, PostgreSQL, MongoDB, Cassandra and SQLite. Just one: MySQL. If I modify a DB Instance or the DB parameter group associated with the instance, should I reboot the instance for the changes to take effect? No Yes What is the name of licensing model in which I can use your existing Oracle Database licenses to run Oracle deployments on Amazon RDS? Bring Your Own License Role Bases License Enterprise License License Included Will I be charged if the DB instance is idle? No Yes Only is running in GovCloud Only if running in VPC What is the minimum charge for the data transferred between Amazon RDS and Amazon EC2 Instances in the same Availability Zone ? USD 0.10 per GB No charge. It is free. USD 0.02 per GB USD 0.01 per GB Does Amazon RDS allow direct host access via Telnet, Secure Shell (SSH), or Windows Remote Desktop Connection? Yes No Depends on if it is in VPC or not What are the two types of licensing options available for using Amazon RDS for Oracle? BYOL and Enterprise License BYOL and License Included Enterprise License and License Included Role based License and License Included A user plans to use RDS as a managed DB platform. Which of the below mentioned features is not supported by RDS? Automated backup Automated scaling to manage a higher load Automated failure detection and recovery Automated software patching A user is launching an AWS RDS with MySQL. Which of the below mentioned options allows the user to configure the InnoDB engine parameters? Options group Engine parameters Parameter groups DB parameters A user is planning to use the AWS RDS with MySQL. Which of the below mentioned services the user is not going to pay? Data transfer RDS CloudWatch metrics Data storage I/O requests per month 2 AWS RDS Replication \u2013 Multi-AZ vs Read Replica 2-1 RDS Multi-AZ vs Read Replica DB instances replicas can be created in two ways Multi-AZ & Read Replica, which provide high availability, durability, and scalability to RDS. 2-2 Multi-AZ deployments RDS Multi-AZ deployment provides high availability, durability, and failover support RDS automatically provisions and manages a synchronous standby instance in a different AZ (independent infrastructure in a physically separate location) RDS automatically fails over to the standby so that database operations can resume quickly without administrative intervention in case of Planned database maintenance Software patching Rebooting of the Primary instance Primary DB instance connectivity or host failure, or an Availability Zone failure RDS maintains the same endpoint for the DB Instance after a failover, so the application can resume database operation without the need for manual administrative intervention. 2-3 RDS Read Replicas Read replicas enable increased scalability and database availability in the case of an AZ failure. Read Replicas allow elastic scaling beyond the capacity constraints of a single DB instance for read-heavy database workloads RDS read replicas can be Multi-AZ i.e. set up with their own standby instances in a different AZ . Load on the source DB instance can be reduced by routing read queries from applications to the Read Replica. one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. RDS uses DB engines\u2019 built-in replication functionality to create a special type of DB instance called a Read Replica from a source DB instance. It uses the engines\u2019 native asynchronous replication to update the read replica whenever there is a change to the source DB instance. Read replicas are available in RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server as well as Aurora. 2-4 QAs You are running a successful multi-tier web application on AWS and your marketing department has asked you to add a reporting tier to the application. The reporting tier will aggregate and publish status reports every 30 minutes from user-generated information that is being stored in your web applications database. You are currently running a Multi-AZ RDS MySQL instance for the database tier. You also have implemented ElastiCache as a database caching layer between the application tier and database tier. Please select the answer that will allow you to successfully implement the reporting tier with as little impact as possible to your database. Continually send transaction logs from your master database to an S3 bucket and generate the reports of the S3 bucket using S3 byte range requests. Generate the reports by querying the synchronously replicated standby RDS MySQL instance maintained through Multi-AZ (Standby instance cannot be used as a scaling solution) Launch an RDS Read Replica connected to your Multi-AZ master database and generate reports by querying the Read Replica. Generate the reports by querying the ElastiCache database caching tier. (ElasticCache does not maintain full data and is simply a caching solution) A company is deploying a new two-tier web application in AWS. The company has limited staff and requires high availability, and the application requires complex queries and table joins. Which configuration provides the solution for the company\u2019s requirements? MySQL Installed on two Amazon EC2 Instances in a single Availability Zone (does not provide High Availability out of the box) Amazon RDS for MySQL with Multi-AZ Amazon ElastiCache (Just a caching solution) Amazon DynamoDB (Not suitable for complex queries and joins) Your company is getting ready to do a major public announcement of a social media site on AWS. The website is running on EC2 instances deployed across multiple Availability Zones with a Multi-AZ RDS MySQL Extra Large DB Instance. The site performs a high number of small reads and writes per second and relies on an eventual consistency model. After comprehensive tests you discover that there is read contention on RDS MySQL. Which are the best approaches to meet these requirements? (Choose 2 answers) Deploy ElastiCache in-memory cache running in each availability zone Implement sharding to distribute load to multiple RDS MySQL instances (this is only a read contention, the writes work fine) Increase the RDS MySQL Instance size and Implement provisioned IOPS (not scalable, this is only a read contention, the writes work fine) Add an RDS MySQL read replica in each availability zone Your company has HQ in Tokyo and branch offices all over the world and is using logistics software with a multi-regional deployment on AWS in Japan, Europe and US. The logistic software has a 3-tier architecture and currently uses MySQL 5.6 for data persistence. Each region has deployed its own database. In the HQ region you run an hourly batch process reading data from every region to compute cross-regional reports that are sent by email to all offices this batch process must be completed as fast as possible to quickly optimize logistics. How do you build the database architecture in order to meet the requirements? For each regional deployment, use RDS MySQL with a master in the region and a read replica in the HQ region For each regional deployment, use MySQL on EC2 with a master in the region and send hourly EBS snapshots to the HQ region For each regional deployment, use RDS MySQL with a master in the region and send hourly RDS snapshots to the HQ region For each regional deployment, use MySQL on EC2 with a master in the region and use S3 to copy data files hourly to the HQ region Use Direct Connect to connect all regional MySQL deployments to the HQ region and reduce network latency for the batch process What would happen to an RDS (Relational Database Service) Multi-Availability Zone deployment if the primary DB instance fails? IP of the primary DB Instance is switched to the standby DB Instance. A new DB instance is created in the standby availability zone. The canonical name record (CNAME) is changed from primary to standby. The RDS (Relational Database Service) DB instance reboots. Your business is building a new application that will store its entire customer database on a RDS MySQL database, and will have various applications and users that will query that data for different purposes. Large analytics jobs on the database are likely to cause other applications to not be able to get the query results they need to, before time out. Also, as your data grows, these analytics jobs will start to take more time, increasing the negative effect on the other applications. How do you solve the contention issues between these different workloads on the same data? Enable Multi-AZ mode on the RDS instance Use ElastiCache to offload the analytics job data Create RDS Read-Replicas for the analytics work Run the RDS instance on the largest size possible Will my standby RDS instance be in the same Availability Zone as my primary? Only for Oracle RDS types Yes Only if configured at launch No A user is planning to set up the Multi-AZ feature of RDS. Which of the below mentioned conditions won\u2019t take advantage of the Multi-AZ feature? Availability zone outage A manual failover of the DB instance using Reboot with failover option Region outage When the user changes the DB instance\u2019s server type When you run a DB Instance as a Multi-AZ deployment, the \u201c_____\u201d serves database writes and reads secondary backup stand by primary When running my DB Instance as a Multi-AZ deployment, can I use the standby for read or write operations? Yes Only with MSSQL based RDS Only for Oracle RDS instances No Read Replicas require a transactional storage engine and are only supported for the _ __ storage engine OracleISAM MSSQLDB InnoDB MyISAM A user is configuring the Multi-AZ feature of an RDS DB. The user came to know that this RDS DB does not use the AWS technology, but uses server mirroring to achieve replication. Which DB is the user using right now? MySQL Oracle MS SQL PostgreSQL If I have multiple Read Replicas for my master DB Instance and I promote one of them, what happens to the rest of the Read Replicas? The remaining Read Replicas will still replicate from the older master DB Instance The remaining Read Replicas will be deleted The remaining Read Replicas will be combined to one read replica If you have chosen Multi-AZ deployment, in the event of a planned or unplanned outage of your primary DB Instance, Amazon RDS automatically switches to the standby replica. The automatic failover mechanism simply changes the ______ record of the main DB Instance to point to the standby DB Instance. DNAME CNAME TXT MX When automatic failover occurs, Amazon RDS will emit a DB Instance event to inform you that automatic failover occurred. You can use the _____ to return information about events related to your DB Instance FetchFailure DescriveFailure DescribeEvents FetchEvents The new DB Instance that is created when you promote a Read Replica retains the backup window period. TRUE FALSE Will I be alerted when automatic failover occurs? Only if SNS configured No Yes 1Only if Cloudwatch configured Can I initiate a \u201cforced failover\u201d for my MySQL Multi-AZ DB Instance deployment? Only in certain regions Only in VPC Yes No A user is accessing RDS from an application. The user has enabled the Multi-AZ feature with the MS SQL RDS DB. During a planned outage how will AWS ensure that a switch from DB to a standby replica will not affect access to the application ? RDS will have an internal IP which will redirect all requests to the new DB RDS uses DNS to switch over to standby replica for seamless transition The switch over changes Hardware so RDS does not need to worry about access RDS will have both the DBs running independently and the user has to manually switch over Which of the following is part of the failover process for a Multi-AZ Amazon Relational Database Service (RDS) instance? The failed RDS DB instance reboots. The IP of the primary DB instance is switched to the standby DB instance. The DNS record for the RDS endpoint is changed from primary to standby . A new DB instance is created in the standby availability zone. Which of these is not a reason a Multi-AZ RDS instance will failover? An Availability Zone outage A manual failover of the DB instance was initiated using Reboot with failover To autoscale to a higher instance class Master database corruption occurs The primary DB instance fails You need to scale an RDS deployment. You are operating at 10% writes and 90% reads, based on your logging. How best can you scale this in a simple way? Create a second master RDS instance and peer the RDS groups. Cache all the database responses on the read side with CloudFront. Create read replicas for RDS since the load is mostly reads . Create a Multi-AZ RDS installs and route read traffic to standby. How does Amazon RDS multi Availability Zone model work? A second, standby database is deployed and maintained in a different availability zone from master, using synchronous replication. A second, standby database is deployed and maintained in a different availability zone from master using asynchronous replication. A second, standby database is deployed and maintained in a different region from master using asynchronous replication. A second, standby database is deployed and maintained in a different region from master using synchronous replication. A customer is running an application in US-West (Northern California) region and wants to setup disaster recovery failover to the Asian Pacific (Singapore) region. The customer is interested in achieving a low Recovery Point Objective (RPO) for an Amazon RDS multi-AZ MySQL database instance. Which approach is best suited to this need? Synchronous replication Asynchronous replication Route53 health checks Copying of RDS incremental snapshots A user is using a small MySQL RDS DB. The user is experiencing high latency due to the Multi AZ feature. Which of the below mentioned options may not help the user in this situation? Schedule the automated back up in non-working hours Use a large or higher size instance Use PIOPS Take a snapshot from standby Replica Are Reserved Instances available for Multi-AZ Deployments? Only for Cluster Compute instances Yes for all instance types Only for M3 instance types My Read Replica appears \u201cstuck\u201d after a Multi-AZ failover and is unable to obtain or apply updates from the source DB Instance. What do I do? You will need to delete the Read Replica and create a new one to replace it. You will need to disassociate the DB Engine and re-associate it. The instance should be deployed to Single AZ and then moved to Multi-AZ once again You will need to delete the DB Instance and create a new one to replace it. What is the charge for the data transfer incurred in replicating data between your primary and standby? No charge. It is free . Double the standard data transfer charge Same as the standard data transfer charge Half of the standard data transfer charge A user has enabled the Multi-AZ feature with the MS SQL RDS database server. Which of the below mentioned statements will help the user understand the Multi-AZ feature better? In a Multi-AZ, AWS runs two DBs in parallel and copies the data asynchronously to the replica copy In a Multi-AZ, AWS runs two DBs in parallel and copies the data synchronously to the replica copy In a Multi-AZ, AWS runs just one DB but copies the data synchronously to the standby replic a AWS MS SQL does not support the Multi-AZ feature A company is running a batch analysis every hour on their main transactional DB running on an RDS MySQL instance to populate their central Data Warehouse running on Redshift. During the execution of the batch their transactional applications are very slow. When the batch completes they need to update the top management dashboard with the new data. The dashboard is produced by another system running on-premises that is currently started when a manually sent email notifies that an update is required The on-premises system cannot be modified because is managed by another team. How would you optimize this scenario to solve performance issues and automate the process as much as possible? Replace RDS with Redshift for the batch analysis and SNS to notify the on-premises system to update the dashboard Replace RDS with Redshift for the batch analysis and SQS to send a message to the on-premises system to update the dashboard Create an RDS Read Replica for the batch analysis and SNS to notify me on-premises system to update the dashboard Create an RDS Read Replica for the batch analysis and SQS to send a message to the on-premises system to update the dashboard. 3 AWS RDS Storage \u2013 Certification 3-1 AWS RDS Storage RDS storage uses Elastic Block Store (EBS) volumes for database and log storage. RDS automatically stripes across multiple EBS volumes to enhance IOPS performance, depending on the amount of storage requested 3-2 RDS Storage Types RDS storage provides three storage types: Magnetic, General Purpose (SSD), and Provisioned IOPS (input/output operations per second) . These storage types differ in performance characteristics and price, which allows tailoring of storage performance and cost to the database needs MySQL, MariaDB, PostgreSQL, and Oracle RDS DB instances can be created with up to 6TB of storage and SQL Server RDS DB instances with up to 4TB of storage when using the Provisioned IOPS and General Purpose (SSD) storage types. Existing MySQL, PostgreSQL, and Oracle RDS database instances can be scaled to these new database storage limits without any downtime. Magnetic (Standard) Magnetic storage, also called standard storage, offers cost-effective storage that is ideal for applications with light or burst I/O requirements. They deliver approximately 100 IOPS on average, with burst capability of up to hundreds of IOPS, and they can range in size from 5 GB to 3 TB, depending on the DB instance engine. Magnetic storage is not reserved for a single DB instance, so performance can vary greatly depending on the demands placed on shared resources by other customers. MGeneral Purpose (SSD) General purpose, SSD-backed storage, also called gp2, can provide faster access than disk-based storage. They can deliver single-digit millisecond latencies, with a base performance of 3 IOPS per Gigabyte (GB) and the ability to burst to 3,000 IOPS for extended periods of time up to a maximum of 10,000 PIOPS. Gp2 volumes can range in size from 5 GB to 6 TB for MySQL, MariaDB, PostgreSQL, and Oracle DB instances, and from 20 GB to 4 TB for SQL Server DB instances. Gp2 is excellent for small to medium-sized databases. Provisioned IOPS Provisioned IOPS storage is designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency in random access I/O throughput. Provisioned IOPS storage is a storage type that delivers fast, predictable, and consistent throughput performance. For any production application that requires fast and consistent I/O performance, Amazon recommends Provisioned IOPS (input/output operations per second) storage. Provisioned IOPS storage is optimized for I/O intensive, online transaction processing (OLTP) workloads that have consistent performance requirements. Provisioned IOPS helps performance tuning. Provisioned IOPS volumes can range in size from 100 GB to 6 TB for MySQL, MariaDB, PostgreSQL, and Oracle DB engines. SQL Server Express and Web editions can range in size from 100 GB to 4 TB, while SQL Server Standard and Enterprise editions can range in size from 200 GB to 4 TB. Dedicated IOPS rate and storage space allocation is specified, when a DB instance is created. RDS provisions that IOPS rate and storage for the lifetime of the DB instance or until its changed. RDS delivers within 10 percent of the provisioned IOPS performance 99.9 percent of the time over a given year. Adding Storage and Changing Storage Type DB instance can be modified to use additional storage and converted to a different storage type. However, storage allocated for a DB instance cannot be decreased MySQL, MariaDB, PostgreSQL, and Oracle DB instances can be scaled up for storage, which helps improve I/O capacity. Storage capacity nor the type of storage for a SQL Server DB instance can be changed due to extensibility limitations of striped storage attached to a Windows Server environment. During the scaling process, the DB instance will be available for reads and writes, but may experience performance degradation Adding storage may take several hours; the duration of the process depends on several factors such as load, storage size, storage type, amount of IOPS provisioned (if any), and number of prior scale storage operations. While storage is being added, nightly backups are suspended and no other RDS operations can take place, including modify, reboot, delete, create Read Replica, and create DB Snapshot 3-3 Performance Metrics Amazon RDS provides several metrics that can be used to determine how the DB instance is performing. IOPS the number of I/O operations completed per second. it is reported as the average IOPS for a given time interval. RDS reports read and write IOPS separately on one minute intervals. Total IOPS is the sum of the read and write IOPS. Typical values for IOPS range from zero to tens of thousands per second. Latency the elapsed time between the submission of an I/O request and its completion it is reported as the average latency for a given time interval. RDS reports read and write latency separately on one minute intervals in units of seconds. Typical values for latency are in the millisecond (ms) Throughput the number of bytes per second transferred to or from disk it is reported as the average throughput for a given time interval. RDS reports read and write throughput separately on one minute intervals using units of megabytes per second (MB/s). Typical values for throughput range from zero to the I/O channel\u2019s maximum bandwidth. Queue Depth the number of I/O requests in the queue waiting to be serviced. these are I/O requests that have been submitted by the application but have not been sent to the device because the device is busy servicing other I/O requests. it is reported as the average queue depth for a given time interval. RDS reports queue depth in one minute intervals. Typical values for queue depth range from zero to several hundred. Time spent waiting in the queue is a component of Latency and Service Time (not available as a metric). 3-4 Amazon RDS Storage Facts First time a DB instance is started and accesses an area of disk for the first time, the process can take longer than all subsequent accesses to the same disk area. This is known as the \u201c first touch penalty \u201d. Once an area of disk has incurred the first touch penalty, that area of disk does not incur the penalty again for the life of the instance, even if the DB instance is rebooted, restarted, or the DB instance class changes. Note that a DB instance created from a snapshot, a point-in-time restore, or a read replica is a new instance and does incur this first touch penalty. RDS manages the DB instance and it reserves overhead space on the instance. While the amount of reserved storage varies by DB instance class and other factors, this reserved space can be as much as one or two percent of the total storage Provisioned IOPS provides a way to reserve I/O capacity by specifying IOPS. Like any other system capacity attribute, maximum throughput under load will be constrained by the resource that is consumed first, which could be IOPS, channel bandwidth, CPU, memory, or database internal resources. Current maximum channel bandwidth available is 4000 megabits per second (Mbps) full duplex. In terms of the read and write throughput metrics, this equates to about 210 megabytes per second (MB/s) in each direction. A perfectly balanced workload of 50% reads and 50% writes may attain a maximum combined throughput of 420 MB/s, which includes protocol overhead, so the actual data throughput may be less. Provisioned IOPS works with an I/O request size of 32 KB. Provisioned IOPS consumption is a linear function of I/O request size above 32 KB. An I/O request smaller than 32 KB is handled as one I/O; for e.g. 1000 16 KB I/O requests are treated the same as 1000 32 KB requests. I/O requests larger than 32 KB consume more than one I/O request; while, a 48 KB I/O request consumes 1.5 I/O requests of storage capacity; a 64 KB I/O request consumes 2 I/O requests 3-5 Factors That Impact Storage Performance Several factors can affect the performance of a DB instance, such as instance configuration, I/O characteristics, and workload demand. System related activities also consume I/O capacity and may reduce database instance performance while in progress: DB snapshot creation Nightly backups Multi-AZ peer creation Read replica creation Scaling storage System resources can constrain the throughput of a DB instance, but there can be other reasons for a bottleneck. Database could be the issue if : Channel throughput limit is not reached Queue depths are consistently low CPU utilization is under 80% Free memory available No swap activity Plenty of free disk space Application has dozens of threads all submitting transactions as fast as the database will take them, but there is clearly unused I/O capacity 3-6 QAs When should I choose Provisioned IOPS over Standard RDS storage? If you have batch-oriented workloads If you use production online transaction processing (OLTP) workloads If you have workloads that are not sensitive to consistent performance Is decreasing the storage size of a DB Instance permitted? Depends on the RDMS used Yes No Because of the extensibility limitations of striped storage attached to Windows Server, Amazon RDS does not currently support increasing storage on a _____ DB Instance. SQL Server MySQL Oracle If I want to run a database in an Amazon instance, which is the most recommended Amazon storage option? Amazon Instance Storage Amazon EBS You can\u2019t run a database inside an Amazon instance. Amazon S3 For each DB Instance class, what is the maximum size of associated storage capacity? 1TiB 2TiB 8TiB 16TiB (The limit keeps on changing so please check the latest always) 4 AWS RDS DB Snapshot, Backup & Restore 4-1 RDS BackUp, Restore and Snapshots RDS creates a storage volume snapshot of the DB instance, backing up the entire DB instance and not just individual databases. RDS provides two different methods Automated and Manual for backing up your DB instances: 4-2 Automated backups Backups of the DB instance are automatically created and retained Automated backups are enabled by default for a new DB instance. Automated backups occur during a daily user-configurable period of time, known as preferred backup window . If a preferred backup window is not specified when an DB instance is created, RDS assigns a default 30-minute backup window which is selected at random from an 8-hour block of time per region. Changes to the backup window take effect immediately. Backup window cannot overlap with the weekly maintenance window for the DB instance. Backups created during the backup window are retained for a user-configurable number of days , known as backup retention period If the backup retention period is not set, RDS defaults the period retention period to one day, if created using RDS API or the AWS CLI, or seven days if created AWS Console Backup retention period can be modified with valid values are 0 (for no backup retention) to a maximum of 35 days. Manual snapshot limits (50 per region) do not apply to automated backups If the backup requires more time than allotted to the backup window, the backup will continue to completion. An immediate outage occurs if the backup retention period is changed from 0 to a non-zero value as the first backup occurs immediately or from non-zero value to 0 as it turns off automatic backups, and deletes all existing automated backups for the instance. RDS uses the periodic data backups in conjunction with the transaction logs to enable restoration of the DB Instance to any second during the retention period, up to the LatestRestorableTime (typically up to the last few minutes). During the backup window, for Single AZ instance, storage I/O may be briefly suspended while the backup process initializes (typically under a few seconds) and a brief period of elevated latency might be experienced. for Multi-AZ DB deployments, there is No I/O suspension since the backup is taken from the standby instance First backup is a full backup, while the others are incremental. Automated DB backups are deleted when the retention period expires the automated DB backups for a DB instance is disabled the DB instance is deleted When a DB instance is deleted , a final DB snapshot can be created upon deletion; which can be used to restore the deleted DB instance at a later date. RDS retains the final user-created DB snapshot along with all other manually created DB snapshots all automated backups are deleted and cannot be recovered 4-3 Point-In-Time Recovery In addition to the daily automated backup, RDS archives database change logs. This enables recovery of the database to any point in time during the backup retention period, up to the last five minutes of database usage. Disabling automated backups also disables point-in-time recovery RDS stores multiple copies of your data, but for Single-AZ DB instances these copies are stored in a single availability zone. If for any reason a Single-AZ DB instance becomes unusable, point-in-time recovery can be used to launch a new DB instance with the latest restorable data 4-4 DB Snapshots (User Initiated \u2013 Manual) DB snapshots are manual, user-initiated backups that enables to backup a DB instance to a known state, and restore to that specific state at any time RDS keeps all manual DB snapshots until explicitly deleted 4-5 DB Snapshots Creation DB snapshot is a user-initiated storage volume snapshot of DB instance, backing up the entire DB instance and not just individual databases. DB snapshots enable backing up of the DB instance in a known state as needed, and can then be restored to that specific state at any time. DB snapshots are kept until explicitly deleted Creating DB snapshot on a Single-AZ DB instance results in a brief I/O suspension that typically lasting no more than a few minutes. Multi-AZ DB instances are not affected by this I/O suspension since the backup is taken on the standby instance 4-6 DB Snapshot Restore DB instance can be restored to any specific time during this retention period, creating a new DB instance. DB restore creates a New DB instance with a different endpoint RDS uses the periodic data backups in conjunction with the transaction logs to enable restoration of the DB Instance to any second during the retention period, up to the LatestRestorableTime (typically up to the last few minutes). Option group associated with the DB snapshot is associated with the restored DB instance once it is created . However, option group is associated with the VPC, so would apply only when the instance is restored in the same VPC as the DB snapshot. Default DB parameter and security groups are associated with the restored instance . After the restoration is complete, any custom DB parameter or security groups used by the instance restored from should be associated explicitly. A DB instance can be restored with a different storage type than the source DB snapshot. In this case the restoration process will be slower because of the additional work required to migrate the data to the new storage type for e.g. from GP2 to Provisioned IOPS A DB instance can be restored with a different edition of the DB engine only if the DB snapshot has the required storage allocated for the new edition for e.g., to change from SQL Server Web Edition to SQL Server Standard Edition, the DB snapshot must have been created from a SQL Server DB instance that had at least 200 GB of allocated storage, which is the minimum allocated storage for SQL Server Standard edition 4-7 DB Snapshot Copy RDS supports two types of DB snapshot copying. Copy an automated DB snapshot to create a manual DB snapshot in the same AWS region. Manual DB snapshot are not deleted automatically and can be kept indefinitely. Copy either an automated or manual DB snapshot from one region to another region. By copying the DB snapshot to another region, a manual DB snapshot is created that is retained in that region Automated backups cannot be shared. They need to be copied to a manual snapshot, and the manual snapshot can be shared . Manual DB snapshots can be shared with other AWS accounts and copy DB snapshots shared to you by other AWS accounts Snapshot Copy Encryption DB snapshot that has been encrypted using an AWS Key Management System (AWS KMS) encryption key can be copied Copying an encrypted DB snapshot, results in an encrypted copy of the DB snapshot When copying, DB snapshot can either be encrypted with the same KMS encryption key as the original DB snapshot, or a different KMS encryption key to encrypt the copy of the DB snapshot. An unencrypted DB snapshot can be copied to an encrypted snapshot, a quick way to add encryption to a previously encrypted DB instance. Encrypted snapshot can be restored only to an encrypted DB instance If a KMS encryption key is specified when restoring from an unencrypted DB cluster snapshot, the restored DB cluster is encrypted using the specified KMS encryption key Copying an encrypted snapshot shared from another AWS account, requires access to the KMS encryption key that was used to encrypt the DB NOTE \u2013 AWS now allows copying encrypted DB snapshots between accounts and across multiple regions as seamlessly as unencrypted snapshots. Refer blog post 4-8 DB Snapshot Sharing Manual DB snapshot or DB cluster snapshot can be shared with up to 20 other AWS accounts . Manual snapshot shared with other AWS accounts can copy the snapshot, or restore a DB instance or DB cluster from that snapshot. Manual snapshot can also be shared as public, which makes the snapshot available to all AWS accounts. Care should be taken when sharing a snapshot as public so that none of the private information is included Shared snapshot can be copied to another region . However, following limitations apply when sharing manual snapshots with other AWS accounts: When a DB instance or DB cluster is restored from a shared snapshot using the AWS CLI or RDS API, the Amazon Resource Name (ARN) of the shared snapshot as the snapshot identifier should be specified DB snapshot that uses an option group with permanent or persistent options cannot be shared A permanent option cannot be removed from an option group. Option groups with persistent options cannot be removed from a DB instance once the option group has been assigned to the DB instance. DB snapshots that have been encrypted \u201cat rest\u201d using the AES-256 encryption algorithm can be shared Users can only copy encrypted DB snapshots if they have access to the AWS Key Management Service (AWS KMS) encryption key that was used to encrypt the DB snapshot. AWS KMS encryption keys can be shared with another AWS account by adding the other account to the KMS key policy. However, KMS key policy must first be updated by adding any accounts to share the snapshot with, before sharing an encrypted DB snapshot 4-9 AWS RDS DB Snapshot, Backup & Restore QAs Amazon RDS automated backups and DB Snapshots are currently supported for only the _ ___ storage engine InnoDB MyISAM Automated backups are enabled by default for a new DB Instance. TRUE FALSE Amazon RDS DB snapshots and automated backups are stored in Amazon S3 Amazon EBS Volume Amazon RDS Amazon EMR You receive a frantic call from a new DBA who accidentally dropped a table containing all your customers. Which Amazon RDS feature will allow you to reliably restore your database to within 5 minutes of when the mistake was made? Multi-AZ RDS RDS snapshots RDS read replicas RDS automated backup Disabling automated backups ______ disable the point-in-time recovery . if configured to can will never will Changes to the backup window take effect ______. from the next billing cycle after 30 minutes immediately after 24 hours You can modify the backup retention period; valid values are 0 (for no backup retention) to a maximum of _ ____ days. 45 35 15 5 Amazon RDS automated backups and DB Snapshots are currently supported for only the ______ storage engine MyISAM InnoDB What happens to the I/O operations while you take a database snapshot? I/O operations to the database are suspended for a few minutes while the backup is in progress . I/O operations to the database are sent to a Replica (if available) for a few minutes while the backup is in progress. I/O operations will be functioning normally I/O operations to the database are suspended for an hour while the backup is in progress True or False: When you perform a restore operation to a point in time or from a DB Snapshot, a new DB Instance is created with a new endpoint. FALSE TRUE True or False: Manually created DB Snapshots are deleted after the DB Instance is deleted. TRUE FALSE A user is running a MySQL RDS instance. The user will not use the DB for the next 3 months. How can the user save costs? Pause the RDS activities from CLI until it is required in the future Stop the RDS instance Create a snapshot of RDS to launch in the future and terminate the instance now Change the instance size to micro 5 AWS RDS Security 5-1 AWS RDS Security AWS provides multiple features to provide RDS security: DB instance can be hosted in a VPC for the greatest possible network access control IAM policies can be used to assign permissions that determine who is allowed to manage RDS resources Security groups allow to control what IP addresses or EC2 instances can connect to the databases on a DB instance Secure Socket Layer (SSL) connections with DB instances RDS encryption to secure RDS instances and snapshots at rest. Network encryption and transparent data encryption (TDE) with Oracle DB instances 5-2 RDS Authentication and Access Control IAM can be used to control which RDS operations each individual user has permission to call 5-3 Encrypting RDS Resources RDS encrypted instances use the industry standard AES-256 encryption algorithm to encrypt data on the server that hosts the RDS instance RDS handles authentication of access and decryption of the data with a minimal impact on performance , and with no need to modify the database client applications Data at Rest Encryption can be enabled on RDS instances to encrypt the underlying storage encryption keys are managed by KMS can be enabled only during instance creation once enabled, the encryption keys cannot be changed if the key is lost, the DB can only be restored from the backup Once encryption is enabled for an RDS instance, logs are encrypted snapshots are encrypted automated backups are encrypted read replicas are encrypted Encrypted snapshot from one AWS Region can be copied to another, by specifing the KMS key identifier of the destination AWS Region as KMS encryption keys are specific to the AWS Region that they are created in. RDS DB Snapshot considerations DB snapshot encrypted using an KMS encryption key can be copied Copying an encrypted DB snapshot , results in an encrypted copy of the DB snapshot When copying, DB snapshot can either be encrypted with the same KMS encryption key as the original DB snapshot, or a different KMS encryption key to encrypt the copy of the DB snapshot. An unencrypted DB snapshot can be copied to an encrypted snapshot, to add encryption to a previously unencrypted DB instance . Encrypted snapshot can be restored only to an encrypted DB instance If a KMS encryption key is specified when restoring from an unencrypted DB cluster snapshot, the restored DB cluster is encrypted using the specified KMS encryption key Copying an encrypted snapshot shared from another AWS account, requires access to the KMS encryption key used to encrypt the DB snapshot. Because KMS encryption keys are specific to the region that they are created in, encrypted snapshot cannot be copied to another region Transparent Data Encryption (TDE) Automatically encrypts the data before it is written to the underlying storage device and decrypts when it is read from the storage device is supported by Oracle and SQL Server Oracle requires key storage outside of the KMS and integrates with CloudHSM for this SQL Server requires a key but is managed by RDS 5-4 SSL to Encrypt a Connection to a DB Instance Encrypt connections using SSL for data in transit between the applications and the DB instance Amazon RDS creates an SSL certificate and installs the certificate on the DB instance when RDS provisions the instance . SSL certificates are signed by a certificate authority. SSL certificate includes the DB instance endpoint as the Common Name (CN) for the SSL certificate to guard against spoofing attacks While SSL offers security benefits, be aware that SSL encryption is a compute-intensive operation and will increase the latency of the database connection. 5-5 IAM Database Authentication IAM database authentication works with MySQL and PostgreSQL. IAM database authentication prevents the need to store static user credentials in the database, because authentication is managed externally using IAM. IAM database authentication does not require password, but needs an authentication token An authentication token is a unique string of characters that RDS generates on request. Authentication tokens are generated using AWS Signature Version 4. Each Authentication token has a lifetime of 15 minutes IAM database authentication provides the following benefits: Network traffic to and from the database is encrypted using Secure Sockets Layer (SSL). helps centrally manage access to the database resources, instead of managing access individually on each DB instance. enables using IAM Roles to access the database instead of a password, for greater security. 5-6 RDS Security Groups Security groups control the access that traffic has in and out of a DB instance VPC security groups act like a firewall controlling network access to your DB instance. VPC security groups can be configured and associated with the DB instance to allow access from an IP address range, port, or EC2 security group Database security groups default to a \u201cdeny all\u201d access mode and customers must specifically authorize network ingress. 5-7 RDS Rotating Secrets RDS supports AWS Secrets Manager to automatically rotate the secret Secrets Manager uses a Lambda function Secrets Manager provides. Secrets Manager provides following benefits Rotate secrets safely \u2013 rotate secrets automatically without disrupting the applications. Secrets Manager offers built-in integrations for rotating credentials for RDS databases for MySQL, PostgreSQL, and Aurora. Secrets Manager can be extended to meet custom rotation requirements by creating an Lambda function to rotate other types of secrets Manage secrets centrally \u2013 to store, view, and manage all the secrets. Security \u2013 By default, Secrets Manager encrypts these secrets with encryption keys that you own and control. Using fine-grained IAM policies, access to secrets can be controlled Monitor and audit easily \u2013 Secrets Manager integrates with AWS logging and monitoring services to enable meet your security and compliance requirements. Pay as you go \u2013 Pay for the secrets stored and for the use of these secrets; there are no long-term contracts or licensing fees. 5-8 Master User Account Privileges When you create a new DB instance, the default master user that used gets certain privileges for that DB instance Subsequently, other users with permissions can be created 5-9 Event Notification Event notifications can be configured for important events that occur on the DB instance Notifications of a variety of important events that can occur on the RDS instance, such as whether the instance was shut down, a backup was started, a failover occurred, the security group was changed, or your storage space is low can be received 5-10 RDS Encrypted DB Instances Limitations Encryption can be enabled only during creation of an RDS DB instance For migrating and unencrypted data \u2013 Encrypt a copy of an unencrypted DB snapshot , Create an encrypted copy of that snapshot . Restore a DB instance from the encrypted snapshot DB instances that are encrypted can\u2019t be modified to disable encryption. Unencrypted DB instance or an unencrypted read replica of an encrypted DB instance can\u2019t have an encrypted read replica Encrypted read replicas must be encrypted with the same CMK as the source DB instance when both are in the same AWS Region. Unencrypted backup or snapshot can\u2019t be restored to an encrypted DB instance. 5-11 QAs Can I encrypt connections between my application and my DB Instance using SSL? No Yes Only in VPC Only in certain regions Which of these configuration or deployment practices is a security risk for RDS? Storing SQL function code in plaintext Non-Multi-AZ RDS instance Having RDS and EC2 instances exist in the same subnet RDS in a public subnet (Making RDS accessible to the public internet in a public subnet poses a security risk, by making your database directly addressable and spammable. DB instances deployed within a VPC can be configured to be accessible from the Internet or from EC2 instances outside the VPC. If a VPC security group specifies a port access such as TCP port 22, you would not be able to access the DB instance because the firewall for the DB instance provides access only via the IP addresses specified by the DB security groups the instance is a member of and the port defined when the DB instance was created.) 6 AWS RDS DB Maintenance & Upgrades 6-1 RDS DB Instance Maintenance and Upgrades Changes to a DB instance can occur when a DB instance is manually modified for e.g. DB engine version is upgraded, or when Amazon RDS performs maintenance on an instance 6-2 Amazon RDS Maintenance Periodically, Amazon RDS performs maintenance on Amazon RDS resources, such as DB instances and most often involves updates to the DB instance\u2019s operating system (OS). Maintenance items can either be applied manually on a DB instance at ones convenience or wait for the automatic maintenance process initiated by Amazon RDS during the defined weekly maintenance window. Maintenance window only determines when pending operations start, but does not limit the total execution time of these operations. Maintenance operations are not guaranteed to finish before the maintenance window ends, and can continue beyond the specified end time. Maintenance update availability can be checked both on the RDS console and by using the RDS API. And if an update is available, one can Defer the maintenance items. Apply the maintenance items immediately. Schedule them to start during the next defined maintenance window Maintenance items marked as Required cannot be deferred indefinitely , if deferred AWS will send a notify the time when the update will be performed next Available and can be deferred indefinitely and the update will not be applied to the DB instance. Required patching is automatically scheduled only for patches that are related to security and instance reliability . Such patching occurs infrequently (typically once every few months) and seldom requires more than a fraction of your maintenance window. Maintenance items require that RDS take your DB instance offline for a short time. Maintenance that requires DB instance to be offline include scale compute operations, which generally take only a few minutes from start to finish, and required operating system or database patching. Multi-AZ deployment for the DB instance reduces the impact of a maintenance event by following these steps: * Perform maintenance on the standby . Promote the standby to primary . Perform maintenance on the old primary, which becomes the new standby . When database engine for the DB instance is modified in a Multi-AZ deployment, RDS upgrades both the primary and secondary DB instances at the same time. In this case, the database engine for the entire Multi-AZ deployment is shut down during the upgrade. 6-3 Operating System Updates Upgrades to the operating system are most often for security issues and should be done as soon as possible. OS updates on a DB instance can be applied at ones convenience or can wait for the maintenance process initiated by RDS to apply the update during the defined maintenance window DB instance is not automatically backed up when an OS update is applied, and should be backup up before the update is applied 6-4 Database Engine Version Upgrade DB instance engine version can be upgraded when a new DB engine version is supported by RDS. Database version upgrades consist of major and minor version upgrades. Major database version upgrades can contain changes that are not backward-compatible RDS doesn\u2019t apply major version upgrades automatically DB instance should be manually modified and thoroughly tested before applying it to the production instances. Minor version upgrades Each DB engine handles minor version upgrade slightly differently for e.g. RDS automatically apply minor version upgrades to a DB instance running PostgreSQL, but must be manually applied to a DB instance running Oracle. Amazon posts an announcement to the forums announcement page and sends a customer e-mail notification before upgrading an DB instance Amazon schedule the upgrades at specific times through the year, to help plan around them, because downtime is required to upgrade a DB engine version, even for Multi-AZ instances. RDS takes two DB snapshots during the upgrade process . First DB snapshot is of the DB instance before any upgrade changes have been made. If the upgrade fails, it can be restored from the snapshot to create a DB instance running the old version. Second DB snapshot is taken when the upgrade completes. After the upgrade is complete, database engine can\u2019t be reverted to the previous version. For returning to the previous version, restore the first DB snapshot taken to create a new DB instance. If the DB instance is using read replication, all of the Read Replicas must be upgraded before upgrading the source instance. If the DB instance is in a Multi-AZ deployment, both the primary and standby replicas are upgraded at the same time and would result in an outage. The time for the outage varies based on your database engine, version, and the size of your DB instance. 6-5 RDS Maintenance Window Every DB instance has a weekly maintenance window defined during which any system changes are applied. Maintenance window is an opportunity to control when DB instance modifications and software patching occur, in the event either are requested or required. If a maintenance event is scheduled for a given week, it will be initiated during the 30 minute maintenance window as defined Maintenance events mostly complete during the 30 minute maintenance window, although larger maintenance events may take more time 30-minute maintenance window is selected at random from an 8-hour block of time per region. If you don\u2019t specify a preferred maintenance window when you create the DB instance, Amazon RDS assigns a 30-minute maintenance window on a randomly selected day of the week. RDS will consume some of the resources on the DB instance while maintenance is being applied, minimally effecting performance. For some maintenance events, a Multi-AZ failover may be required for a maintenance update to complete. 6-6 AWS RDS DB Maintenance & Upgrades QAs A user has launched an RDS MySQL DB with the Multi AZ feature. The user has scheduled the scaling of instance storage during maintenance window. What is the correct order of events during maintenance window? 1. Perform maintenance on standby 2. Promote standby to primary 3. Perform maintenance on original primary 4. Promote original master back as primary 1, 2, 3, 4 1, 2, 3 2, 3, 4, 1 Can I control if and when MySQL based RDS Instance is upgraded to new supported versions? No Only in VPC Yes A user has scheduled the maintenance window of an RDS DB on Monday at 3 AM. Which of the below mentioned events may force to take the DB instance offline during the maintenance window? Enabling Read Replica Making the DB Multi AZ DB password change Security patching A user has launched an RDS postgreSQL DB with AWS. The user did not specify the maintenance window during creation. The user has configured RDS to update the DB instance type from micro to large. If the user wants to have it during the maintenance window, what will AWS do? AWS will not allow to update the DB until the maintenance window is configured AWS will select the default maintenance window if the user has not provided it AWS will ask the user to specify the maintenance window during the update It is not possible to change the DB size from micro to large with RDS Can I test my DB Instance against a new version before upgrading? No Yes Only in VPC 7 AWS RDS Monitoring & Notification 7-1 AWS RDS Monitoring & Notification RDS integrates with CloudWatch and provides metrics for monitoring CloudWatch alarms can be created over a single metric that sends an SNS message when the alarm changes state RDS also provides SNS notification whenever any RDS event occurs 7-2 RDS CloudWatch Monitoring RDS DB instance can be monitored using CloudWatch, which collects and processes raw data from RDS into readable, near real-time metrics. Statistics are recorded for a period of two weeks, so that you can access historical information and gain a better perspective on how the service is performing. By default, RDS metric data is automatically sent to CloudWatch in 1-minute periods CloudWatch RDS Metrics BinLogDiskUsage \u2013 Amount of disk space occupied by binary logs on the master. Applies to MySQL read replicas. CPUUtilization \u2013 Percentage of CPU utilization. DatabaseConnections \u2013 Number of database connections in use. DiskQueueDepth \u2013 The number of outstanding IOs (read/write requests) waiting to access the disk. FreeableMemory \u2013 Amount of available random access memory. FreeStorageSpace \u2013 Amount of available storage space. ReplicaLag \u2013 Amount of time a Read Replica DB instance lags behind the source DB instance. SwapUsage \u2013 Amount of swap space used on the DB instance. ReadIOPS \u2013 Average number of disk I/O operations per second. WriteIOPS \u2013 Average number of disk I/O operations per second. ReadLatency \u2013 Average amount of time taken per disk I/O operation. WriteLatency \u2013 Average amount of time taken per disk I/O operation. ReadThroughput \u2013 Average number of bytes read from disk per second. WriteThroughput \u2013 Average number of bytes written to disk per second. NetworkReceiveThroughput \u2013 Incoming (Receive) network traffic on the DB instance, including both customer database traffic and Amazon RDS traffic used for monitoring and replication. NetworkTransmitThroughput \u2013 Outgoing (Transmit) network traffic on the DB instance, including both customer database traffic and Amazon RDS traffic used for monitoring and replication. 7-3 RDS Enhanced Monitoring RDS provides metrics in real time for the operating system (OS) that the DB instance runs on. By default, Enhanced Monitoring metrics are stored for 30 days in the CloudWatch Logs, which are different from typical CloudWatch metrics. 7-4 CloudWatch vs Enhanced Monitoring Metrics CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance, and Enhanced Monitoring gathers its metrics from an agent on the instance. Enhanced Monitoring metrics are useful to understand how different processes or threads on a DB instance use the CPU. There might be differences between the measurements, because the hypervisor layer performs a small amount of work. The differences can be greater if the DB instances use smaller instance classes, because then there are likely more virtual machines (VMs) that are managed by the hypervisor layer on a single physical instance. 7-5 RDS Performance Insights Performance Insights is a database performance tuning and monitoring feature that helps illustrate the database\u2019s performance and help analyze any issues that affect it With the Performance Insights dashboard, you can visualize the database load and filter the load by waits, SQL statements, hosts, or users. 7-6 RDS CloudTrail Logs CloudTrail provides a record of actions taken by a user, role, or an AWS service in RDS. CloudTrail captures all API calls for RDS as events, including calls from the console and from code calls to RDS API operations. CloudTrail can help determine the request that was made to RDS, the IP address from which the request was made, who made the request, when it was made, and additional details. 7-7 RDS Recommendations RDS provides automated recommendations for database resources. The recommendations provide best practice guidance by analyzing DB instance configuration, usage, and performance data. 7-8 RDS Event Notification RDS uses the SNS to provide notification when an RDS event occurs RDS groups the events into categories, which can be subscribed so that a notification is sent when an event in that category occurs. Event category for a DB instance, DB cluster, DB snapshot, DB cluster snapshot, DB security group or for a DB parameter group can be subscribed Event notifications are sent to the email addresses provided during subscription creation Subscription can be easily turn off notification without deleting a subscription by setting the Enabled radio button to No in the RDS console or by setting the Enabled parameter to false using the CLI or RDS API. 7-9 RDS Trusted Advisor Trusted Advisor inspects the AWS environment and then makes recommendations when opportunities exist to save money, improve system availability and performance, or help close security gaps. Trusted Advisor has the following RDS-related checks: RDS Idle DB Instances RDS Security Group Access Risk RDS Backups RDS Multi-AZ 7-10 AWS RDS Monitoring & Notification QAs You run a web application with the following components Elastic Load Balancer (ELB), 3 Web/Application servers, 1 MySQL RDS database with read replicas, and Amazon Simple Storage Service (Amazon S3) for static content. Average response time for users is increasing slowly. What three CloudWatch RDS metrics will allow you to identify if the database is the bottleneck? Choose 3 answers The number of outstanding IOs waiting to access the disk The amount of write latency The amount of disk space occupied by binary logs on the master. The amount of time a Read Replica DB Instance lags behind the source DB Instance The average number of disk I/O operations per second. Typically, you want your application to check whether a request generated an error before you spend any time processing results. The easiest way to find out if an error occurred is to look for an _ ___ node in the response from the Amazon RDS API. Incorrect Error FALSE In the Amazon CloudWatch, which metric should I be checking to ensure that your DB Instance has enough free storage space? FreeStorage FreeStorageSpace FreeStorageVolume FreeDBStorageSpace A user is receiving a notification from the RDS DB whenever there is a change in the DB security group. The user does not want to receive these notifications for only a month. Thus, he does not want to delete the notification. How can the user configure this? Change the Disable button for notification to \u201cYes\u201d in the RDS console Set the send mail flag to false in the DB event notification console The only option is to delete the notification from the console Change the Enable button for notification to \u201cNo\u201d in the RDS console A sys admin is planning to subscribe to the RDS event notifications. For which of the below mentioned source categories the subscription cannot be configured? DB security group DB snapshot DB options group DB parameter group A user is planning to setup notifications on the RDS DB for a snapshot. Which of the below mentioned event categories is not supported by RDS for this snapshot source type? Backup Creation Deletion Restoration A system admin is planning to setup event notifications on RDS. Which of the below mentioned services will help the admin setup notifications? AWS SES AWS Cloudtrail AWS CloudWatch AWS SNS A user has setup an RDS DB with Oracle. The user wants to get notifications when someone modifies the security group of that DB. How can the user configure that? It is not possible to get the notifications on a change in the security group Configure SNS to monitor security group changes Configure event notification on the DB security group Configure the CloudWatch alarm on the DB for a change in the security group It is advised that you watch the Amazon CloudWatch \u201c_____\u201d metric (available via the AWS Management Console or Amazon Cloud Watch APIs) carefully and recreate the Read Replica should it fall behind due to replication errors. Write Lag Read Replica Replica Lag Single Replica 8 AWS RDS Best Practices 8-1 AWS RDS Best Practices AWS recommends RDS best practices in terms of Monitoring, Performance and security 8-2 Amazon RDS Basic Operational Guidelines Monitoring Memory, CPU, and storage usage should be monitored. CloudWatch can be setup for notifications when usage patterns change or when the capacity of deployment is approached, so that system performance and availability can be maintained Scaling Scale up the DB instance when approaching storage capacity limits. There should be some buffer in storage and memory to accommodate unforeseen increases in demand from the applications. Backups Enable Automatic Backups and set the backup window to occur during the daily low in WriteIOPS. On a MySQL DB instance , Do not create more than 10,000 tables using Provisioned IOPS or 1000 tables using standard storage. Large numbers of tables will significantly increase database recovery time after a failover or database crash. If you need to create more tables than recommended, set the innodb_file_per_table parameter to 0. Avoid tables in the database growing too large. Provisioned storage limits restrict the maximum size of a MySQL table file to 6 TB. Instead, partition the large tables so that file sizes are well under the 6 TB limit. This can also improve performance and recovery time. Performance If the database workload requires more I/O than provisioned, recovery after a failover or database failure will be slow. To increase the I/O capacity of a DB instance, Migrate to a DB instance class with High I/O capacity. Convert from standard storage to Provisioned IOPS storage, and use a DB instance class that is optimized for Provisioned IOPS. if using Provisioned IOPS storage, provision additional throughput capacity. Multi-AZ & Failover Deploy applications in all Availability Zones, so if an AZ goes down, applications in other AZs will still be available. Use Amazon RDS DB events to monitor failovers. Set a TTL of less than 30 seconds, if the client application is caching the DNS data of the DB instances. As the underlying IP address of a DB instance can change after a failover, caching the DNS data for an extended time can lead to connection failures if the application tries to connect to an IP address that no longer is in service. Multi-AZ requires transaction logging feature to be enabled. Do not use features like Simple recover mode, offline mode or Read-only mode which turn of transaction logging. To shorten failover time Ensure that sufficient Provisioned IOPS allocated for your workload. Inadequate I/O can lengthen failover times. Database recovery requires I/O. Use smaller transactions. Database recovery relies on transactions, so break up large transactions into multiple smaller transactions to shorten failover time Test failover for your DB instance to understand how long the process takes for your use case and to ensure that the application that accesses your DB instance can automatically connect to the new DB instance after failover. 8-3 DB Instance RAM Recommendations An Amazon RDS performance best practice is to allocate enough RAM so that the working set resides almost completely in memory. Value of ReadIOPS should be small and stable. ReadIOPS metric can be checked, using AWS CloudWatch while the DB instance is under load, to tell if the working set is almost all in memory If scaling up the DB instance class with more RAM, results in a dramatic drop in ReadIOPS, the working set was not almost completely in memory. Continue to scale up until ReadIOPS no longer drops dramatically after a scaling operation, or ReadIOPS is reduced to a very small amount. 8-4 RDS Security Best Practices Do not use AWS root credentials to manage Amazon RDS resources; and IAM users should be created for everyone, Grant each user the minimum set of permissions required to perform his or her duties. Use IAM groups to effectively manage permissions for multiple users. Rotate your IAM credentials regularly . 8-5 Using Enhanced Monitoring to Identify Operating System Issues Amazon RDS provides metrics in real time for the operating system (OS) that your DB instance runs on. Enhanced monitoring is available for all DB instance classes except for db.t1.micro and db.m1.small. Using Metrics to Identify Performance Issues To identify performance issues caused by insufficient resources and other common bottlenecks, you can monitor the metrics available for your Amazon RDS DB instance Performance metrics should be monitored on a regular basis to benchmark the average, maximum, and minimum values for a variety of time ranges. to help identify performance degradation. Amazon CloudWatch alarms can be set for particular metric thresholds to be alerted when they are reached or breached A DB instance has a number of different categories of metrics which includes CPU, memory, disk space, IOPS, db connections and network traffic, and how to determine acceptable values depends on the metric. One of the best ways to improve DB instance performance is to tune the most commonly used and most resource-intensive queries to make them less expensive to run. 8-6 Recovery MySQL InnoDB is the recommended and supported storage engine for MySQL DB instances on Amazon RDS. However, MyISAM performs better than InnoDB if you require intense, full-text search capability. Point-In-Time Restore and snapshot restore features of Amazon RDS for MySQL require a crash-recoverable storage engine and are supported for the InnoDB storage engine only. Although MySQL supports multiple storage engines with varying capabilities, not all of them are optimized for crash recovery and data durability. MyISAM storage engine does not support reliable crash recovery and might prevent a Point-In-Time Restore or snapshot restore from working as intended which might result in lost or corrupt data when MySQL is restarted after a crash. MariaDB XtraDB is the recommended and supported storage engine for MariaDB DB instances on Amazon RDS. Point-In-Time Restore and snapshot restore features of Amazon RDS for MariaDB require a crash-recoverable storage engine and are supported for the XtraDB storage engine only. Although MariaDB supports multiple storage engines with varying capabilities, not all of them are optimized for crash recovery and data durability. For e.g although Aria is a crash-safe replacement for MyISAM, it might still prevent a Point-In-Time Restore or snapshot restore from working as intended. This might result in lost or corrupt data when MariaDB is restarted after a crash. 8-7 QAs You are running a database on an EC2 instance, with the data stored on Elastic Block Store (EBS) for persistence At times throughout the day, you are seeing large variance in the response times of the database queries Looking into the instance with the isolate command you see a lot of wait time on the disk volume that the database\u2019s data is stored on. What two ways can you improve the performance of the database\u2019s storage while maintaining the current persistence of the data? Choose 2 answers Move to an SSD backed instance Move the database to an EBS-Optimized Instance Use Provisioned IOPs EBS Use the ephemeral storage on an m2.4xLarge Instance Instead Amazon RDS automated backups and DB Snapshots are currently supported for only the _ ___ storage engine InnoDB MyISAM","title":"L1 AWS RDS"},{"location":"chap7/1rds/#l1-aws-rds","text":"","title":"L1 AWS RDS"},{"location":"chap7/1rds/#1-intro","text":"","title":"1 Intro"},{"location":"chap7/1rds/#1-1-relational-database-service-rds","text":"Relational Database Service (RDS) is a web service that makes it easier to set up, operate, and scale a relational database in the cloud. RDS provides cost-efficient, resizable capacity for an industry-standard relational database and manages common database administration tasks such as hardware provisioning, database setup, patching, and backups . RDS features & benefits CPU, memory, storage, and IOPs can be scaled independently. manages backups, software patching, automatic failure detection, and recovery. automated backups can be performed as needed, or manual backups can be triggered as well. Backups can be used to restore a database, and the RDS restore process works reliably and efficiently. provides high availability with a primary instance and a synchronous standby secondary instance that can be failover to seamlessly when a problem occurs. provides elasticity & scalability by enabling Read Replicas to increase read scaling. supports MySQL, MariaDB, PostgreSQL, Oracle, Microsoft SQL Server, and the new, MySQL-compatible Aurora DB engine supports IAM users and permissions to control who has access to the RDS database service databases can be further protected by putting them in a VPC , using SSL for data in transit and encryption for data in rest However, as it is a managed service, shell (root ssh) access to DB instances is not provided, and this restricts access to certain system procedures and tables that require advanced privileges.","title":"1-1 Relational Database Service \u2013 RDS"},{"location":"chap7/1rds/#1-2-rds-components","text":"DB Instance is a basic building block of RDS is an isolated database environment in the cloud each DB instance runs a DB engine. AWS currently supports MySQL, MariaDB, PostgreSQL, Oracle, and Microsoft SQL Server & Aurora DB engines can be accessed from AWS command-line tools, RDS APIs, or the AWS Management RDS Console. computation and memory capacity of a DB instance is determined by its DB instance class, which can be selected as per the needs supports three storage types: Magnetic, General Purpose (SSD), and Provisioned IOPS (SSD), which differ in performance and price each DB instance has a DB instance identifier, which is a customer-supplied name and must be unique for that customer in an AWS region. It uniquely identifies the DB instance when interacting with the RDS API and AWS CLI commands. each DB instance can host multiple user-created databases or a single Oracle database with multiple schemas. can be hosted in an AWS VPC environment for better control Regions and Availability Zones AWS resources are housed in highly available data center facilities in different areas of the world, these data centers are called regions which further contain multiple distinct locations called Availability Zones Each AZ is engineered to be i solated from failures in other AZs and to provide inexpensive, low-latency network connectivity to other AZs in the same region DB instances can be hosted in different AZs, an option called a Multi-AZ deployment. RDS automatically provisions and maintains a synchronous standby replica of the DB instance in a different AZ. Primary DB instance is synchronously replicated across AZs to the standby replica Provides data redundancy, failover support, eliminates I/O freezes, and minimizes latency spikes during system backups. Security Groups security group controls the access to a DB instance, by allowing access to the specified IP address ranges or EC2 instances DB Parameter Groups A DB parameter group contains engine configuration values that can be applied to one or more DB instances of the same instance type help define configuration values specific to the selected DB Engine for e.g. max_connections , force_ssl , autocommit supports default parameter group, which cannot be edited. supports custom parameter group, to override values supports static and dynamic parameter groups changes to dynamic parameters are applied immediately (irrespective of apply immediately setting) changes to static parameters are NOT applied immediately and require a manual reboot. DB Option Groups Some DB engines offer tools or optional features that simplify managing the databases and making the best use of data. RDS makes such tools available through option groups for e.g. Oracle Application Express (APEX), SQL Server Transparent Data Encryption, and MySQL Memcached support.","title":"1-2 RDS Components"},{"location":"chap7/1rds/#1-3-rds-interfaces","text":"RDS can be interacted with multiple interfaces AWS RDS Management console Command Line Interface Programmatic Interfaces which include SDKs, libraries in different languages, and RDS API","title":"1-3 RDS Interfaces"},{"location":"chap7/1rds/#1-4-rds-pricing","text":"Instance class Pricing is based on the class (e.g., micro, small, large, xlarge) of the DB instance consumed. Running time Usage is billed in one-second increments, with a minimum of 10 minutes Storage Storage capacity provisioned for the DB instance is billed per GB per month If the provisioned storage capacity is scaled within the month, the bill will be pro-rated. I/O requests per month Total number of storage I/O requests made in a billing cycle. Provisioned IOPS (per IOPS per month) Provisioned IOPS rate, regardless of IOPS consumed, for RDS Provisioned IOPS (SSD) storage only. Provisioned storage for EBS volumes is billed in one-second increments, with a minimum of 10 minutes. Backup storag e Automated backups & any active database snapshots consume storage Increasing backup retention period or taking additional database snapshots increases the backup storage consumed by the database. RDS provides backup storage up to 100% of the provisioned database storage at no additional charge for e.g., if you have 10 GB-months of provisioned database storage, RDS provides up to 10 GB-months of backup storage at no additional charge. Most databases require less raw storage for a backup than for the primary dataset, so if multiple backups are not maintained, you will never pay for backup storage. Backup storage is free only for active DB instances. Data transfer Internet data transfer in and out of your DB instance. Reserved Instances In addition to regular RDS pricing, reserved DB instances can be purchased","title":"1-4 RDS Pricing"},{"location":"chap7/1rds/#1-5-qa","text":"What does Amazon RDS stand for? Regional Data Server. Relational Database Service Regional Database Service. How many relational database engines does RDS currently support? MySQL, Postgres, MariaDB, Oracle, and Microsoft SQL Server Just two: MySQL and Oracle. Five: MySQL, PostgreSQL, MongoDB, Cassandra and SQLite. Just one: MySQL. If I modify a DB Instance or the DB parameter group associated with the instance, should I reboot the instance for the changes to take effect? No Yes What is the name of licensing model in which I can use your existing Oracle Database licenses to run Oracle deployments on Amazon RDS? Bring Your Own License Role Bases License Enterprise License License Included Will I be charged if the DB instance is idle? No Yes Only is running in GovCloud Only if running in VPC What is the minimum charge for the data transferred between Amazon RDS and Amazon EC2 Instances in the same Availability Zone ? USD 0.10 per GB No charge. It is free. USD 0.02 per GB USD 0.01 per GB Does Amazon RDS allow direct host access via Telnet, Secure Shell (SSH), or Windows Remote Desktop Connection? Yes No Depends on if it is in VPC or not What are the two types of licensing options available for using Amazon RDS for Oracle? BYOL and Enterprise License BYOL and License Included Enterprise License and License Included Role based License and License Included A user plans to use RDS as a managed DB platform. Which of the below mentioned features is not supported by RDS? Automated backup Automated scaling to manage a higher load Automated failure detection and recovery Automated software patching A user is launching an AWS RDS with MySQL. Which of the below mentioned options allows the user to configure the InnoDB engine parameters? Options group Engine parameters Parameter groups DB parameters A user is planning to use the AWS RDS with MySQL. Which of the below mentioned services the user is not going to pay? Data transfer RDS CloudWatch metrics Data storage I/O requests per month","title":"1-5 QA"},{"location":"chap7/1rds/#2-aws-rds-replication-multi-az-vs-read-replica","text":"","title":"2 AWS RDS Replication \u2013 Multi-AZ vs Read Replica"},{"location":"chap7/1rds/#2-1-rds-multi-az-vs-read-replica","text":"DB instances replicas can be created in two ways Multi-AZ & Read Replica, which provide high availability, durability, and scalability to RDS.","title":"2-1 RDS Multi-AZ vs Read Replica"},{"location":"chap7/1rds/#2-2-multi-az-deployments","text":"RDS Multi-AZ deployment provides high availability, durability, and failover support RDS automatically provisions and manages a synchronous standby instance in a different AZ (independent infrastructure in a physically separate location) RDS automatically fails over to the standby so that database operations can resume quickly without administrative intervention in case of Planned database maintenance Software patching Rebooting of the Primary instance Primary DB instance connectivity or host failure, or an Availability Zone failure RDS maintains the same endpoint for the DB Instance after a failover, so the application can resume database operation without the need for manual administrative intervention.","title":"2-2 Multi-AZ deployments"},{"location":"chap7/1rds/#2-3-rds-read-replicas","text":"Read replicas enable increased scalability and database availability in the case of an AZ failure. Read Replicas allow elastic scaling beyond the capacity constraints of a single DB instance for read-heavy database workloads RDS read replicas can be Multi-AZ i.e. set up with their own standby instances in a different AZ . Load on the source DB instance can be reduced by routing read queries from applications to the Read Replica. one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. RDS uses DB engines\u2019 built-in replication functionality to create a special type of DB instance called a Read Replica from a source DB instance. It uses the engines\u2019 native asynchronous replication to update the read replica whenever there is a change to the source DB instance. Read replicas are available in RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server as well as Aurora.","title":"2-3 RDS Read Replicas"},{"location":"chap7/1rds/#2-4-qas","text":"You are running a successful multi-tier web application on AWS and your marketing department has asked you to add a reporting tier to the application. The reporting tier will aggregate and publish status reports every 30 minutes from user-generated information that is being stored in your web applications database. You are currently running a Multi-AZ RDS MySQL instance for the database tier. You also have implemented ElastiCache as a database caching layer between the application tier and database tier. Please select the answer that will allow you to successfully implement the reporting tier with as little impact as possible to your database. Continually send transaction logs from your master database to an S3 bucket and generate the reports of the S3 bucket using S3 byte range requests. Generate the reports by querying the synchronously replicated standby RDS MySQL instance maintained through Multi-AZ (Standby instance cannot be used as a scaling solution) Launch an RDS Read Replica connected to your Multi-AZ master database and generate reports by querying the Read Replica. Generate the reports by querying the ElastiCache database caching tier. (ElasticCache does not maintain full data and is simply a caching solution) A company is deploying a new two-tier web application in AWS. The company has limited staff and requires high availability, and the application requires complex queries and table joins. Which configuration provides the solution for the company\u2019s requirements? MySQL Installed on two Amazon EC2 Instances in a single Availability Zone (does not provide High Availability out of the box) Amazon RDS for MySQL with Multi-AZ Amazon ElastiCache (Just a caching solution) Amazon DynamoDB (Not suitable for complex queries and joins) Your company is getting ready to do a major public announcement of a social media site on AWS. The website is running on EC2 instances deployed across multiple Availability Zones with a Multi-AZ RDS MySQL Extra Large DB Instance. The site performs a high number of small reads and writes per second and relies on an eventual consistency model. After comprehensive tests you discover that there is read contention on RDS MySQL. Which are the best approaches to meet these requirements? (Choose 2 answers) Deploy ElastiCache in-memory cache running in each availability zone Implement sharding to distribute load to multiple RDS MySQL instances (this is only a read contention, the writes work fine) Increase the RDS MySQL Instance size and Implement provisioned IOPS (not scalable, this is only a read contention, the writes work fine) Add an RDS MySQL read replica in each availability zone Your company has HQ in Tokyo and branch offices all over the world and is using logistics software with a multi-regional deployment on AWS in Japan, Europe and US. The logistic software has a 3-tier architecture and currently uses MySQL 5.6 for data persistence. Each region has deployed its own database. In the HQ region you run an hourly batch process reading data from every region to compute cross-regional reports that are sent by email to all offices this batch process must be completed as fast as possible to quickly optimize logistics. How do you build the database architecture in order to meet the requirements? For each regional deployment, use RDS MySQL with a master in the region and a read replica in the HQ region For each regional deployment, use MySQL on EC2 with a master in the region and send hourly EBS snapshots to the HQ region For each regional deployment, use RDS MySQL with a master in the region and send hourly RDS snapshots to the HQ region For each regional deployment, use MySQL on EC2 with a master in the region and use S3 to copy data files hourly to the HQ region Use Direct Connect to connect all regional MySQL deployments to the HQ region and reduce network latency for the batch process What would happen to an RDS (Relational Database Service) Multi-Availability Zone deployment if the primary DB instance fails? IP of the primary DB Instance is switched to the standby DB Instance. A new DB instance is created in the standby availability zone. The canonical name record (CNAME) is changed from primary to standby. The RDS (Relational Database Service) DB instance reboots. Your business is building a new application that will store its entire customer database on a RDS MySQL database, and will have various applications and users that will query that data for different purposes. Large analytics jobs on the database are likely to cause other applications to not be able to get the query results they need to, before time out. Also, as your data grows, these analytics jobs will start to take more time, increasing the negative effect on the other applications. How do you solve the contention issues between these different workloads on the same data? Enable Multi-AZ mode on the RDS instance Use ElastiCache to offload the analytics job data Create RDS Read-Replicas for the analytics work Run the RDS instance on the largest size possible Will my standby RDS instance be in the same Availability Zone as my primary? Only for Oracle RDS types Yes Only if configured at launch No A user is planning to set up the Multi-AZ feature of RDS. Which of the below mentioned conditions won\u2019t take advantage of the Multi-AZ feature? Availability zone outage A manual failover of the DB instance using Reboot with failover option Region outage When the user changes the DB instance\u2019s server type When you run a DB Instance as a Multi-AZ deployment, the \u201c_____\u201d serves database writes and reads secondary backup stand by primary When running my DB Instance as a Multi-AZ deployment, can I use the standby for read or write operations? Yes Only with MSSQL based RDS Only for Oracle RDS instances No Read Replicas require a transactional storage engine and are only supported for the _ __ storage engine OracleISAM MSSQLDB InnoDB MyISAM A user is configuring the Multi-AZ feature of an RDS DB. The user came to know that this RDS DB does not use the AWS technology, but uses server mirroring to achieve replication. Which DB is the user using right now? MySQL Oracle MS SQL PostgreSQL If I have multiple Read Replicas for my master DB Instance and I promote one of them, what happens to the rest of the Read Replicas? The remaining Read Replicas will still replicate from the older master DB Instance The remaining Read Replicas will be deleted The remaining Read Replicas will be combined to one read replica If you have chosen Multi-AZ deployment, in the event of a planned or unplanned outage of your primary DB Instance, Amazon RDS automatically switches to the standby replica. The automatic failover mechanism simply changes the ______ record of the main DB Instance to point to the standby DB Instance. DNAME CNAME TXT MX When automatic failover occurs, Amazon RDS will emit a DB Instance event to inform you that automatic failover occurred. You can use the _____ to return information about events related to your DB Instance FetchFailure DescriveFailure DescribeEvents FetchEvents The new DB Instance that is created when you promote a Read Replica retains the backup window period. TRUE FALSE Will I be alerted when automatic failover occurs? Only if SNS configured No Yes 1Only if Cloudwatch configured Can I initiate a \u201cforced failover\u201d for my MySQL Multi-AZ DB Instance deployment? Only in certain regions Only in VPC Yes No A user is accessing RDS from an application. The user has enabled the Multi-AZ feature with the MS SQL RDS DB. During a planned outage how will AWS ensure that a switch from DB to a standby replica will not affect access to the application ? RDS will have an internal IP which will redirect all requests to the new DB RDS uses DNS to switch over to standby replica for seamless transition The switch over changes Hardware so RDS does not need to worry about access RDS will have both the DBs running independently and the user has to manually switch over Which of the following is part of the failover process for a Multi-AZ Amazon Relational Database Service (RDS) instance? The failed RDS DB instance reboots. The IP of the primary DB instance is switched to the standby DB instance. The DNS record for the RDS endpoint is changed from primary to standby . A new DB instance is created in the standby availability zone. Which of these is not a reason a Multi-AZ RDS instance will failover? An Availability Zone outage A manual failover of the DB instance was initiated using Reboot with failover To autoscale to a higher instance class Master database corruption occurs The primary DB instance fails You need to scale an RDS deployment. You are operating at 10% writes and 90% reads, based on your logging. How best can you scale this in a simple way? Create a second master RDS instance and peer the RDS groups. Cache all the database responses on the read side with CloudFront. Create read replicas for RDS since the load is mostly reads . Create a Multi-AZ RDS installs and route read traffic to standby. How does Amazon RDS multi Availability Zone model work? A second, standby database is deployed and maintained in a different availability zone from master, using synchronous replication. A second, standby database is deployed and maintained in a different availability zone from master using asynchronous replication. A second, standby database is deployed and maintained in a different region from master using asynchronous replication. A second, standby database is deployed and maintained in a different region from master using synchronous replication. A customer is running an application in US-West (Northern California) region and wants to setup disaster recovery failover to the Asian Pacific (Singapore) region. The customer is interested in achieving a low Recovery Point Objective (RPO) for an Amazon RDS multi-AZ MySQL database instance. Which approach is best suited to this need? Synchronous replication Asynchronous replication Route53 health checks Copying of RDS incremental snapshots A user is using a small MySQL RDS DB. The user is experiencing high latency due to the Multi AZ feature. Which of the below mentioned options may not help the user in this situation? Schedule the automated back up in non-working hours Use a large or higher size instance Use PIOPS Take a snapshot from standby Replica Are Reserved Instances available for Multi-AZ Deployments? Only for Cluster Compute instances Yes for all instance types Only for M3 instance types My Read Replica appears \u201cstuck\u201d after a Multi-AZ failover and is unable to obtain or apply updates from the source DB Instance. What do I do? You will need to delete the Read Replica and create a new one to replace it. You will need to disassociate the DB Engine and re-associate it. The instance should be deployed to Single AZ and then moved to Multi-AZ once again You will need to delete the DB Instance and create a new one to replace it. What is the charge for the data transfer incurred in replicating data between your primary and standby? No charge. It is free . Double the standard data transfer charge Same as the standard data transfer charge Half of the standard data transfer charge A user has enabled the Multi-AZ feature with the MS SQL RDS database server. Which of the below mentioned statements will help the user understand the Multi-AZ feature better? In a Multi-AZ, AWS runs two DBs in parallel and copies the data asynchronously to the replica copy In a Multi-AZ, AWS runs two DBs in parallel and copies the data synchronously to the replica copy In a Multi-AZ, AWS runs just one DB but copies the data synchronously to the standby replic a AWS MS SQL does not support the Multi-AZ feature A company is running a batch analysis every hour on their main transactional DB running on an RDS MySQL instance to populate their central Data Warehouse running on Redshift. During the execution of the batch their transactional applications are very slow. When the batch completes they need to update the top management dashboard with the new data. The dashboard is produced by another system running on-premises that is currently started when a manually sent email notifies that an update is required The on-premises system cannot be modified because is managed by another team. How would you optimize this scenario to solve performance issues and automate the process as much as possible? Replace RDS with Redshift for the batch analysis and SNS to notify the on-premises system to update the dashboard Replace RDS with Redshift for the batch analysis and SQS to send a message to the on-premises system to update the dashboard Create an RDS Read Replica for the batch analysis and SNS to notify me on-premises system to update the dashboard Create an RDS Read Replica for the batch analysis and SQS to send a message to the on-premises system to update the dashboard.","title":"2-4 QAs"},{"location":"chap7/1rds/#3-aws-rds-storage-certification","text":"","title":"3 AWS RDS Storage \u2013 Certification"},{"location":"chap7/1rds/#3-1-aws-rds-storage","text":"RDS storage uses Elastic Block Store (EBS) volumes for database and log storage. RDS automatically stripes across multiple EBS volumes to enhance IOPS performance, depending on the amount of storage requested","title":"3-1 AWS RDS Storage"},{"location":"chap7/1rds/#3-2-rds-storage-types","text":"RDS storage provides three storage types: Magnetic, General Purpose (SSD), and Provisioned IOPS (input/output operations per second) . These storage types differ in performance characteristics and price, which allows tailoring of storage performance and cost to the database needs MySQL, MariaDB, PostgreSQL, and Oracle RDS DB instances can be created with up to 6TB of storage and SQL Server RDS DB instances with up to 4TB of storage when using the Provisioned IOPS and General Purpose (SSD) storage types. Existing MySQL, PostgreSQL, and Oracle RDS database instances can be scaled to these new database storage limits without any downtime.","title":"3-2 RDS Storage Types"},{"location":"chap7/1rds/#magnetic-standard","text":"Magnetic storage, also called standard storage, offers cost-effective storage that is ideal for applications with light or burst I/O requirements. They deliver approximately 100 IOPS on average, with burst capability of up to hundreds of IOPS, and they can range in size from 5 GB to 3 TB, depending on the DB instance engine. Magnetic storage is not reserved for a single DB instance, so performance can vary greatly depending on the demands placed on shared resources by other customers.","title":"Magnetic (Standard)"},{"location":"chap7/1rds/#mgeneral-purpose-ssd","text":"General purpose, SSD-backed storage, also called gp2, can provide faster access than disk-based storage. They can deliver single-digit millisecond latencies, with a base performance of 3 IOPS per Gigabyte (GB) and the ability to burst to 3,000 IOPS for extended periods of time up to a maximum of 10,000 PIOPS. Gp2 volumes can range in size from 5 GB to 6 TB for MySQL, MariaDB, PostgreSQL, and Oracle DB instances, and from 20 GB to 4 TB for SQL Server DB instances. Gp2 is excellent for small to medium-sized databases.","title":"MGeneral Purpose (SSD)"},{"location":"chap7/1rds/#provisioned-iops","text":"Provisioned IOPS storage is designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency in random access I/O throughput. Provisioned IOPS storage is a storage type that delivers fast, predictable, and consistent throughput performance. For any production application that requires fast and consistent I/O performance, Amazon recommends Provisioned IOPS (input/output operations per second) storage. Provisioned IOPS storage is optimized for I/O intensive, online transaction processing (OLTP) workloads that have consistent performance requirements. Provisioned IOPS helps performance tuning. Provisioned IOPS volumes can range in size from 100 GB to 6 TB for MySQL, MariaDB, PostgreSQL, and Oracle DB engines. SQL Server Express and Web editions can range in size from 100 GB to 4 TB, while SQL Server Standard and Enterprise editions can range in size from 200 GB to 4 TB. Dedicated IOPS rate and storage space allocation is specified, when a DB instance is created. RDS provisions that IOPS rate and storage for the lifetime of the DB instance or until its changed. RDS delivers within 10 percent of the provisioned IOPS performance 99.9 percent of the time over a given year.","title":"Provisioned IOPS"},{"location":"chap7/1rds/#adding-storage-and-changing-storage-type","text":"DB instance can be modified to use additional storage and converted to a different storage type. However, storage allocated for a DB instance cannot be decreased MySQL, MariaDB, PostgreSQL, and Oracle DB instances can be scaled up for storage, which helps improve I/O capacity. Storage capacity nor the type of storage for a SQL Server DB instance can be changed due to extensibility limitations of striped storage attached to a Windows Server environment. During the scaling process, the DB instance will be available for reads and writes, but may experience performance degradation Adding storage may take several hours; the duration of the process depends on several factors such as load, storage size, storage type, amount of IOPS provisioned (if any), and number of prior scale storage operations. While storage is being added, nightly backups are suspended and no other RDS operations can take place, including modify, reboot, delete, create Read Replica, and create DB Snapshot","title":"Adding Storage and Changing Storage Type"},{"location":"chap7/1rds/#3-3-performance-metrics","text":"Amazon RDS provides several metrics that can be used to determine how the DB instance is performing. IOPS the number of I/O operations completed per second. it is reported as the average IOPS for a given time interval. RDS reports read and write IOPS separately on one minute intervals. Total IOPS is the sum of the read and write IOPS. Typical values for IOPS range from zero to tens of thousands per second. Latency the elapsed time between the submission of an I/O request and its completion it is reported as the average latency for a given time interval. RDS reports read and write latency separately on one minute intervals in units of seconds. Typical values for latency are in the millisecond (ms) Throughput the number of bytes per second transferred to or from disk it is reported as the average throughput for a given time interval. RDS reports read and write throughput separately on one minute intervals using units of megabytes per second (MB/s). Typical values for throughput range from zero to the I/O channel\u2019s maximum bandwidth. Queue Depth the number of I/O requests in the queue waiting to be serviced. these are I/O requests that have been submitted by the application but have not been sent to the device because the device is busy servicing other I/O requests. it is reported as the average queue depth for a given time interval. RDS reports queue depth in one minute intervals. Typical values for queue depth range from zero to several hundred. Time spent waiting in the queue is a component of Latency and Service Time (not available as a metric).","title":"3-3 Performance Metrics"},{"location":"chap7/1rds/#3-4-amazon-rds-storage-facts","text":"First time a DB instance is started and accesses an area of disk for the first time, the process can take longer than all subsequent accesses to the same disk area. This is known as the \u201c first touch penalty \u201d. Once an area of disk has incurred the first touch penalty, that area of disk does not incur the penalty again for the life of the instance, even if the DB instance is rebooted, restarted, or the DB instance class changes. Note that a DB instance created from a snapshot, a point-in-time restore, or a read replica is a new instance and does incur this first touch penalty. RDS manages the DB instance and it reserves overhead space on the instance. While the amount of reserved storage varies by DB instance class and other factors, this reserved space can be as much as one or two percent of the total storage Provisioned IOPS provides a way to reserve I/O capacity by specifying IOPS. Like any other system capacity attribute, maximum throughput under load will be constrained by the resource that is consumed first, which could be IOPS, channel bandwidth, CPU, memory, or database internal resources. Current maximum channel bandwidth available is 4000 megabits per second (Mbps) full duplex. In terms of the read and write throughput metrics, this equates to about 210 megabytes per second (MB/s) in each direction. A perfectly balanced workload of 50% reads and 50% writes may attain a maximum combined throughput of 420 MB/s, which includes protocol overhead, so the actual data throughput may be less. Provisioned IOPS works with an I/O request size of 32 KB. Provisioned IOPS consumption is a linear function of I/O request size above 32 KB. An I/O request smaller than 32 KB is handled as one I/O; for e.g. 1000 16 KB I/O requests are treated the same as 1000 32 KB requests. I/O requests larger than 32 KB consume more than one I/O request; while, a 48 KB I/O request consumes 1.5 I/O requests of storage capacity; a 64 KB I/O request consumes 2 I/O requests","title":"3-4 Amazon RDS Storage Facts"},{"location":"chap7/1rds/#3-5-factors-that-impact-storage-performance","text":"Several factors can affect the performance of a DB instance, such as instance configuration, I/O characteristics, and workload demand. System related activities also consume I/O capacity and may reduce database instance performance while in progress: DB snapshot creation Nightly backups Multi-AZ peer creation Read replica creation Scaling storage System resources can constrain the throughput of a DB instance, but there can be other reasons for a bottleneck. Database could be the issue if : Channel throughput limit is not reached Queue depths are consistently low CPU utilization is under 80% Free memory available No swap activity Plenty of free disk space Application has dozens of threads all submitting transactions as fast as the database will take them, but there is clearly unused I/O capacity","title":"3-5 Factors That Impact Storage Performance"},{"location":"chap7/1rds/#3-6-qas","text":"When should I choose Provisioned IOPS over Standard RDS storage? If you have batch-oriented workloads If you use production online transaction processing (OLTP) workloads If you have workloads that are not sensitive to consistent performance Is decreasing the storage size of a DB Instance permitted? Depends on the RDMS used Yes No Because of the extensibility limitations of striped storage attached to Windows Server, Amazon RDS does not currently support increasing storage on a _____ DB Instance. SQL Server MySQL Oracle If I want to run a database in an Amazon instance, which is the most recommended Amazon storage option? Amazon Instance Storage Amazon EBS You can\u2019t run a database inside an Amazon instance. Amazon S3 For each DB Instance class, what is the maximum size of associated storage capacity? 1TiB 2TiB 8TiB 16TiB (The limit keeps on changing so please check the latest always)","title":"3-6 QAs"},{"location":"chap7/1rds/#4-aws-rds-db-snapshot-backup-restore","text":"","title":"4 AWS RDS DB Snapshot, Backup &amp; Restore"},{"location":"chap7/1rds/#4-1-rds-backup-restore-and-snapshots","text":"RDS creates a storage volume snapshot of the DB instance, backing up the entire DB instance and not just individual databases. RDS provides two different methods Automated and Manual for backing up your DB instances:","title":"4-1 RDS BackUp, Restore and Snapshots"},{"location":"chap7/1rds/#4-2-automated-backups","text":"Backups of the DB instance are automatically created and retained Automated backups are enabled by default for a new DB instance. Automated backups occur during a daily user-configurable period of time, known as preferred backup window . If a preferred backup window is not specified when an DB instance is created, RDS assigns a default 30-minute backup window which is selected at random from an 8-hour block of time per region. Changes to the backup window take effect immediately. Backup window cannot overlap with the weekly maintenance window for the DB instance. Backups created during the backup window are retained for a user-configurable number of days , known as backup retention period If the backup retention period is not set, RDS defaults the period retention period to one day, if created using RDS API or the AWS CLI, or seven days if created AWS Console Backup retention period can be modified with valid values are 0 (for no backup retention) to a maximum of 35 days. Manual snapshot limits (50 per region) do not apply to automated backups If the backup requires more time than allotted to the backup window, the backup will continue to completion. An immediate outage occurs if the backup retention period is changed from 0 to a non-zero value as the first backup occurs immediately or from non-zero value to 0 as it turns off automatic backups, and deletes all existing automated backups for the instance. RDS uses the periodic data backups in conjunction with the transaction logs to enable restoration of the DB Instance to any second during the retention period, up to the LatestRestorableTime (typically up to the last few minutes). During the backup window, for Single AZ instance, storage I/O may be briefly suspended while the backup process initializes (typically under a few seconds) and a brief period of elevated latency might be experienced. for Multi-AZ DB deployments, there is No I/O suspension since the backup is taken from the standby instance First backup is a full backup, while the others are incremental. Automated DB backups are deleted when the retention period expires the automated DB backups for a DB instance is disabled the DB instance is deleted When a DB instance is deleted , a final DB snapshot can be created upon deletion; which can be used to restore the deleted DB instance at a later date. RDS retains the final user-created DB snapshot along with all other manually created DB snapshots all automated backups are deleted and cannot be recovered","title":"4-2 Automated backups"},{"location":"chap7/1rds/#4-3-point-in-time-recovery","text":"In addition to the daily automated backup, RDS archives database change logs. This enables recovery of the database to any point in time during the backup retention period, up to the last five minutes of database usage. Disabling automated backups also disables point-in-time recovery RDS stores multiple copies of your data, but for Single-AZ DB instances these copies are stored in a single availability zone. If for any reason a Single-AZ DB instance becomes unusable, point-in-time recovery can be used to launch a new DB instance with the latest restorable data","title":"4-3 Point-In-Time Recovery"},{"location":"chap7/1rds/#4-4-db-snapshots-user-initiated-manual","text":"DB snapshots are manual, user-initiated backups that enables to backup a DB instance to a known state, and restore to that specific state at any time RDS keeps all manual DB snapshots until explicitly deleted","title":"4-4 DB Snapshots (User Initiated \u2013 Manual)"},{"location":"chap7/1rds/#4-5-db-snapshots-creation","text":"DB snapshot is a user-initiated storage volume snapshot of DB instance, backing up the entire DB instance and not just individual databases. DB snapshots enable backing up of the DB instance in a known state as needed, and can then be restored to that specific state at any time. DB snapshots are kept until explicitly deleted Creating DB snapshot on a Single-AZ DB instance results in a brief I/O suspension that typically lasting no more than a few minutes. Multi-AZ DB instances are not affected by this I/O suspension since the backup is taken on the standby instance","title":"4-5 DB Snapshots Creation"},{"location":"chap7/1rds/#4-6-db-snapshot-restore","text":"DB instance can be restored to any specific time during this retention period, creating a new DB instance. DB restore creates a New DB instance with a different endpoint RDS uses the periodic data backups in conjunction with the transaction logs to enable restoration of the DB Instance to any second during the retention period, up to the LatestRestorableTime (typically up to the last few minutes). Option group associated with the DB snapshot is associated with the restored DB instance once it is created . However, option group is associated with the VPC, so would apply only when the instance is restored in the same VPC as the DB snapshot. Default DB parameter and security groups are associated with the restored instance . After the restoration is complete, any custom DB parameter or security groups used by the instance restored from should be associated explicitly. A DB instance can be restored with a different storage type than the source DB snapshot. In this case the restoration process will be slower because of the additional work required to migrate the data to the new storage type for e.g. from GP2 to Provisioned IOPS A DB instance can be restored with a different edition of the DB engine only if the DB snapshot has the required storage allocated for the new edition for e.g., to change from SQL Server Web Edition to SQL Server Standard Edition, the DB snapshot must have been created from a SQL Server DB instance that had at least 200 GB of allocated storage, which is the minimum allocated storage for SQL Server Standard edition","title":"4-6 DB Snapshot Restore"},{"location":"chap7/1rds/#4-7-db-snapshot-copy","text":"RDS supports two types of DB snapshot copying. Copy an automated DB snapshot to create a manual DB snapshot in the same AWS region. Manual DB snapshot are not deleted automatically and can be kept indefinitely. Copy either an automated or manual DB snapshot from one region to another region. By copying the DB snapshot to another region, a manual DB snapshot is created that is retained in that region Automated backups cannot be shared. They need to be copied to a manual snapshot, and the manual snapshot can be shared . Manual DB snapshots can be shared with other AWS accounts and copy DB snapshots shared to you by other AWS accounts Snapshot Copy Encryption DB snapshot that has been encrypted using an AWS Key Management System (AWS KMS) encryption key can be copied Copying an encrypted DB snapshot, results in an encrypted copy of the DB snapshot When copying, DB snapshot can either be encrypted with the same KMS encryption key as the original DB snapshot, or a different KMS encryption key to encrypt the copy of the DB snapshot. An unencrypted DB snapshot can be copied to an encrypted snapshot, a quick way to add encryption to a previously encrypted DB instance. Encrypted snapshot can be restored only to an encrypted DB instance If a KMS encryption key is specified when restoring from an unencrypted DB cluster snapshot, the restored DB cluster is encrypted using the specified KMS encryption key Copying an encrypted snapshot shared from another AWS account, requires access to the KMS encryption key that was used to encrypt the DB NOTE \u2013 AWS now allows copying encrypted DB snapshots between accounts and across multiple regions as seamlessly as unencrypted snapshots. Refer blog post","title":"4-7 DB Snapshot Copy"},{"location":"chap7/1rds/#4-8-db-snapshot-sharing","text":"Manual DB snapshot or DB cluster snapshot can be shared with up to 20 other AWS accounts . Manual snapshot shared with other AWS accounts can copy the snapshot, or restore a DB instance or DB cluster from that snapshot. Manual snapshot can also be shared as public, which makes the snapshot available to all AWS accounts. Care should be taken when sharing a snapshot as public so that none of the private information is included Shared snapshot can be copied to another region . However, following limitations apply when sharing manual snapshots with other AWS accounts: When a DB instance or DB cluster is restored from a shared snapshot using the AWS CLI or RDS API, the Amazon Resource Name (ARN) of the shared snapshot as the snapshot identifier should be specified DB snapshot that uses an option group with permanent or persistent options cannot be shared A permanent option cannot be removed from an option group. Option groups with persistent options cannot be removed from a DB instance once the option group has been assigned to the DB instance. DB snapshots that have been encrypted \u201cat rest\u201d using the AES-256 encryption algorithm can be shared Users can only copy encrypted DB snapshots if they have access to the AWS Key Management Service (AWS KMS) encryption key that was used to encrypt the DB snapshot. AWS KMS encryption keys can be shared with another AWS account by adding the other account to the KMS key policy. However, KMS key policy must first be updated by adding any accounts to share the snapshot with, before sharing an encrypted DB snapshot","title":"4-8 DB Snapshot Sharing"},{"location":"chap7/1rds/#4-9-aws-rds-db-snapshot-backup-restore-qas","text":"Amazon RDS automated backups and DB Snapshots are currently supported for only the _ ___ storage engine InnoDB MyISAM Automated backups are enabled by default for a new DB Instance. TRUE FALSE Amazon RDS DB snapshots and automated backups are stored in Amazon S3 Amazon EBS Volume Amazon RDS Amazon EMR You receive a frantic call from a new DBA who accidentally dropped a table containing all your customers. Which Amazon RDS feature will allow you to reliably restore your database to within 5 minutes of when the mistake was made? Multi-AZ RDS RDS snapshots RDS read replicas RDS automated backup Disabling automated backups ______ disable the point-in-time recovery . if configured to can will never will Changes to the backup window take effect ______. from the next billing cycle after 30 minutes immediately after 24 hours You can modify the backup retention period; valid values are 0 (for no backup retention) to a maximum of _ ____ days. 45 35 15 5 Amazon RDS automated backups and DB Snapshots are currently supported for only the ______ storage engine MyISAM InnoDB What happens to the I/O operations while you take a database snapshot? I/O operations to the database are suspended for a few minutes while the backup is in progress . I/O operations to the database are sent to a Replica (if available) for a few minutes while the backup is in progress. I/O operations will be functioning normally I/O operations to the database are suspended for an hour while the backup is in progress True or False: When you perform a restore operation to a point in time or from a DB Snapshot, a new DB Instance is created with a new endpoint. FALSE TRUE True or False: Manually created DB Snapshots are deleted after the DB Instance is deleted. TRUE FALSE A user is running a MySQL RDS instance. The user will not use the DB for the next 3 months. How can the user save costs? Pause the RDS activities from CLI until it is required in the future Stop the RDS instance Create a snapshot of RDS to launch in the future and terminate the instance now Change the instance size to micro","title":"4-9 AWS RDS DB Snapshot, Backup &amp; Restore QAs"},{"location":"chap7/1rds/#5-aws-rds-security","text":"","title":"5 AWS RDS Security"},{"location":"chap7/1rds/#5-1-aws-rds-security","text":"AWS provides multiple features to provide RDS security: DB instance can be hosted in a VPC for the greatest possible network access control IAM policies can be used to assign permissions that determine who is allowed to manage RDS resources Security groups allow to control what IP addresses or EC2 instances can connect to the databases on a DB instance Secure Socket Layer (SSL) connections with DB instances RDS encryption to secure RDS instances and snapshots at rest. Network encryption and transparent data encryption (TDE) with Oracle DB instances","title":"5-1 AWS RDS Security"},{"location":"chap7/1rds/#5-2-rds-authentication-and-access-control","text":"IAM can be used to control which RDS operations each individual user has permission to call","title":"5-2 RDS Authentication and Access Control"},{"location":"chap7/1rds/#5-3-encrypting-rds-resources","text":"RDS encrypted instances use the industry standard AES-256 encryption algorithm to encrypt data on the server that hosts the RDS instance RDS handles authentication of access and decryption of the data with a minimal impact on performance , and with no need to modify the database client applications Data at Rest Encryption can be enabled on RDS instances to encrypt the underlying storage encryption keys are managed by KMS can be enabled only during instance creation once enabled, the encryption keys cannot be changed if the key is lost, the DB can only be restored from the backup Once encryption is enabled for an RDS instance, logs are encrypted snapshots are encrypted automated backups are encrypted read replicas are encrypted Encrypted snapshot from one AWS Region can be copied to another, by specifing the KMS key identifier of the destination AWS Region as KMS encryption keys are specific to the AWS Region that they are created in. RDS DB Snapshot considerations DB snapshot encrypted using an KMS encryption key can be copied Copying an encrypted DB snapshot , results in an encrypted copy of the DB snapshot When copying, DB snapshot can either be encrypted with the same KMS encryption key as the original DB snapshot, or a different KMS encryption key to encrypt the copy of the DB snapshot. An unencrypted DB snapshot can be copied to an encrypted snapshot, to add encryption to a previously unencrypted DB instance . Encrypted snapshot can be restored only to an encrypted DB instance If a KMS encryption key is specified when restoring from an unencrypted DB cluster snapshot, the restored DB cluster is encrypted using the specified KMS encryption key Copying an encrypted snapshot shared from another AWS account, requires access to the KMS encryption key used to encrypt the DB snapshot. Because KMS encryption keys are specific to the region that they are created in, encrypted snapshot cannot be copied to another region Transparent Data Encryption (TDE) Automatically encrypts the data before it is written to the underlying storage device and decrypts when it is read from the storage device is supported by Oracle and SQL Server Oracle requires key storage outside of the KMS and integrates with CloudHSM for this SQL Server requires a key but is managed by RDS","title":"5-3 Encrypting RDS Resources"},{"location":"chap7/1rds/#5-4-ssl-to-encrypt-a-connection-to-a-db-instance","text":"Encrypt connections using SSL for data in transit between the applications and the DB instance Amazon RDS creates an SSL certificate and installs the certificate on the DB instance when RDS provisions the instance . SSL certificates are signed by a certificate authority. SSL certificate includes the DB instance endpoint as the Common Name (CN) for the SSL certificate to guard against spoofing attacks While SSL offers security benefits, be aware that SSL encryption is a compute-intensive operation and will increase the latency of the database connection.","title":"5-4 SSL to Encrypt a Connection to a DB Instance"},{"location":"chap7/1rds/#5-5-iam-database-authentication","text":"IAM database authentication works with MySQL and PostgreSQL. IAM database authentication prevents the need to store static user credentials in the database, because authentication is managed externally using IAM. IAM database authentication does not require password, but needs an authentication token An authentication token is a unique string of characters that RDS generates on request. Authentication tokens are generated using AWS Signature Version 4. Each Authentication token has a lifetime of 15 minutes IAM database authentication provides the following benefits: Network traffic to and from the database is encrypted using Secure Sockets Layer (SSL). helps centrally manage access to the database resources, instead of managing access individually on each DB instance. enables using IAM Roles to access the database instead of a password, for greater security.","title":"5-5 IAM Database Authentication"},{"location":"chap7/1rds/#5-6-rds-security-groups","text":"Security groups control the access that traffic has in and out of a DB instance VPC security groups act like a firewall controlling network access to your DB instance. VPC security groups can be configured and associated with the DB instance to allow access from an IP address range, port, or EC2 security group Database security groups default to a \u201cdeny all\u201d access mode and customers must specifically authorize network ingress.","title":"5-6 RDS Security Groups"},{"location":"chap7/1rds/#5-7-rds-rotating-secrets","text":"RDS supports AWS Secrets Manager to automatically rotate the secret Secrets Manager uses a Lambda function Secrets Manager provides. Secrets Manager provides following benefits Rotate secrets safely \u2013 rotate secrets automatically without disrupting the applications. Secrets Manager offers built-in integrations for rotating credentials for RDS databases for MySQL, PostgreSQL, and Aurora. Secrets Manager can be extended to meet custom rotation requirements by creating an Lambda function to rotate other types of secrets Manage secrets centrally \u2013 to store, view, and manage all the secrets. Security \u2013 By default, Secrets Manager encrypts these secrets with encryption keys that you own and control. Using fine-grained IAM policies, access to secrets can be controlled Monitor and audit easily \u2013 Secrets Manager integrates with AWS logging and monitoring services to enable meet your security and compliance requirements. Pay as you go \u2013 Pay for the secrets stored and for the use of these secrets; there are no long-term contracts or licensing fees.","title":"5-7 RDS Rotating Secrets"},{"location":"chap7/1rds/#5-8-master-user-account-privileges","text":"When you create a new DB instance, the default master user that used gets certain privileges for that DB instance Subsequently, other users with permissions can be created","title":"5-8 Master User Account Privileges"},{"location":"chap7/1rds/#5-9-event-notification","text":"Event notifications can be configured for important events that occur on the DB instance Notifications of a variety of important events that can occur on the RDS instance, such as whether the instance was shut down, a backup was started, a failover occurred, the security group was changed, or your storage space is low can be received","title":"5-9 Event Notification"},{"location":"chap7/1rds/#5-10-rds-encrypted-db-instances-limitations","text":"Encryption can be enabled only during creation of an RDS DB instance For migrating and unencrypted data \u2013 Encrypt a copy of an unencrypted DB snapshot , Create an encrypted copy of that snapshot . Restore a DB instance from the encrypted snapshot DB instances that are encrypted can\u2019t be modified to disable encryption. Unencrypted DB instance or an unencrypted read replica of an encrypted DB instance can\u2019t have an encrypted read replica Encrypted read replicas must be encrypted with the same CMK as the source DB instance when both are in the same AWS Region. Unencrypted backup or snapshot can\u2019t be restored to an encrypted DB instance.","title":"5-10 RDS Encrypted DB Instances Limitations"},{"location":"chap7/1rds/#5-11-qas","text":"Can I encrypt connections between my application and my DB Instance using SSL? No Yes Only in VPC Only in certain regions Which of these configuration or deployment practices is a security risk for RDS? Storing SQL function code in plaintext Non-Multi-AZ RDS instance Having RDS and EC2 instances exist in the same subnet RDS in a public subnet (Making RDS accessible to the public internet in a public subnet poses a security risk, by making your database directly addressable and spammable. DB instances deployed within a VPC can be configured to be accessible from the Internet or from EC2 instances outside the VPC. If a VPC security group specifies a port access such as TCP port 22, you would not be able to access the DB instance because the firewall for the DB instance provides access only via the IP addresses specified by the DB security groups the instance is a member of and the port defined when the DB instance was created.)","title":"5-11 QAs"},{"location":"chap7/1rds/#6-aws-rds-db-maintenance-upgrades","text":"","title":"6 AWS RDS DB Maintenance &amp; Upgrades"},{"location":"chap7/1rds/#6-1-rds-db-instance-maintenance-and-upgrades","text":"Changes to a DB instance can occur when a DB instance is manually modified for e.g. DB engine version is upgraded, or when Amazon RDS performs maintenance on an instance","title":"6-1 RDS DB Instance Maintenance and Upgrades"},{"location":"chap7/1rds/#6-2-amazon-rds-maintenance","text":"Periodically, Amazon RDS performs maintenance on Amazon RDS resources, such as DB instances and most often involves updates to the DB instance\u2019s operating system (OS). Maintenance items can either be applied manually on a DB instance at ones convenience or wait for the automatic maintenance process initiated by Amazon RDS during the defined weekly maintenance window. Maintenance window only determines when pending operations start, but does not limit the total execution time of these operations. Maintenance operations are not guaranteed to finish before the maintenance window ends, and can continue beyond the specified end time. Maintenance update availability can be checked both on the RDS console and by using the RDS API. And if an update is available, one can Defer the maintenance items. Apply the maintenance items immediately. Schedule them to start during the next defined maintenance window Maintenance items marked as Required cannot be deferred indefinitely , if deferred AWS will send a notify the time when the update will be performed next Available and can be deferred indefinitely and the update will not be applied to the DB instance. Required patching is automatically scheduled only for patches that are related to security and instance reliability . Such patching occurs infrequently (typically once every few months) and seldom requires more than a fraction of your maintenance window. Maintenance items require that RDS take your DB instance offline for a short time. Maintenance that requires DB instance to be offline include scale compute operations, which generally take only a few minutes from start to finish, and required operating system or database patching. Multi-AZ deployment for the DB instance reduces the impact of a maintenance event by following these steps: * Perform maintenance on the standby . Promote the standby to primary . Perform maintenance on the old primary, which becomes the new standby . When database engine for the DB instance is modified in a Multi-AZ deployment, RDS upgrades both the primary and secondary DB instances at the same time. In this case, the database engine for the entire Multi-AZ deployment is shut down during the upgrade.","title":"6-2 Amazon RDS Maintenance"},{"location":"chap7/1rds/#6-3-operating-system-updates","text":"Upgrades to the operating system are most often for security issues and should be done as soon as possible. OS updates on a DB instance can be applied at ones convenience or can wait for the maintenance process initiated by RDS to apply the update during the defined maintenance window DB instance is not automatically backed up when an OS update is applied, and should be backup up before the update is applied","title":"6-3 Operating System Updates"},{"location":"chap7/1rds/#6-4-database-engine-version-upgrade","text":"DB instance engine version can be upgraded when a new DB engine version is supported by RDS. Database version upgrades consist of major and minor version upgrades. Major database version upgrades can contain changes that are not backward-compatible RDS doesn\u2019t apply major version upgrades automatically DB instance should be manually modified and thoroughly tested before applying it to the production instances. Minor version upgrades Each DB engine handles minor version upgrade slightly differently for e.g. RDS automatically apply minor version upgrades to a DB instance running PostgreSQL, but must be manually applied to a DB instance running Oracle. Amazon posts an announcement to the forums announcement page and sends a customer e-mail notification before upgrading an DB instance Amazon schedule the upgrades at specific times through the year, to help plan around them, because downtime is required to upgrade a DB engine version, even for Multi-AZ instances. RDS takes two DB snapshots during the upgrade process . First DB snapshot is of the DB instance before any upgrade changes have been made. If the upgrade fails, it can be restored from the snapshot to create a DB instance running the old version. Second DB snapshot is taken when the upgrade completes. After the upgrade is complete, database engine can\u2019t be reverted to the previous version. For returning to the previous version, restore the first DB snapshot taken to create a new DB instance. If the DB instance is using read replication, all of the Read Replicas must be upgraded before upgrading the source instance. If the DB instance is in a Multi-AZ deployment, both the primary and standby replicas are upgraded at the same time and would result in an outage. The time for the outage varies based on your database engine, version, and the size of your DB instance.","title":"6-4 Database Engine Version Upgrade"},{"location":"chap7/1rds/#6-5-rds-maintenance-window","text":"Every DB instance has a weekly maintenance window defined during which any system changes are applied. Maintenance window is an opportunity to control when DB instance modifications and software patching occur, in the event either are requested or required. If a maintenance event is scheduled for a given week, it will be initiated during the 30 minute maintenance window as defined Maintenance events mostly complete during the 30 minute maintenance window, although larger maintenance events may take more time 30-minute maintenance window is selected at random from an 8-hour block of time per region. If you don\u2019t specify a preferred maintenance window when you create the DB instance, Amazon RDS assigns a 30-minute maintenance window on a randomly selected day of the week. RDS will consume some of the resources on the DB instance while maintenance is being applied, minimally effecting performance. For some maintenance events, a Multi-AZ failover may be required for a maintenance update to complete.","title":"6-5 RDS Maintenance Window"},{"location":"chap7/1rds/#6-6-aws-rds-db-maintenance-upgrades-qas","text":"A user has launched an RDS MySQL DB with the Multi AZ feature. The user has scheduled the scaling of instance storage during maintenance window. What is the correct order of events during maintenance window? 1. Perform maintenance on standby 2. Promote standby to primary 3. Perform maintenance on original primary 4. Promote original master back as primary 1, 2, 3, 4 1, 2, 3 2, 3, 4, 1 Can I control if and when MySQL based RDS Instance is upgraded to new supported versions? No Only in VPC Yes A user has scheduled the maintenance window of an RDS DB on Monday at 3 AM. Which of the below mentioned events may force to take the DB instance offline during the maintenance window? Enabling Read Replica Making the DB Multi AZ DB password change Security patching A user has launched an RDS postgreSQL DB with AWS. The user did not specify the maintenance window during creation. The user has configured RDS to update the DB instance type from micro to large. If the user wants to have it during the maintenance window, what will AWS do? AWS will not allow to update the DB until the maintenance window is configured AWS will select the default maintenance window if the user has not provided it AWS will ask the user to specify the maintenance window during the update It is not possible to change the DB size from micro to large with RDS Can I test my DB Instance against a new version before upgrading? No Yes Only in VPC","title":"6-6 AWS RDS DB Maintenance &amp; Upgrades QAs"},{"location":"chap7/1rds/#7-aws-rds-monitoring-notification","text":"","title":"7 AWS RDS Monitoring &amp; Notification"},{"location":"chap7/1rds/#7-1-aws-rds-monitoring-notification","text":"RDS integrates with CloudWatch and provides metrics for monitoring CloudWatch alarms can be created over a single metric that sends an SNS message when the alarm changes state RDS also provides SNS notification whenever any RDS event occurs","title":"7-1 AWS RDS Monitoring &amp; Notification"},{"location":"chap7/1rds/#7-2-rds-cloudwatch-monitoring","text":"RDS DB instance can be monitored using CloudWatch, which collects and processes raw data from RDS into readable, near real-time metrics. Statistics are recorded for a period of two weeks, so that you can access historical information and gain a better perspective on how the service is performing. By default, RDS metric data is automatically sent to CloudWatch in 1-minute periods CloudWatch RDS Metrics BinLogDiskUsage \u2013 Amount of disk space occupied by binary logs on the master. Applies to MySQL read replicas. CPUUtilization \u2013 Percentage of CPU utilization. DatabaseConnections \u2013 Number of database connections in use. DiskQueueDepth \u2013 The number of outstanding IOs (read/write requests) waiting to access the disk. FreeableMemory \u2013 Amount of available random access memory. FreeStorageSpace \u2013 Amount of available storage space. ReplicaLag \u2013 Amount of time a Read Replica DB instance lags behind the source DB instance. SwapUsage \u2013 Amount of swap space used on the DB instance. ReadIOPS \u2013 Average number of disk I/O operations per second. WriteIOPS \u2013 Average number of disk I/O operations per second. ReadLatency \u2013 Average amount of time taken per disk I/O operation. WriteLatency \u2013 Average amount of time taken per disk I/O operation. ReadThroughput \u2013 Average number of bytes read from disk per second. WriteThroughput \u2013 Average number of bytes written to disk per second. NetworkReceiveThroughput \u2013 Incoming (Receive) network traffic on the DB instance, including both customer database traffic and Amazon RDS traffic used for monitoring and replication. NetworkTransmitThroughput \u2013 Outgoing (Transmit) network traffic on the DB instance, including both customer database traffic and Amazon RDS traffic used for monitoring and replication.","title":"7-2 RDS CloudWatch Monitoring"},{"location":"chap7/1rds/#7-3-rds-enhanced-monitoring","text":"RDS provides metrics in real time for the operating system (OS) that the DB instance runs on. By default, Enhanced Monitoring metrics are stored for 30 days in the CloudWatch Logs, which are different from typical CloudWatch metrics.","title":"7-3 RDS Enhanced Monitoring"},{"location":"chap7/1rds/#7-4-cloudwatch-vs-enhanced-monitoring-metrics","text":"CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance, and Enhanced Monitoring gathers its metrics from an agent on the instance. Enhanced Monitoring metrics are useful to understand how different processes or threads on a DB instance use the CPU. There might be differences between the measurements, because the hypervisor layer performs a small amount of work. The differences can be greater if the DB instances use smaller instance classes, because then there are likely more virtual machines (VMs) that are managed by the hypervisor layer on a single physical instance.","title":"7-4 CloudWatch vs Enhanced Monitoring Metrics"},{"location":"chap7/1rds/#7-5-rds-performance-insights","text":"Performance Insights is a database performance tuning and monitoring feature that helps illustrate the database\u2019s performance and help analyze any issues that affect it With the Performance Insights dashboard, you can visualize the database load and filter the load by waits, SQL statements, hosts, or users.","title":"7-5 RDS Performance Insights"},{"location":"chap7/1rds/#7-6-rds-cloudtrail-logs","text":"CloudTrail provides a record of actions taken by a user, role, or an AWS service in RDS. CloudTrail captures all API calls for RDS as events, including calls from the console and from code calls to RDS API operations. CloudTrail can help determine the request that was made to RDS, the IP address from which the request was made, who made the request, when it was made, and additional details.","title":"7-6 RDS CloudTrail Logs"},{"location":"chap7/1rds/#7-7-rds-recommendations","text":"RDS provides automated recommendations for database resources. The recommendations provide best practice guidance by analyzing DB instance configuration, usage, and performance data.","title":"7-7 RDS Recommendations"},{"location":"chap7/1rds/#7-8-rds-event-notification","text":"RDS uses the SNS to provide notification when an RDS event occurs RDS groups the events into categories, which can be subscribed so that a notification is sent when an event in that category occurs. Event category for a DB instance, DB cluster, DB snapshot, DB cluster snapshot, DB security group or for a DB parameter group can be subscribed Event notifications are sent to the email addresses provided during subscription creation Subscription can be easily turn off notification without deleting a subscription by setting the Enabled radio button to No in the RDS console or by setting the Enabled parameter to false using the CLI or RDS API.","title":"7-8 RDS Event Notification"},{"location":"chap7/1rds/#7-9-rds-trusted-advisor","text":"Trusted Advisor inspects the AWS environment and then makes recommendations when opportunities exist to save money, improve system availability and performance, or help close security gaps. Trusted Advisor has the following RDS-related checks: RDS Idle DB Instances RDS Security Group Access Risk RDS Backups RDS Multi-AZ","title":"7-9 RDS Trusted Advisor"},{"location":"chap7/1rds/#7-10-aws-rds-monitoring-notification-qas","text":"You run a web application with the following components Elastic Load Balancer (ELB), 3 Web/Application servers, 1 MySQL RDS database with read replicas, and Amazon Simple Storage Service (Amazon S3) for static content. Average response time for users is increasing slowly. What three CloudWatch RDS metrics will allow you to identify if the database is the bottleneck? Choose 3 answers The number of outstanding IOs waiting to access the disk The amount of write latency The amount of disk space occupied by binary logs on the master. The amount of time a Read Replica DB Instance lags behind the source DB Instance The average number of disk I/O operations per second. Typically, you want your application to check whether a request generated an error before you spend any time processing results. The easiest way to find out if an error occurred is to look for an _ ___ node in the response from the Amazon RDS API. Incorrect Error FALSE In the Amazon CloudWatch, which metric should I be checking to ensure that your DB Instance has enough free storage space? FreeStorage FreeStorageSpace FreeStorageVolume FreeDBStorageSpace A user is receiving a notification from the RDS DB whenever there is a change in the DB security group. The user does not want to receive these notifications for only a month. Thus, he does not want to delete the notification. How can the user configure this? Change the Disable button for notification to \u201cYes\u201d in the RDS console Set the send mail flag to false in the DB event notification console The only option is to delete the notification from the console Change the Enable button for notification to \u201cNo\u201d in the RDS console A sys admin is planning to subscribe to the RDS event notifications. For which of the below mentioned source categories the subscription cannot be configured? DB security group DB snapshot DB options group DB parameter group A user is planning to setup notifications on the RDS DB for a snapshot. Which of the below mentioned event categories is not supported by RDS for this snapshot source type? Backup Creation Deletion Restoration A system admin is planning to setup event notifications on RDS. Which of the below mentioned services will help the admin setup notifications? AWS SES AWS Cloudtrail AWS CloudWatch AWS SNS A user has setup an RDS DB with Oracle. The user wants to get notifications when someone modifies the security group of that DB. How can the user configure that? It is not possible to get the notifications on a change in the security group Configure SNS to monitor security group changes Configure event notification on the DB security group Configure the CloudWatch alarm on the DB for a change in the security group It is advised that you watch the Amazon CloudWatch \u201c_____\u201d metric (available via the AWS Management Console or Amazon Cloud Watch APIs) carefully and recreate the Read Replica should it fall behind due to replication errors. Write Lag Read Replica Replica Lag Single Replica","title":"7-10 AWS RDS Monitoring &amp; Notification QAs"},{"location":"chap7/1rds/#8-aws-rds-best-practices","text":"","title":"8 AWS RDS Best Practices"},{"location":"chap7/1rds/#8-1-aws-rds-best-practices","text":"AWS recommends RDS best practices in terms of Monitoring, Performance and security","title":"8-1 AWS RDS Best Practices"},{"location":"chap7/1rds/#8-2-amazon-rds-basic-operational-guidelines","text":"Monitoring Memory, CPU, and storage usage should be monitored. CloudWatch can be setup for notifications when usage patterns change or when the capacity of deployment is approached, so that system performance and availability can be maintained Scaling Scale up the DB instance when approaching storage capacity limits. There should be some buffer in storage and memory to accommodate unforeseen increases in demand from the applications. Backups Enable Automatic Backups and set the backup window to occur during the daily low in WriteIOPS. On a MySQL DB instance , Do not create more than 10,000 tables using Provisioned IOPS or 1000 tables using standard storage. Large numbers of tables will significantly increase database recovery time after a failover or database crash. If you need to create more tables than recommended, set the innodb_file_per_table parameter to 0. Avoid tables in the database growing too large. Provisioned storage limits restrict the maximum size of a MySQL table file to 6 TB. Instead, partition the large tables so that file sizes are well under the 6 TB limit. This can also improve performance and recovery time. Performance If the database workload requires more I/O than provisioned, recovery after a failover or database failure will be slow. To increase the I/O capacity of a DB instance, Migrate to a DB instance class with High I/O capacity. Convert from standard storage to Provisioned IOPS storage, and use a DB instance class that is optimized for Provisioned IOPS. if using Provisioned IOPS storage, provision additional throughput capacity. Multi-AZ & Failover Deploy applications in all Availability Zones, so if an AZ goes down, applications in other AZs will still be available. Use Amazon RDS DB events to monitor failovers. Set a TTL of less than 30 seconds, if the client application is caching the DNS data of the DB instances. As the underlying IP address of a DB instance can change after a failover, caching the DNS data for an extended time can lead to connection failures if the application tries to connect to an IP address that no longer is in service. Multi-AZ requires transaction logging feature to be enabled. Do not use features like Simple recover mode, offline mode or Read-only mode which turn of transaction logging. To shorten failover time Ensure that sufficient Provisioned IOPS allocated for your workload. Inadequate I/O can lengthen failover times. Database recovery requires I/O. Use smaller transactions. Database recovery relies on transactions, so break up large transactions into multiple smaller transactions to shorten failover time Test failover for your DB instance to understand how long the process takes for your use case and to ensure that the application that accesses your DB instance can automatically connect to the new DB instance after failover.","title":"8-2 Amazon RDS Basic Operational Guidelines"},{"location":"chap7/1rds/#8-3-db-instance-ram-recommendations","text":"An Amazon RDS performance best practice is to allocate enough RAM so that the working set resides almost completely in memory. Value of ReadIOPS should be small and stable. ReadIOPS metric can be checked, using AWS CloudWatch while the DB instance is under load, to tell if the working set is almost all in memory If scaling up the DB instance class with more RAM, results in a dramatic drop in ReadIOPS, the working set was not almost completely in memory. Continue to scale up until ReadIOPS no longer drops dramatically after a scaling operation, or ReadIOPS is reduced to a very small amount.","title":"8-3 DB Instance RAM Recommendations"},{"location":"chap7/1rds/#8-4-rds-security-best-practices","text":"Do not use AWS root credentials to manage Amazon RDS resources; and IAM users should be created for everyone, Grant each user the minimum set of permissions required to perform his or her duties. Use IAM groups to effectively manage permissions for multiple users. Rotate your IAM credentials regularly .","title":"8-4 RDS Security Best Practices"},{"location":"chap7/1rds/#8-5-using-enhanced-monitoring-to-identify-operating-system-issues","text":"Amazon RDS provides metrics in real time for the operating system (OS) that your DB instance runs on. Enhanced monitoring is available for all DB instance classes except for db.t1.micro and db.m1.small. Using Metrics to Identify Performance Issues To identify performance issues caused by insufficient resources and other common bottlenecks, you can monitor the metrics available for your Amazon RDS DB instance Performance metrics should be monitored on a regular basis to benchmark the average, maximum, and minimum values for a variety of time ranges. to help identify performance degradation. Amazon CloudWatch alarms can be set for particular metric thresholds to be alerted when they are reached or breached A DB instance has a number of different categories of metrics which includes CPU, memory, disk space, IOPS, db connections and network traffic, and how to determine acceptable values depends on the metric. One of the best ways to improve DB instance performance is to tune the most commonly used and most resource-intensive queries to make them less expensive to run.","title":"8-5 Using Enhanced Monitoring to Identify Operating System Issues"},{"location":"chap7/1rds/#8-6-recovery","text":"MySQL InnoDB is the recommended and supported storage engine for MySQL DB instances on Amazon RDS. However, MyISAM performs better than InnoDB if you require intense, full-text search capability. Point-In-Time Restore and snapshot restore features of Amazon RDS for MySQL require a crash-recoverable storage engine and are supported for the InnoDB storage engine only. Although MySQL supports multiple storage engines with varying capabilities, not all of them are optimized for crash recovery and data durability. MyISAM storage engine does not support reliable crash recovery and might prevent a Point-In-Time Restore or snapshot restore from working as intended which might result in lost or corrupt data when MySQL is restarted after a crash. MariaDB XtraDB is the recommended and supported storage engine for MariaDB DB instances on Amazon RDS. Point-In-Time Restore and snapshot restore features of Amazon RDS for MariaDB require a crash-recoverable storage engine and are supported for the XtraDB storage engine only. Although MariaDB supports multiple storage engines with varying capabilities, not all of them are optimized for crash recovery and data durability. For e.g although Aria is a crash-safe replacement for MyISAM, it might still prevent a Point-In-Time Restore or snapshot restore from working as intended. This might result in lost or corrupt data when MariaDB is restarted after a crash.","title":"8-6 Recovery"},{"location":"chap7/1rds/#8-7-qas","text":"You are running a database on an EC2 instance, with the data stored on Elastic Block Store (EBS) for persistence At times throughout the day, you are seeing large variance in the response times of the database queries Looking into the instance with the isolate command you see a lot of wait time on the disk volume that the database\u2019s data is stored on. What two ways can you improve the performance of the database\u2019s storage while maintaining the current persistence of the data? Choose 2 answers Move to an SSD backed instance Move the database to an EBS-Optimized Instance Use Provisioned IOPs EBS Use the ephemeral storage on an m2.4xLarge Instance Instead Amazon RDS automated backups and DB Snapshots are currently supported for only the _ ___ storage engine InnoDB MyISAM","title":"8-7 QAs"},{"location":"chap7/2DynamoDB/","text":"L2 AWS DynamoDB(unfinished) 1 AWS DynamoDB 1-1 AWS DynamoDB Amazon DynamoDB is a fully managed NoSQL database service that makes it simple and cost-effective to store and retrieve any amount of data and serve any level of request traffic. provides fast and predictable performance with seamless scalability DynamoDB enables customers to offload the administrative burdens of operating and scaling distributed databases to AWS, without having to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling. DynamoDB tables do not have fixed schemas, and table consists of items and each item may have a different number of attributes. DynamoDB synchronously replicates data across three facilities in an AWS Region, giving high availability and data durability. DynamoDB supports fast in-place updates. A numeric attribute can be incremented or decremented in a row using a single API call DynamoDB uses proven cryptographic methods to securely authenticate users and prevent unauthorized data access Durability, performance, reliability, and security are built in, with SSD (solid state drive) storage and automatic 3-way replication. DynamoDB supports two different kinds of primary keys: Partition Key (previously called the Hash key) A simple primary key, composed of one attribute DynamoDB uses the partition key\u2019s value as input to an internal hash function; the output from the hash function determine the partition where the item will be stored. No two items in a table can have the same partition key value. Partition Key and Sort Key (previously called the Hash and Range key) A composite primary key composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key. DynamoDB uses the partition key value as input to an internal hash function; the output from the hash function determines the partition where the item will be stored. All items with the same partition key are stored together, in sorted order by sort key value. Combination of partition key and sort key must be unique It is possible for two items to have the same partition key value, but those two items must have different sort key values. DynamoDB Secondary indexes add flexibility to the queries, without impacting performance. are automatically maintained as sparse objects, items will only appear in an index if they exist in the table on which the index is defined making queries against an index very efficient DynamoDB throughput and single-digit millisecond latency makes it a great fit for gaming, ad tech, mobile, and many other applications ElastiCache can be used in front of DynamoDB in order to offload high amount of reads for non frequently changed data 1-2 DynamoDB Consistency Each DynamoDB table is automatically stored in the three geographically distributed locations for durability Read consistency represents the manner and timing in which the successful write or update of a data item is reflected in a subsequent read operation of that same item DynamoDB allows user to specify whether the read should be eventually consistent or strongly consistent at the time of the request Eventually Consistent Reads (Default) Eventual consistency option maximizes the read throughput. Consistency across all copies is usually reached within a second However, an eventually consistent read might not reflect the results of a recently completed write. Repeating a read after a short time should return the updated data. DynamoDB uses eventually consistent reads, by default. Strongly Consistent Reads Strongly consistent read returns a result that reflects all writes that received a successful response prior to the read Strongly consistent reads are 2x the cost of Eventually consistent reads Strongly Consistent Reads comes with disadvantages A strongly consistent read might not be available if there is a network delay or outage. In this case, DynamoDB may return a server error (HTTP 500). Strongly consistent reads may have higher latency than eventually consistent reads. Strongly consistent reads are not supported on global secondary indexes. Strongly consistent reads use more throughput capacity than eventually consistent reads. Read operations (such as GetItem, Query, and Scan ) provide a ConsistentRead parameter, if set to true, DynamoDB uses strongly consistent reads during the operation. Query, GetItem, and BatchGetItem operations perform eventually consistent reads by default Query and GetItem operations can be forced to be strongly consistent Query operations cannot perform strongly consistent reads on Global Secondary Indexes BatchGetItem operations can be forced to be strongly consistent on a per-table basis 1-3 DynamoDB Advanced Topics DynamoDB Streams, Triggers, Cross Region Replication, DAX, VPC Endpoints etc. 1-4 DynamoDB Performance Automatically scales horizontally runs exclusively on Solid State Drives (SSDs). SSDs help achieve the design goals of predictable low-latency response times for storing and accessing data at any scale. SSDs High I/O performance enables it to serve high-scale request workloads cost efficiently, and to pass this efficiency along in low request pricing allows provisioned table reads and writes Scale up throughput when needed Scale down throughput four times per UTC calendar day automatically partitions, reallocates and re-partitions the data and provisions additional server capacity as the table size grows or provisioned throughput is increased Global Secondary indexes (GSI) can be created upfront or added later 1-5 DynamoDB Security Fine Grained Access Control (FGAC) gives a high degree of control over data in the table FGAC helps control who (caller) can access which items or attributes of the table and perform what actions (read/write capability). FGAC is integrated with IAM, which manages the security credentials and the associated permissions. 1-6 DynamoDB Encryption Data in Transit Encryption can be done by encrypting sensitive data on the client side or using encrypted connections (TLS) All the data in DynamoDB is encrypted in transit (except the data in DAX) communications to and from DynamoDB use the HTTPS protocol, which protects network traffic by using SSL/TLS encryption. DynamoDB supports Encryption at rest Encryption at rest is enabled on all DynamoDB table data and cannot be disabled E ncryption at rest enables encryption for the data persisted (data at rest) in the DynamoDB tables. E ncryption at rest includes the base tables, primary key, local and global secondary indexes, streams, global tables, backups, and DynamoDB Accelerator (DAX) clusters Encryption at rest automatically integrates with AWS KMS for managing the keys used for encrypting the tables. Encryption at rest also supports AWS owned CMK, AWS managed CMK, and Customer managed CMK Encryption at rest can be enabled only for a new table and not for an existing table Encryption once enabled for a table, cannot be disabled DynamoDB streams can be used with encrypted tables and are always encrypted with a table-level encryption key On-Demand Backups of encrypted DynamoDB tables are encrypted using S3\u2019s Server-Side Encryptio n Encryption at rest encrypts the data using 256-bit AES encryption. 1-7 DynamoDB Encryption Client DynamoDB Encryption Client is a software library that helps protect the table data before sending it to DynamoDB. Encrypting the sensitive data in transit and at rest helps ensure that the plaintext data isn\u2019t available to any third party, including AWS. Encryption Client encrypts attribute values which can be controlled, but does not encrypt entire table, attribute names or primary key 1-8 DynamoDB Costs Index Storage DynamoDB is an indexed data store Billable Data = Raw byte data size + 100 byte per-item storage indexing overhead Provisioned throughput Pay flat, hourly rate based on the capacity reserved as the throughput provisioned for the table one Write Capacity Unit provides one write per second for items < 1KB in size. one Read Capacity Unit provides one strongly consistent read (or two eventually consistent reads) per second for items < 4KB in size. Provisioned throughput charges for every 10 units of Write Capacity and every 50 units of Read Capacity. Reserved capacity Significant savings over the normal price Pay a one-time upfront fee DynamoDB also charges for storage, backup, replication, streams, caching, data transfer out. 1-9 Intro QAs Which of the following are use cases for Amazon DynamoDB? Choose 3 answers Storing BLOB data. Managing web sessions Storing JSON documents Storing metadata for Amazon S3 objects Running relational joins and complex updates. Storing large amounts of infrequently accessed data. You are configuring your company\u2019s application to use Auto Scaling and need to move user state information. Which of the following AWS services provides a shared data store with durability and low latency? AWS ElastiCache Memcached (does not allow writes) Amazon Simple Storage Service (does not provide low latency) Amazon EC2 instance storage (not durable) Amazon DynamoDB Does Dynamo DB support in-place atomic updates? It is not defined No Yes It does support in-place non-atomic updates What is the maximum write throughput I can provision for a single Dynamic DB table? 1,000 write capacity units 100,000 write capacity units Dynamic DB is designed to scale without limits, but if you go beyond 10,000 you have to contact AWS first 10,000 write capacity units For a DynamoDB table, what happens if the application performs more reads or writes than your provisioned capacity? Nothing requests above the provisioned capacity will be performed but you will receive 400 error codes. requests above the provisioned capacity will be performed but you will receive 200 error codes. requests above the provisioned capacity will be throttled and you will receive 400 error codes . In which of the following situations might you benefit from using DynamoDB? (Choose 2 answers) You need fully managed database to handle highly complex queries You need to deal with massive amount of \u201chot\u201d data and require very low latency You need a rapid ingestion of clickstream in order to collect data about user behavior Your on-premises data center runs Oracle database, and you need to host a backup in AWS cloud Does Amazon DynamoDB support both increment and decrement atomic operations? No, neither increment nor decrement operations. Only increment, since decrement are inherently impossible with DynamoDB\u2019s data model. Only decrement, since increment are inherently impossible with DynamoDB\u2019s data model. Yes, both increment and decrement operations . What is the data model of DynamoDB? \u201cItems\u201d, with Keys and one or more Attribute; and \u201cAttribute\u201d, with Name and Value. \u201cDatabase\u201d, which is a set of \u201cTables\u201d, which is a set of \u201cItems\u201d, which is a set of \u201cAttributes\u201d. \u201cTable\u201d, a collection of Items; \u201cItems\u201d, with Keys and one or more Attribute; and \u201cAttribute\u201d, with Name and Value. \u201cDatabase\u201d, a collection of Tables; \u201cTables\u201d, with Keys and one or more Attribute; and \u201cAttribute\u201d, with Name and Value. In regard to DynamoDB, for which one of the following parameters does Amazon not charge you? Cost per provisioned write units Cost per provisioned read units Storage cost I/O usage within the same Region Which statements about DynamoDB are true? Choose 2 answers. DynamoDB uses a pessimistic locking model DynamoDB uses optimistic concurrency control DynamoDB uses conditional writes for consistency DynamoDB restricts item access during reads DynamoDB restricts item access during writes Which of the following is an example of a good DynamoDB hash key schema for provisioned throughput efficiency? User ID, where the application has many different users . Status Code where most status codes is the same. Device ID, where one is by far more popular than all the others. Game Type, where there are three possible game types. You are inserting 1000 new items every second in a DynamoDB table. Once an hour these items are analyzed and then are no longer needed. You need to minimize provisioned throughput, storage, and API calls. Given these requirements, what is the most efficient way to manage these Items after the analysis? Retain the items in a single table Delete items individually over a 24 hour period Delete the table and create a new table per hour Create a new table per hour When using a large Scan operation in DynamoDB, what technique can be used to minimize the impact of a scan on a table\u2019s provisioned throughput? Set a smaller page size for the scan Use parallel scans Define a range index on the table Prewarm the table by updating all items In regard to DynamoDB, which of the following statements is correct? An Item should have at least two value sets, a primary key and another attribute. An Item can have more than one attributes A primary key should be single-valued. An attribute can have one or several other attributes. Which one of the following statements is NOT an advantage of DynamoDB being built on Solid State Drives? serve high-scale request workloads low request pricing high I/O performance of WebApp on EC2 instance (Not related to DynamoDB) low-latency response times Which one of the following operations is NOT a DynamoDB operation? BatchWriteItem DescribeTable BatchGetItem BatchDeleteItem (DeleteItem deletes a single item in a table by primary key, but BatchDeleteItem doesn\u2019t exist) What item operation allows the retrieval of multiple items from a DynamoDB table in a single API call? GetItem BatchGetItem GetMultipleItems GetItemRange An application stores payroll information nightly in DynamoDB for a large number of employees across hundreds of offices. Item attributes consist of individual name, office identifier, and cumulative daily hours. Managers run reports for ranges of names working in their office. One query is. \u201cReturn all Items in this office for names starting with A through E\u201d. Which table configuration will result in the lowest impact on provisioned throughput for this query? [PROFESSIONAL] Configure the table to have a hash index on the name attribute, and a range index on the office identifier Configure the table to have a range index on the name attribute, and a hash index on the office identifier Configure a hash index on the name attribute and no range index Configure a hash index on the office Identifier attribute and no range index You need to migrate 10 million records in one hour into DynamoDB. All records are 1.5KB in size. The data is evenly distributed across the partition key. How many write capacity units should you provision during this batch load? 6667 4166 5556 ( 2 write units (1 for each 1KB) * 10 million/3600 secs, refer link) 2778 A meteorological system monitors 600 temperature gauges, obtaining temperature samples every minute and saving each sample to a DynamoDB table. Each sample involves writing 1K of data and the writes are evenly distributed over time. How much write throughput is required for the target table? 1 write capacity unit 10 write capacity units ( 1 write unit for 1K * 600 gauges/60 secs) 60 write capacity units 600 write capacity units 3600 write capacity units You are building a game high score table in DynamoDB. You will store each user\u2019s highest score for each game, with many games, all of which have relatively similar usage levels and numbers of players. You need to be able to look up the highest score for any game. What\u2019s the best DynamoDB key structure? HighestScore as the hash / only key. GameID as the hash key, HighestScore as the range key. (hash (partition) key should be the GameID, and there should be a range key for ordering HighestScore. ) GameID as the hash / only key. GameID as the range / only key. You are experiencing performance issues writing to a DynamoDB table. Your system tracks high scores for video games on a marketplace. Your most popular game experiences all of the performance issues. What is the most likely problem? DynamoDB\u2019s vector clock is out of sync, because of the rapid growth in request for the most popular game. You selected the Game ID or equivalent identifier as the primary partition key for the table. Users of the most popular video game each perform more read and write requests than average. You did not provision enough read or write throughput to the table. You are writing to a DynamoDB table and receive the following exception:\u201d ProvisionedThroughputExceededException\u201d. Though according to your Cloudwatch metrics for the table, you are not exceeding your provisioned throughput. What could be an explanation for this? You haven\u2019t provisioned enough DynamoDB storage instances You\u2019re exceeding your capacity on a particular Range Key You\u2019re exceeding your capacity on a particular Hash Key (Hash key determines the partition and hence the performance) You\u2019re exceeding your capacity on a particular Sort Key You haven\u2019t configured DynamoDB Auto Scaling triggers","title":"L2 AWS DynamoDB(unfinished)"},{"location":"chap7/2DynamoDB/#l2-aws-dynamodbunfinished","text":"","title":"L2 AWS DynamoDB(unfinished)"},{"location":"chap7/2DynamoDB/#1-aws-dynamodb","text":"","title":"1 AWS DynamoDB"},{"location":"chap7/2DynamoDB/#1-1-aws-dynamodb","text":"Amazon DynamoDB is a fully managed NoSQL database service that makes it simple and cost-effective to store and retrieve any amount of data and serve any level of request traffic. provides fast and predictable performance with seamless scalability DynamoDB enables customers to offload the administrative burdens of operating and scaling distributed databases to AWS, without having to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling. DynamoDB tables do not have fixed schemas, and table consists of items and each item may have a different number of attributes. DynamoDB synchronously replicates data across three facilities in an AWS Region, giving high availability and data durability. DynamoDB supports fast in-place updates. A numeric attribute can be incremented or decremented in a row using a single API call DynamoDB uses proven cryptographic methods to securely authenticate users and prevent unauthorized data access Durability, performance, reliability, and security are built in, with SSD (solid state drive) storage and automatic 3-way replication. DynamoDB supports two different kinds of primary keys: Partition Key (previously called the Hash key) A simple primary key, composed of one attribute DynamoDB uses the partition key\u2019s value as input to an internal hash function; the output from the hash function determine the partition where the item will be stored. No two items in a table can have the same partition key value. Partition Key and Sort Key (previously called the Hash and Range key) A composite primary key composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key. DynamoDB uses the partition key value as input to an internal hash function; the output from the hash function determines the partition where the item will be stored. All items with the same partition key are stored together, in sorted order by sort key value. Combination of partition key and sort key must be unique It is possible for two items to have the same partition key value, but those two items must have different sort key values. DynamoDB Secondary indexes add flexibility to the queries, without impacting performance. are automatically maintained as sparse objects, items will only appear in an index if they exist in the table on which the index is defined making queries against an index very efficient DynamoDB throughput and single-digit millisecond latency makes it a great fit for gaming, ad tech, mobile, and many other applications ElastiCache can be used in front of DynamoDB in order to offload high amount of reads for non frequently changed data","title":"1-1 AWS DynamoDB"},{"location":"chap7/2DynamoDB/#1-2-dynamodb-consistency","text":"Each DynamoDB table is automatically stored in the three geographically distributed locations for durability Read consistency represents the manner and timing in which the successful write or update of a data item is reflected in a subsequent read operation of that same item DynamoDB allows user to specify whether the read should be eventually consistent or strongly consistent at the time of the request Eventually Consistent Reads (Default) Eventual consistency option maximizes the read throughput. Consistency across all copies is usually reached within a second However, an eventually consistent read might not reflect the results of a recently completed write. Repeating a read after a short time should return the updated data. DynamoDB uses eventually consistent reads, by default. Strongly Consistent Reads Strongly consistent read returns a result that reflects all writes that received a successful response prior to the read Strongly consistent reads are 2x the cost of Eventually consistent reads Strongly Consistent Reads comes with disadvantages A strongly consistent read might not be available if there is a network delay or outage. In this case, DynamoDB may return a server error (HTTP 500). Strongly consistent reads may have higher latency than eventually consistent reads. Strongly consistent reads are not supported on global secondary indexes. Strongly consistent reads use more throughput capacity than eventually consistent reads. Read operations (such as GetItem, Query, and Scan ) provide a ConsistentRead parameter, if set to true, DynamoDB uses strongly consistent reads during the operation. Query, GetItem, and BatchGetItem operations perform eventually consistent reads by default Query and GetItem operations can be forced to be strongly consistent Query operations cannot perform strongly consistent reads on Global Secondary Indexes BatchGetItem operations can be forced to be strongly consistent on a per-table basis","title":"1-2 DynamoDB Consistency"},{"location":"chap7/2DynamoDB/#1-3-dynamodb-advanced-topics","text":"DynamoDB Streams, Triggers, Cross Region Replication, DAX, VPC Endpoints etc.","title":"1-3 DynamoDB Advanced Topics"},{"location":"chap7/2DynamoDB/#1-4-dynamodb-performance","text":"Automatically scales horizontally runs exclusively on Solid State Drives (SSDs). SSDs help achieve the design goals of predictable low-latency response times for storing and accessing data at any scale. SSDs High I/O performance enables it to serve high-scale request workloads cost efficiently, and to pass this efficiency along in low request pricing allows provisioned table reads and writes Scale up throughput when needed Scale down throughput four times per UTC calendar day automatically partitions, reallocates and re-partitions the data and provisions additional server capacity as the table size grows or provisioned throughput is increased Global Secondary indexes (GSI) can be created upfront or added later","title":"1-4 DynamoDB Performance"},{"location":"chap7/2DynamoDB/#1-5-dynamodb-security","text":"Fine Grained Access Control (FGAC) gives a high degree of control over data in the table FGAC helps control who (caller) can access which items or attributes of the table and perform what actions (read/write capability). FGAC is integrated with IAM, which manages the security credentials and the associated permissions.","title":"1-5 DynamoDB Security"},{"location":"chap7/2DynamoDB/#1-6-dynamodb-encryption","text":"Data in Transit Encryption can be done by encrypting sensitive data on the client side or using encrypted connections (TLS) All the data in DynamoDB is encrypted in transit (except the data in DAX) communications to and from DynamoDB use the HTTPS protocol, which protects network traffic by using SSL/TLS encryption. DynamoDB supports Encryption at rest Encryption at rest is enabled on all DynamoDB table data and cannot be disabled E ncryption at rest enables encryption for the data persisted (data at rest) in the DynamoDB tables. E ncryption at rest includes the base tables, primary key, local and global secondary indexes, streams, global tables, backups, and DynamoDB Accelerator (DAX) clusters Encryption at rest automatically integrates with AWS KMS for managing the keys used for encrypting the tables. Encryption at rest also supports AWS owned CMK, AWS managed CMK, and Customer managed CMK Encryption at rest can be enabled only for a new table and not for an existing table Encryption once enabled for a table, cannot be disabled DynamoDB streams can be used with encrypted tables and are always encrypted with a table-level encryption key On-Demand Backups of encrypted DynamoDB tables are encrypted using S3\u2019s Server-Side Encryptio n Encryption at rest encrypts the data using 256-bit AES encryption.","title":"1-6 DynamoDB Encryption"},{"location":"chap7/2DynamoDB/#1-7-dynamodb-encryption-client","text":"DynamoDB Encryption Client is a software library that helps protect the table data before sending it to DynamoDB. Encrypting the sensitive data in transit and at rest helps ensure that the plaintext data isn\u2019t available to any third party, including AWS. Encryption Client encrypts attribute values which can be controlled, but does not encrypt entire table, attribute names or primary key","title":"1-7 DynamoDB Encryption Client"},{"location":"chap7/2DynamoDB/#1-8-dynamodb-costs","text":"Index Storage DynamoDB is an indexed data store Billable Data = Raw byte data size + 100 byte per-item storage indexing overhead Provisioned throughput Pay flat, hourly rate based on the capacity reserved as the throughput provisioned for the table one Write Capacity Unit provides one write per second for items < 1KB in size. one Read Capacity Unit provides one strongly consistent read (or two eventually consistent reads) per second for items < 4KB in size. Provisioned throughput charges for every 10 units of Write Capacity and every 50 units of Read Capacity. Reserved capacity Significant savings over the normal price Pay a one-time upfront fee DynamoDB also charges for storage, backup, replication, streams, caching, data transfer out.","title":"1-8 DynamoDB Costs"},{"location":"chap7/2DynamoDB/#1-9-intro-qas","text":"Which of the following are use cases for Amazon DynamoDB? Choose 3 answers Storing BLOB data. Managing web sessions Storing JSON documents Storing metadata for Amazon S3 objects Running relational joins and complex updates. Storing large amounts of infrequently accessed data. You are configuring your company\u2019s application to use Auto Scaling and need to move user state information. Which of the following AWS services provides a shared data store with durability and low latency? AWS ElastiCache Memcached (does not allow writes) Amazon Simple Storage Service (does not provide low latency) Amazon EC2 instance storage (not durable) Amazon DynamoDB Does Dynamo DB support in-place atomic updates? It is not defined No Yes It does support in-place non-atomic updates What is the maximum write throughput I can provision for a single Dynamic DB table? 1,000 write capacity units 100,000 write capacity units Dynamic DB is designed to scale without limits, but if you go beyond 10,000 you have to contact AWS first 10,000 write capacity units For a DynamoDB table, what happens if the application performs more reads or writes than your provisioned capacity? Nothing requests above the provisioned capacity will be performed but you will receive 400 error codes. requests above the provisioned capacity will be performed but you will receive 200 error codes. requests above the provisioned capacity will be throttled and you will receive 400 error codes . In which of the following situations might you benefit from using DynamoDB? (Choose 2 answers) You need fully managed database to handle highly complex queries You need to deal with massive amount of \u201chot\u201d data and require very low latency You need a rapid ingestion of clickstream in order to collect data about user behavior Your on-premises data center runs Oracle database, and you need to host a backup in AWS cloud Does Amazon DynamoDB support both increment and decrement atomic operations? No, neither increment nor decrement operations. Only increment, since decrement are inherently impossible with DynamoDB\u2019s data model. Only decrement, since increment are inherently impossible with DynamoDB\u2019s data model. Yes, both increment and decrement operations . What is the data model of DynamoDB? \u201cItems\u201d, with Keys and one or more Attribute; and \u201cAttribute\u201d, with Name and Value. \u201cDatabase\u201d, which is a set of \u201cTables\u201d, which is a set of \u201cItems\u201d, which is a set of \u201cAttributes\u201d. \u201cTable\u201d, a collection of Items; \u201cItems\u201d, with Keys and one or more Attribute; and \u201cAttribute\u201d, with Name and Value. \u201cDatabase\u201d, a collection of Tables; \u201cTables\u201d, with Keys and one or more Attribute; and \u201cAttribute\u201d, with Name and Value. In regard to DynamoDB, for which one of the following parameters does Amazon not charge you? Cost per provisioned write units Cost per provisioned read units Storage cost I/O usage within the same Region Which statements about DynamoDB are true? Choose 2 answers. DynamoDB uses a pessimistic locking model DynamoDB uses optimistic concurrency control DynamoDB uses conditional writes for consistency DynamoDB restricts item access during reads DynamoDB restricts item access during writes Which of the following is an example of a good DynamoDB hash key schema for provisioned throughput efficiency? User ID, where the application has many different users . Status Code where most status codes is the same. Device ID, where one is by far more popular than all the others. Game Type, where there are three possible game types. You are inserting 1000 new items every second in a DynamoDB table. Once an hour these items are analyzed and then are no longer needed. You need to minimize provisioned throughput, storage, and API calls. Given these requirements, what is the most efficient way to manage these Items after the analysis? Retain the items in a single table Delete items individually over a 24 hour period Delete the table and create a new table per hour Create a new table per hour When using a large Scan operation in DynamoDB, what technique can be used to minimize the impact of a scan on a table\u2019s provisioned throughput? Set a smaller page size for the scan Use parallel scans Define a range index on the table Prewarm the table by updating all items In regard to DynamoDB, which of the following statements is correct? An Item should have at least two value sets, a primary key and another attribute. An Item can have more than one attributes A primary key should be single-valued. An attribute can have one or several other attributes. Which one of the following statements is NOT an advantage of DynamoDB being built on Solid State Drives? serve high-scale request workloads low request pricing high I/O performance of WebApp on EC2 instance (Not related to DynamoDB) low-latency response times Which one of the following operations is NOT a DynamoDB operation? BatchWriteItem DescribeTable BatchGetItem BatchDeleteItem (DeleteItem deletes a single item in a table by primary key, but BatchDeleteItem doesn\u2019t exist) What item operation allows the retrieval of multiple items from a DynamoDB table in a single API call? GetItem BatchGetItem GetMultipleItems GetItemRange An application stores payroll information nightly in DynamoDB for a large number of employees across hundreds of offices. Item attributes consist of individual name, office identifier, and cumulative daily hours. Managers run reports for ranges of names working in their office. One query is. \u201cReturn all Items in this office for names starting with A through E\u201d. Which table configuration will result in the lowest impact on provisioned throughput for this query? [PROFESSIONAL] Configure the table to have a hash index on the name attribute, and a range index on the office identifier Configure the table to have a range index on the name attribute, and a hash index on the office identifier Configure a hash index on the name attribute and no range index Configure a hash index on the office Identifier attribute and no range index You need to migrate 10 million records in one hour into DynamoDB. All records are 1.5KB in size. The data is evenly distributed across the partition key. How many write capacity units should you provision during this batch load? 6667 4166 5556 ( 2 write units (1 for each 1KB) * 10 million/3600 secs, refer link) 2778 A meteorological system monitors 600 temperature gauges, obtaining temperature samples every minute and saving each sample to a DynamoDB table. Each sample involves writing 1K of data and the writes are evenly distributed over time. How much write throughput is required for the target table? 1 write capacity unit 10 write capacity units ( 1 write unit for 1K * 600 gauges/60 secs) 60 write capacity units 600 write capacity units 3600 write capacity units You are building a game high score table in DynamoDB. You will store each user\u2019s highest score for each game, with many games, all of which have relatively similar usage levels and numbers of players. You need to be able to look up the highest score for any game. What\u2019s the best DynamoDB key structure? HighestScore as the hash / only key. GameID as the hash key, HighestScore as the range key. (hash (partition) key should be the GameID, and there should be a range key for ordering HighestScore. ) GameID as the hash / only key. GameID as the range / only key. You are experiencing performance issues writing to a DynamoDB table. Your system tracks high scores for video games on a marketplace. Your most popular game experiences all of the performance issues. What is the most likely problem? DynamoDB\u2019s vector clock is out of sync, because of the rapid growth in request for the most popular game. You selected the Game ID or equivalent identifier as the primary partition key for the table. Users of the most popular video game each perform more read and write requests than average. You did not provision enough read or write throughput to the table. You are writing to a DynamoDB table and receive the following exception:\u201d ProvisionedThroughputExceededException\u201d. Though according to your Cloudwatch metrics for the table, you are not exceeding your provisioned throughput. What could be an explanation for this? You haven\u2019t provisioned enough DynamoDB storage instances You\u2019re exceeding your capacity on a particular Range Key You\u2019re exceeding your capacity on a particular Hash Key (Hash key determines the partition and hence the performance) You\u2019re exceeding your capacity on a particular Sort Key You haven\u2019t configured DynamoDB Auto Scaling triggers","title":"1-9 Intro QAs"}]}